<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="LLM输出向量池化方式 LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。\n">
<title>LLM系列-4：LLM的使用</title>

<link rel='canonical' href='https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-4llm%E7%9A%84%E4%BD%BF%E7%94%A8/'>

<link rel="stylesheet" href="/scss/style.min.ecadfb139628a394c81c08548f4d8f20e087bb8f183b587b1b4a7d2cdd03c920.css"><meta property='og:title' content="LLM系列-4：LLM的使用">
<meta property='og:description' content="LLM输出向量池化方式 LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。\n">
<meta property='og:url' content='https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-4llm%E7%9A%84%E4%BD%BF%E7%94%A8/'>
<meta property='og:site_name' content='xxcjw&#39;s blog!'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-04-15T18:28:48&#43;08:00'/><meta property='article:modified_time' content='2025-04-19T10:05:15&#43;08:00'/>
<meta name="twitter:title" content="LLM系列-4：LLM的使用">
<meta name="twitter:description" content="LLM输出向量池化方式 LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。\n">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">

        <div id="article-toolbar" style="position: sticky;top: 5px;z-index: 1000;">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>返回</span>
            </a>
        </div>
    

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#llm输出向量池化方式">LLM输出向量池化方式</a>
      <ol>
        <li><a href="#平均池化">平均池化</a></li>
        <li><a href="#最大池化">最大池化</a></li>
        <li><a href="#cls-池化">[CLS] 池化</a></li>
        <li><a href="#加权平均池化">加权平均池化</a></li>
        <li><a href="#last-池化">Last 池化</a></li>
      </ol>
    </li>
    <li><a href="#transformers库中相关类">Transformers库中相关类</a>
      <ol>
        <li><a href="#常用的-automodel类">常用的 AutoModel类</a></li>
        <li><a href="#基础模型">基础模型</a></li>
        <li><a href="#生成任务">生成任务</a></li>
        <li><a href="#序列分类任务">序列分类任务</a></li>
        <li><a href="#标记分类任务">标记分类任务</a></li>
        <li><a href="#问答任务类">问答任务类</a></li>
      </ol>
    </li>
    <li><a href="#大模型词表嵌入">大模型词表嵌入</a>
      <ol>
        <li><a href="#llm流程">LLM流程</a></li>
        <li><a href="#参考链接">参考链接</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/llm%E7%B3%BB%E5%88%97-4llm%E7%9A%84%E4%BD%BF%E7%94%A8/">LLM系列-4：LLM的使用</a>
        </h2>
    
        
    </div>

    
    
    
    

    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-04-15</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" width="24" height="24" stroke-width="2"> <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4"></path> <path d="M13.5 6.5l4 4"></path> <path d="M16 19h6"></path> </svg> 
                <time class="article-words">
                    3454 字
                </time>
            </div>
        


        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 7 分钟
                </time>
            </div>
        
    </footer>
    

    


    
    <header class="article-category">
        
            <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="background-color: #2a9d8f; color: #fff;">
                大模型
            </a>
        
        </header>
    

</div>


</header>

    <section class="article-content">
    
    
    <h2 id="llm输出向量池化方式">LLM输出向量池化方式
</h2><p>LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。</p>
<h3 id="平均池化">平均池化
</h3><p>平均池化会计算序列中所有向量的平均值，从而得到一个固定长度的向量。这种方式能有效综合序列里所有元素的信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pooled_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 在序列长度维度上求平均</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: torch.Size([2, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="最大池化">最大池化
</h3><p>最大池化会在序列中每个维度选取最大值，以此生成一个固定长度的向量。它能提取序列中的关键特征。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pooled_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 在序列长度维度上取最大值</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: torch.Size([2, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="cls-池化">[CLS] 池化
</h3><p>也就是 CLS Token Pooling 。在使用预训练模型（像 BERT 这类）时，通常会在输入序列开头添加一个特殊的 <code>[CLS]</code> 标记。<code>[CLS]</code> 池化就是直接选取这个标记对应的输出向量作为整个序列的表示。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pooled_output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># 选取每个样本的第一个向量</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: torch.Size([2, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="加权平均池化">加权平均池化
</h3><p>也就是 Weighted Mean Pooling ，加权平均池化会依据每个向量的重要性为其分配不同的权重，再计算加权平均值。这样可以更有针对性地综合序列信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 weights 是每个向量的权重，形状为 (batch_size, sequence_length)</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 确保权重和为 1</span>
</span></span><span class="line"><span class="cl"><span class="n">weighted_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pooled_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weighted_hidden_states</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: torch.Size([2, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="last-池化">Last 池化
</h3><p>也就是 Last Token 池化。选取序列中最后一个 token 对应的向量作为整个序列的表示。这种池化方式在某些场景下是有效的，例如在处理文本生成或者问答任务时，模型最后一个输出的 token 可能包含了整个序列处理后的关键信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pooled_output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># 选取每个样本的最后一个向量</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: torch.Size([2, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="transformers库中相关类">Transformers库中相关类
</h2><p>在 <code>transformers</code> 库中，<code>AutoModel</code> 和相关类提供了一个统一的接口来加载不同类型的预训练模型。你可以通过 <code>AutoModel</code> 系列类来简化模型加载过程，而无需关心特定模型的类型。</p>
<h3 id="常用的-automodel类">常用的 AutoModel类
</h3><p><strong><code>AutoModel</code></strong>: 加载基础模型，适用于不带头的 Transformer 模型。</p>
<p><strong><code>AutoModelForSequenceClassification</code></strong>: 加载用于序列分类任务的预训练模型。</p>
<p><strong><code>AutoModelForTokenClassification</code></strong>: 加载用于标记分类（如命名实体识别）的模型。</p>
<p><strong><code>AutoModelForQuestionAnswering</code></strong>: 加载用于问答任务的模型。</p>
<p><strong><code>AutoModelForCausalLM</code></strong>: 加载用于自回归语言建模（生成任务）的模型。</p>
<p><strong><code>AutoTokenizer</code></strong>: 用于加载与模型匹配的标记器（tokenizer），负责文本预处理和编码。</p>
<p><strong><code>AutoFeatureExtractor</code></strong>: 用于加载与图像或其他类型输入匹配的特征提取器。</p>
<h3 id="基础模型">基础模型
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;bert-base-uncased&#34;</span> 
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;Hugging Face is creating a tool that democratizes AI.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>  <span class="c1"># 获取模型的最后一层隐藏状态</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Last hidden state shape:&#34;</span><span class="p">,</span> <span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># -----------------查看隐藏层形状----------------------</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ------------获取输入转换后的 token-------------------</span>
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ----------------获取输入转换后的 token---------------</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 方式一</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 方式二</span>
</span></span><span class="line"><span class="cl"><span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># -------------查看每个 token 对应的文字----------------</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 方式一</span>
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;每个 token 对应的文字：&#34;</span><span class="p">,</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 方式二（）</span>
</span></span><span class="line"><span class="cl"><span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Decoded text:&#34;</span><span class="p">,</span> <span class="n">decoded_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 查看大模型词表大小（也可以根据config.json查看）</span>
</span></span><span class="line"><span class="cl"><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;嵌入矩阵大小: </span><span class="si">{</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h3 id="生成任务">生成任务
</h3><p>用于加载 <strong>自回归语言模型</strong> 的类。自回归语言模型（Causal LM）在给定一些上下文时，会生成下一个可能的单词或 token。通常用于生成任务。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;gpt2&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;Hugging Face is creating a tool that democratizes AI.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>              <span class="c1"># 输入ID（token化后的文本）</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>         <span class="c1"># 生成的最大长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># 返回生成的序列数</span>
</span></span><span class="line"><span class="cl">    <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># 防止生成重复的n-gram</span>
</span></span><span class="line"><span class="cl">    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>        <span class="c1"># 温度（控制随机性,值越大越具创造性）</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>               <span class="c1"># 只考虑前k个最可能的词</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>             <span class="c1"># 采用 nucleus sampling（前p%的词）</span>
</span></span><span class="line"><span class="cl">    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>          <span class="c1"># 启用采样（避免确定性输出）</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>top_k</code>：从生成词汇中选择前 k 个最有可能的词进行采样。较小的值意味着选择更有限的词汇。</p>
<p><code>top_p</code>：从概率累积值中选择前 <code>p</code>% 的词汇，控制采样的多样性。与 <code>top_k</code> 类似，也是用于限制候选词的范围。</p>
<p><code>do_sample</code>：是否启用采样。如果为 <code>True</code>，模型将从预测的概率分布中随机采样生成下一个 token；如果为 <code>False</code>，它将选择概率最大的 token（即确定性生成）。启用采样机制可以增加生成文本的多样性，避免每次生成的文本都相同。</p></blockquote>
<h3 id="序列分类任务">序列分类任务
</h3><p>如果在做分类任务（例如情感分析），可以使用这个类。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;bert-base-uncased&#34;</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># 假设是二分类任务</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;Hugging Face is creating a tool that democratizes AI.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 前向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>  <span class="c1"># 获取分类任务的输出logits</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Logits:&#34;</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>    <span class="c1"># 查看输出形状</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>模型的输出并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h3 id="标记分类任务">标记分类任务
</h3><p>假设做的是命名实体识别任务，可以使用这个类。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForTokenClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;bert-base-uncased&#34;</span> 
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;Hugging Face is creating a tool that democratizes AI.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 前向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">logits_token_class</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>  <span class="c1"># 获取每个token的分类结果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Token classification logits:&#34;</span><span class="p">,</span> <span class="n">logits_token_class</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="问答任务类">问答任务类
</h3><p>通常用于从给定的上下文中回答问题。模型通过读取文本（context）和问题（question），然后返回答案。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForQuestionAnswering</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;distilbert-base-uncased-distilled-squad&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;What is the capital of France?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">context</span> <span class="o">=</span> <span class="s2">&#34;France is a country in Europe. Paris is its capital and the largest city.&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># `start_logits` 和 `end_logits` 分别表示答案的起始和结束位置</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">start_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span><span class="p">)</span>  <span class="c1"># 答案起始位置</span>
</span></span><span class="line"><span class="cl"><span class="n">end_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span><span class="p">)</span>      <span class="c1"># 答案结束位置</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 7. 解码答案</span>
</span></span><span class="line"><span class="cl"><span class="n">answer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_string</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">start_position</span><span class="p">:</span><span class="n">end_position</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Question:&#34;</span><span class="p">,</span> <span class="n">question</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Answer:&#34;</span><span class="p">,</span> <span class="n">answer</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>在问答任务中，模型通过 <code>start_logits</code> 和 <code>end_logits</code> 来输出答案的位置。需要从模型输出的 tokens 中提取出 <strong>起始位置</strong> 和 <strong>结束位置</strong> 的答案，然后将其通过 <code>convert_ids_to_tokens</code> 转换为字符串。<code>decode</code> 方法主要用于生成文本，而不是处理直接的预测位置（例如起始和结束位置）。</p>
<p>在问答任务中，我们不是生成每个 token，而是从模型输出的 <code>start_logits</code> 和 <code>end_logits</code> 中直接找到答案的位置。</p></blockquote>
<h2 id="大模型词表嵌入">大模型词表嵌入
</h2><div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">模型</th>
          <th style="text-align: center">词表嵌入维度</th>
          <th style="text-align: center">参数数量</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">GPT-2</td>
          <td style="text-align: center">1,024维</td>
          <td style="text-align: center">15亿</td>
      </tr>
      <tr>
          <td style="text-align: center">GPT-3</td>
          <td style="text-align: center">12,288维</td>
          <td style="text-align: center">1750亿</td>
      </tr>
      <tr>
          <td style="text-align: center">BERT-base</td>
          <td style="text-align: center">768维</td>
          <td style="text-align: center">1.1亿</td>
      </tr>
      <tr>
          <td style="text-align: center">BERT-large</td>
          <td style="text-align: center">1,024维</td>
          <td style="text-align: center">3.4亿</td>
      </tr>
      <tr>
          <td style="text-align: center">LLaMA2-7B</td>
          <td style="text-align: center">4,096维</td>
          <td style="text-align: center">70亿</td>
      </tr>
      <tr>
          <td style="text-align: center">LLaMA-2-13B</td>
          <td style="text-align: center">5,120维</td>
          <td style="text-align: center">130亿</td>
      </tr>
      <tr>
          <td style="text-align: center">LLaMA-3-8B</td>
          <td style="text-align: center">4,096维</td>
          <td style="text-align: center">80亿</td>
      </tr>
      <tr>
          <td style="text-align: center">deepseek-R1-14B</td>
          <td style="text-align: center">5120维</td>
          <td style="text-align: center">140亿</td>
      </tr>
      <tr>
          <td style="text-align: center">qwen2.5-7B</td>
          <td style="text-align: center">3,584维</td>
          <td style="text-align: center">70亿</td>
      </tr>
      <tr>
          <td style="text-align: center">Bloom-7B1</td>
          <td style="text-align: center">4,096维</td>
          <td style="text-align: center">71亿</td>
      </tr>
      <tr>
          <td style="text-align: center">Bloom-1B7</td>
          <td style="text-align: center">2,048维</td>
          <td style="text-align: center">17亿</td>
      </tr>
      <tr>
          <td style="text-align: center">Bloom-560M</td>
          <td style="text-align: center">1,024维</td>
          <td style="text-align: center">5.6亿</td>
      </tr>
  </tbody>
</table></div>
<p>BERT模型分为24层和12层两种，BERT-base使用的是12层的Transformer Encoder结构，BERT-Large使用的是24层的Transformer Encoder结构。</p>
<p>对于不同的大型语言模型，它们能处理的最大token长度（也称为上下文窗口大小）是由模型本身决定的。</p>
<p>大模型内部都存在一个词表，存储了一系列token的集合。它决定了模型所 “认识” 的词汇和符号的范围。</p>
<blockquote>
<p>在LLM的<code>config.json</code> 文件中，一般可以找到相关参数。</p>
<ul>
<li>如在 deepseek-r1-14b 的配置文件中，<code>max_position_embeddings</code>参数定义了模型能够处理的最大位置嵌入数，也就是模型能够接受的最大token长度</li>
<li>而<code>vocab_size</code>参数就是词表的大小（即多少行）；<code>hidden_size</code>参数就是每一个token所对应的嵌入向量大小（即多少列）</li>
</ul></blockquote>
<h3 id="llm流程">LLM流程
</h3><p>Token是LLM处理文本的基本单位。当我们将文本输入 LLM 时，模型首先会将文本切分成 Token 序列，然后再对这些 Token 通过词表转换成高维嵌入，最终经过大模型处理生成我们期望的输出结果。</p>
<img src="fb3c64ab5417435.gif" alt="fb3c64ab5417435" style="zoom:80%;" />
<p>此外，还有一些相关概念：</p>
<ul>
<li>最大输出长度（输出限制）：模型单次生成文本的最大长度</li>
<li>上下文长度：指的是模型在处理输入时能够 “看到” 或考虑的文本范围，即模型在生成输出时，会参考的前文的 token 数量。</li>
<li>上下文截断：应对超长对话的策略。</li>
</ul>
<blockquote>
<p><strong>上下文截断</strong>是一种在工程层面实施的策略，而非模型本身固有的能力。其具体指的是：如果用户在多轮对话中累积的输入和输出 Token 数量超出最大上下文长度的限制，服务端通常会保留最近的内容，而丢弃早期输入，即只能 “记住最近的，遗忘久远的”。</p></blockquote>
<blockquote>
<p>模型的最大输入 token 长度有多种不同叫法：</p>
<ul>
<li>
<p><strong>最大位置嵌入（Max Position Embeddings）</strong>：位置嵌入用于标记每个 token 在输入序列中的位置，规定了模型能处理的输入序列最大长度</p>
</li>
<li>
<p><strong>上下文窗口大小（Context Window Size）</strong>：指的是模型在处理输入时能够考虑的上下文范围，也就是输入序列的最大长度。这个概念在自回归模型中常用，模型生成每个 token 时会参考之前的上下文，上下文窗口大小决定了能参考的最大 token 数量。</p>
</li>
</ul></blockquote>
<h3 id="参考链接">参考链接
</h3><blockquote>
<p><a class="link" href="https://www.aisharenet.com/damoxingguanjiancanai/"  target="_blank" rel="noopener"
    >大模型关键参数解读</a></p></blockquote>

</section>


    <footer class="article-footer">
    
    
    <section class="article-category">
        
            <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="background-color: #2a9d8f; color: #fff;">
                大模型
            </a>
        
    </section>
    

    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            最后更新于 2025-04-19 10:05
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    
     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 xxcjw&#39;s bolg！
    </section>
    
    <section class="powerby">
        
            光终究会洒在你的身上，你也会灿烂一场！ <br/>
        
    </section>

    
    <section class="running-time">
        本博客已稳定运行
        <span id="runningdays" class="running-days"></span>
    </section>


</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script>
    let s1 = '2025-4-1'; 
    s1 = new Date(s1.replace(/-/g, "/"));
    let s2 = new Date();
    let timeDifference = s2.getTime() - s1.getTime();

    let days = Math.floor(timeDifference / (1000 * 60 * 60 * 24));
    let hours = Math.floor((timeDifference % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
    let minutes = Math.floor((timeDifference % (1000 * 60 * 60)) / (1000 * 60));

    let result = days + "天" + hours + "小时" + minutes + "分钟";
    document.getElementById('runningdays').innerHTML = result;
</script>
    </body>
</html>
