<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="pipelines Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？\n">
<title>LLM系列-3：模型与分词器</title>

<link rel='canonical' href='https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/'>

<link rel="stylesheet" href="/scss/style.min.ecadfb139628a394c81c08548f4d8f20e087bb8f183b587b1b4a7d2cdd03c920.css"><meta property='og:title' content="LLM系列-3：模型与分词器">
<meta property='og:description' content="pipelines Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？\n">
<meta property='og:url' content='https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/'>
<meta property='og:site_name' content='xxcjw&#39;s blog!'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-04-13T10:56:01&#43;08:00'/><meta property='article:modified_time' content='2025-04-14T11:49:59&#43;08:00'/>
<meta name="twitter:title" content="LLM系列-3：模型与分词器">
<meta name="twitter:description" content="pipelines Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？\n">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">

        <div id="article-toolbar" style="position: sticky;top: 5px;z-index: 1000;">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>返回</span>
            </a>
        </div>
    

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#pipelines">pipelines</a>
      <ol>
        <li><a href="#使用分词器进行预处理">使用分词器进行预处理</a></li>
        <li><a href="#将预处理好的输入送入模型">将预处理好的输入送入模型</a></li>
        <li><a href="#对模型输出进行后处理">对模型输出进行后处理</a></li>
      </ol>
    </li>
    <li><a href="#底层模型">底层&ndash;模型</a></li>
    <li><a href="#底层分词器">底层&ndash;分词器</a>
      <ol>
        <li><a href="#分词器编码-方式1">分词器编码-方式1</a></li>
        <li><a href="#分词器编码-方式2">分词器编码-方式2</a></li>
        <li><a href="#分词器解码">分词器解码</a></li>
      </ol>
    </li>
    <li><a href="#底层padding-与-attention-mask">底层&ndash;Padding 与 Attention Mask</a>
      <ol>
        <li><a href="#padding-操作">Padding 操作</a></li>
        <li><a href="#截断操作">截断操作</a></li>
        <li><a href="#返回格式">返回格式</a></li>
        <li><a href="#完整格式">完整格式</a></li>
      </ol>
    </li>
    <li><a href="#底层模型推理和输出">底层&ndash;模型推理和输出</a>
      <ol>
        <li><a href="#生成任务输出">生成任务输出</a></li>
        <li><a href="#隐藏状态提取">隐藏状态提取</a></li>
      </ol>
    </li>
    <li><a href="#完整代码">完整代码</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/llm%E7%B3%BB%E5%88%97-3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/">LLM系列-3：模型与分词器</a>
        </h2>
    
        
    </div>

    
    
    
    

    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-04-13</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" width="24" height="24" stroke-width="2"> <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4"></path> <path d="M13.5 6.5l4 4"></path> <path d="M16 19h6"></path> </svg> 
                <time class="article-words">
                    5753 字
                </time>
            </div>
        


        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 12 分钟
                </time>
            </div>
        
    </footer>
    

    


    
    <header class="article-category">
        
            <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="background-color: #2a9d8f; color: #fff;">
                大模型
            </a>
        
        </header>
    

</div>


</header>

    <section class="article-content">
    
    
    <h2 id="pipelines">pipelines
</h2><p>Transformers 库最基础的对象就是 <code>pipeline()</code> 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">## 文本生成任务示例：</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-generation&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&#34;distilgpt2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;In this course, we will teach you how to&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其背后经过了三个步骤：</p>
<ul>
<li>预处理，将原始文本转换为模型可以接受的输入格式</li>
<li>将处理好的输入送入模型，根据具体任务进行推理生成</li>
<li>对模型的输出进行后处理，将其转换为人类方便阅读的格式</li>
</ul>
<h3 id="使用分词器进行预处理">使用分词器进行预处理
</h3><p>因为神经网络模型无法直接处理文本，因此首先需要通过<strong>分词器</strong> (tokenizer)将文本转换为模型可以理解的数字。</p>
<p>我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作。因此我们使用 <code>AutoTokenizer</code> 类和它的 <code>from_pretrained()</code> 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;魔都是哪个城市?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>输出为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mi">101</span><span class="p">,</span>  <span class="mi">1045</span><span class="p">,</span>  <span class="mi">1005</span><span class="p">,</span>  <span class="mi">2310</span><span class="p">,</span>  <span class="mi">2042</span><span class="p">,</span>  <span class="mi">3403</span><span class="p">,</span>  <span class="mi">2005</span><span class="p">,</span>  <span class="mi">1037</span><span class="p">,</span> <span class="mi">17662</span><span class="p">,</span> <span class="mi">12172</span><span class="p">,</span> <span class="mi">2607</span><span class="p">,</span>  <span class="mi">2026</span><span class="p">,</span>  <span class="mi">2878</span><span class="p">,</span>  <span class="mi">2166</span><span class="p">,</span>  <span class="mi">1012</span><span class="p">,</span>   <span class="mi">102</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span>  <span class="mi">101</span><span class="p">,</span>  <span class="mi">1045</span><span class="p">,</span>  <span class="mi">5223</span><span class="p">,</span>  <span class="mi">2023</span><span class="p">,</span>  <span class="mi">2061</span><span class="p">,</span>  <span class="mi">2172</span><span class="p">,</span>   <span class="mi">999</span><span class="p">,</span>   <span class="mi">102</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>     <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">]),</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，输出中包含两个键 <code>input_ids</code> 和 <code>attention_mask</code>，其中 <code>input_ids</code> 对应分词之后的 tokens 映射到的数字编号列表，而 <code>attention_mask</code> 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。</p>
<blockquote>
<p><code>checkpoint</code>（检查点）是指在模型训练过程中，为了避免可能出现的意外情况，定期保存模型的状态，这个状态包含了所使用的模型名称、模型的参数、优化器的状态、训练步数等信息。</p>
<p>注意，此时的到的只是分词后token对应的ID，并没有得到经过LLM编码的高维向量。</p></blockquote>
<h3 id="将预处理好的输入送入模型">将预处理好的输入送入模型
</h3><p>预训练模型的下载/加载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 <code>AutoModel</code> 类和对应的 <code>from_pretrained()</code> 函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务。</p>
<center><img src="image-20250413150359777.png" alt="image-20250413150359777" style="zoom:70%;" /></center>
<p>相信你还没有理解，来看接下来这一段代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>那它们之间的区别就在于<code>Head</code>部分，如果使用<code>AutoModel</code>就相当于只是用预训练模型中最基础的 Transformer 模块，得到的是大模型的高维嵌入向量。而如果使用<code>AutoModelForCausalLM</code>，就相当于多加了一个<code>Head</code>来完成特定的任务。</p>
<p>Transformers 库封装了很多这样不同的结构，常见的有：</p>
<ul>
<li><code>*Model</code> （返回 hidden states）</li>
<li><code>*ForCausalLM</code> （用于条件语言模型）【我用过的就是这种】</li>
<li><code>*ForMaskedLM</code> （用于遮盖语言模型）</li>
<li><code>*ForMultipleChoice</code> （用于多选任务）</li>
<li><code>*ForQuestionAnswering</code> （用于自动问答任务）</li>
<li><code>*ForSequenceClassification</code> （用于文本分类任务）</li>
<li><code>*ForTokenClassification</code> （用于 token 分类任务，例如 NER）</li>
</ul>
<p>Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。</p>
<blockquote>
<p><code>AutoModel.from_pretrained(checkpoint)</code></p>
<ul>
<li>这个方法加载的是基础的预训练模型架构，不包含特定任务的头部（head）。基础模型通常会输出隐藏状态（hidden states），这些隐藏状态可以作为特征表示，用于下游任务的进一步处理。例如，在文本分类任务中，可以将这些隐藏状态输入到一个全连接层进行分类。</li>
<li>适用于那些需要对模型进行自定义扩展或微调的场景，可以根据自己的需求添加特定任务的头部，以适应不同的任务。</li>
</ul>
<p><code>AutoModelForCausalLM.from_pretrained(checkpoint)</code></p>
<ul>
<li>这个方法加载的是专门用于因果语言模型（Causal Language Model，CLM）任务的预训练模型。因果语言模型的目标是根据前面的上下文预测下一个词，常用于文本生成任务等。该模型已经包含了一个语言建模头部（language modeling head），可以直接用于生成文本，无需额外添加特定任务的头部。</li>
<li>使用于文本生成任务类。</li>
</ul></blockquote>
<blockquote>
<p>完整代码示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#------------------使用 AutoModel 加载基础模型-----------------</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&#34;Once upon a time&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#------使用 AutoModelForSequenceClassification 加载模型--------</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&#34;Once upon a time&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>outputs.logits</code> 是模型最终的未经过归一化处理的预测分数，如[-1.5607,  1.6123]，它们并不是概率值。只是<code>outputs</code>中的一部分信息。如果直接<code>print(outputs)</code>输出的更全。</p></blockquote>
<p>其输出分别为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># AutoModel输出，(batch_size, sequence_length, vocab_size)
</span></span><span class="line"><span class="cl">torch.Size([1, 4, 768])  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># AutoModelForSequenceClassification输出,句子的情感分类值
</span></span><span class="line"><span class="cl">tensor([[-1.5607,  1.6123], grad_fn=&lt;AddmmBackward0&gt;)    
</span></span><span class="line"><span class="cl">torch.Size([1, 2])    # 标签，positive 或 negative
</span></span></code></pre></td></tr></table>
</div>
</div><p>补充：查看输出的最后隐藏状态形状</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------等价于-------------------</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>
<h3 id="对模型输出进行后处理">对模型输出进行后处理
</h3><p>由于模型的输出只是一些数值，因此并不适合人类阅读。还要经过解码操作，比如以下代码可以得到生成的文本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&#34;Once upon a time&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到， pipeline 背后的工作原理就是最底层的实现方式，它封装好了底层代码方便使用而已。下面会具体介绍组成 pipeline 的两个重要组件<strong>模型</strong>（<code>Models</code> 类）和<strong>分词器</strong>（<code>Tokenizers</code> 类）。</p>
<blockquote>
<p><code>outputs = model(**inputs)</code></p>
<p>这种调用方式主要用于获取模型在输入数据上的中间结果，比如隐藏状态（hidden states）、未归一化的预测分数（logits）等。这些结果通常用于进一步的任务处理。</p>
<p><code>output = model.generate(**inputs)</code></p>
<p>此调用方式专门用于文本生成任务。它会基于输入的上下文，使用特定的生成策略（如贪心搜索、束搜索等）来生成后续的文本序列。</p></blockquote>
<h2 id="底层模型">底层&ndash;模型
</h2><p>在大部分情况下，我们都应该使用 <code>AutoModel</code> 来加载模型。这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。</p>
<p>所有存储在<code>HuggingFace</code>上的模型都可以通过 <code>Model.from_pretrained()</code> 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;bert-base-cased&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;./models/bert/&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果本地没有下载该模型的权重文件，代码运行后会自动缓存下载的模型权重，默认保存到 <code>~/.cache/huggingface/transformers</code>。</p>
<h2 id="底层分词器">底层&ndash;分词器
</h2><p>由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为<strong>编码 (Encoding)</strong>，包含两个步骤：</p>
<ol>
<li>分词：使用分词器按某种策略将文本切分为 tokens；</li>
<li>映射：将 tokens 转化为对应的 token IDs（词表）；</li>
</ol>
<p>分词器的加载与模型相似，使用 <code>Tokenizer.from_pretrained()</code>函数。同样地，在大部分情况下我们都应该使用 <code>AutoTokenizer</code> 来加载分词器。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;bert-base-cased&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="分词器编码-方式1">分词器编码-方式1
</h3><p>可以通过 <code>encode()</code> 函数将上述两个步骤合并，并且 <code>encode()</code> 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 [CLS] 和 [SEP] 。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/bert-base-cased&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&#34;Using a Transformer network is simple&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">sequence_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sequence_ids</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其输出为如下，其中 101 和 102 分别是 [CLS] 和 [SEP] 对应的 token IDs。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="分词器编码-方式2">分词器编码-方式2
</h3><p>注意，上面这些只是为了演示。<strong>在实际编码文本时，最常见的是直接使用分词器进行处理</strong>，这样不仅会返回分词后的 token IDs，还自动包含了模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 <code>token_type_ids</code> 和 <code>attention_mask</code>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/bert-base-cased&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&#34;Using a Transformer network is simple&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">sequence_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sequence_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其输出为如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">{&#39;input_ids&#39;: [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 
</span></span><span class="line"><span class="cl"> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], 
</span></span><span class="line"><span class="cl"> &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]}
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>也就是说，两者区别在于：</p>
<ul>
<li><code>tokenizer.encode</code> 方法：返回的是编码后的 ID 列表</li>
<li><code>tokenizer</code> 方法：更通用，返回一个包含更多信息的字典,如 attention_mask 、token_type_ids等</li>
</ul></blockquote>
<h3 id="分词器解码">分词器解码
</h3><p>文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。通过 <code>decode()</code> 函数解码前面生成的 token IDs：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/bert-base-cased&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">decoded_string</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">7993</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">11303</span><span class="p">,</span> <span class="mi">1200</span><span class="p">,</span> <span class="mi">2443</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">3014</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">decoded_string</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">decoded_string</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">101</span><span class="p">,</span> <span class="mi">7993</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">13809</span><span class="p">,</span> <span class="mi">23763</span><span class="p">,</span> <span class="mi">2443</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">3014</span><span class="p">,</span> <span class="mi">102</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">decoded_string</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>输出为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Using a transformer network is simple
</span></span><span class="line"><span class="cl">[CLS] Using a Transformer network is simple [SEP]
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="底层padding-与-attention-mask">底层&ndash;Padding 与 Attention Mask
</h2><p>在实际中，一个 batch 包含多个输入，每个输入有长有短，而输入张量必须是严格的二维矩形，维度为 <strong>(batch size,sequence length)</strong>，即每一段文本编码后的 token IDs 数量必须一样多。我们需要通过 <code>Padding</code> 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度。</p>
<p>在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 <code>Attention Mask</code> 了。它且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。</p>
<p>正如前面所说，在实际使用时，应该直接使用分词器来完成包括分词、转换 token IDs、Padding、构建 Attention Mask、截断等操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/bert-base-cased&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;So have I!&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&#34;longest&#34;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>分词器的输出包含了模型需要的所有输入项。包括 <code>input_ids</code>和 <code>attention_mask</code>。</p>
<h3 id="padding-操作">Padding 操作
</h3><p><strong>Padding 操作</strong>通过 <code>padding</code> 参数来控制：</p>
<ul>
<li><code>padding=&quot;longest&quot;</code>： 将序列填充到当前 batch 中最长序列的长度；</li>
<li><code>padding=&quot;max_length&quot;</code>：将所有序列填充到模型能够接受的最大长度，例如 BERT 模型就是 512。（如果代码指定了<code>max_length</code>的长度，就不是 512 了）</li>
<li><code>padding=True</code>： 等同于 <code>padding=&quot;longest&quot;</code>；</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">model_inputs = tokenizer(sequences, padding=&#34;max_length&#34;)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="截断操作">截断操作
</h3><p><strong>截断操作</strong>通过 <code>truncation</code> 参数来控制，如果 <code>truncation=True</code>，那么大于模型最大接受长度的序列都会被截断。此外，也可以通过 <code>max_length</code> 参数来控制截断长度。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="返回格式">返回格式
</h3><p>分词器还可以通过 <code>return_tensors</code> 参数指定返回的张量格式：设为 <code>pt</code> 则返回 PyTorch 张量；<code>tf</code> 则返回 TensorFlow 张量，<code>np</code> 则返回 NumPy 数组。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">model_inputs = tokenizer(sequences, padding=True, return_tensors=&#34;pt&#34;)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="完整格式">完整格式
</h3><p>综上所述，实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;xxx-models&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sequences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;So have I!&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其输出为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">{&#39;input_ids&#39;: tensor([
</span></span><span class="line"><span class="cl">    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
</span></span><span class="line"><span class="cl">      2607,  2026,  2878,  2166,  1012,   102],
</span></span><span class="line"><span class="cl">    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
</span></span><span class="line"><span class="cl">         0,     0,     0,     0,     0,     0]]), 
</span></span><span class="line"><span class="cl"> &#39;attention_mask&#39;: tensor([
</span></span><span class="line"><span class="cl">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
</span></span><span class="line"><span class="cl">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">tensor([[-1.5607,  1.6123],
</span></span><span class="line"><span class="cl">        [-3.6183,  3.9137]], grad_fn=&lt;AddmmBackward0&gt;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 <code>padding=True, truncation=True</code> 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断。</p>
<h2 id="底层模型推理和输出">底层&ndash;模型推理和输出
</h2><p>在将文本转换为模型可接受的输入格式后，就可以进行模型推理了。模型推理是指将预处理后的输入数据送入模型，让模型根据其内部的参数和结构进行计算，从而得到每个token包含上下文信息的高维嵌入。不同任务类型的模型会返回不同形式的输出。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面是一些参数解释：</p>
<ul>
<li><code>**inputs</code>：这是一个解包操作，<code>inputs</code> 通常是一个字典，包含了<code>input_ids、attention_mask</code>等信息。通过 <code>**inputs</code> 可以将字典中的键值对作为关键字参数传递给 <code>generate()</code> 方法，自动处理 input 的所有参数</li>
<li><code>输出长度</code>：指定生成文本的最大长度（以token为单位），当生成文本达到这个长度时，生成过程将停止
<ul>
<li><code>max_length</code> 生成序列的最大总长度，包含了输入加输出的token总长度</li>
<li><code>max_new_tokens</code> 仅关注新生成的token数量，而不考虑输入序列本身的长度</li>
</ul>
</li>
<li><code>num_beams</code>：束搜索（Beam Search）的束宽。束搜索会在每一步保留 <code>num_beams</code> 个最有可能的候选序列，可以提高生成文本的质量，从而找到更优的生成结果，但同时也会增加计算量和内存消耗</li>
<li><code>no_repeat_ngram_size</code>：模型在生成过程中不允许出现重复的 n - gram 大小。n - gram 是指连续的 n 个标记组成的序列，例如当 <code>n = 2</code> 时，就是不允许连续的两个标记组成的序列重复出现。用于避免生成的文本中出现重复的短语或句子，提高生成文本的多样性和质量。例如，如果生成的文本中已经出现了 “the dog”，那么在后续的生成过程中就不会再出现 “the dog” 这个 2 - gram</li>
<li><code>output[0]</code>：<code>model.generate()</code> 方法返回的是一个包含多个生成序列的张量，<code>output[0]</code> 表示取第一个生成序列。在 <code>num_return_sequences = 1</code> 的情况下，通常只生成一个序列</li>
<li><code>skip_special_tokens=True</code>：在解码过程中是否跳过特殊符号，如[CLS]、[SEP]。设置为 <code>True</code> 可以去除这些特殊标记，使生成的文本更加干净和易读</li>
</ul>
<h3 id="生成任务输出">生成任务输出
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span><span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&#34;The future of AI is&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于文本生成任务（如使用 <code>AutoModelForCausalLM</code>），模型通过 <code>generate()</code> 方法直接生成 token IDs，需通过分词器解码为文本。</p>
<h3 id="隐藏状态提取">隐藏状态提取
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span><span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&#34;bert-base-uncased&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 获取最后一层隐藏状态</span>
</span></span><span class="line"><span class="cl"><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&#34;Once upon a time&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Hidden states shape:&#34;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 输出 torch.Size([2, sequence_length, 768])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>生成参数控制：<code>max_length</code>、<code>num_beams</code>（束搜索）、<code>temperature</code>（采样温度）等参数可调节生成结果的质量和多样性。</p>
<h2 id="完整代码">完整代码
</h2><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span><span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">,</span><span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;魔都是哪个城市?&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">	<span class="n">question</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># inputs = tokenizer(question, padding=True, return_tensors=&#39;pt&#39;).to(model.device)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#  temperature=0.7,   # 增加创造性</span>
</span></span><span class="line"><span class="cl">   <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output_text</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>device_map=&quot;auto&quot;</code>：</p>
<p>该参数用于指定模型在设备（如 CPU、GPU）上的分布方式。<code>&quot;auto&quot;</code> 表示让 <code>transformers</code> 库自动分配显卡，将模型拆分到多个 GPU 上，避免内存不足的问题。</p></blockquote>
<blockquote>
<p>注意，如果设置了<code>device_map=&quot;auto&quot;</code>，就不要在输入以下代码，这可能会覆盖之前的设备映射，导致模型被加载到单一设备上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 检查是否有可用的 GPU,将模型移动到 GPU 设备</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>例如，下面代码不可取</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">question</span> <span class="o">=</span> <span class="s2">&#34;介绍一下上海大学。&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></blockquote>

</section>


    <footer class="article-footer">
    
    
    <section class="article-category">
        
            <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="background-color: #2a9d8f; color: #fff;">
                大模型
            </a>
        
    </section>
    

    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            最后更新于 2025-04-14 11:49
        </span>
    </section></footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    
     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 xxcjw&#39;s bolg！
    </section>
    
    <section class="powerby">
        
            光终究会洒在你的身上，你也会灿烂一场！ <br/>
        
    </section>

    
    <section class="running-time">
        本博客已稳定运行
        <span id="runningdays" class="running-days"></span>
    </section>


</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script>
    let s1 = '2025-4-1'; 
    s1 = new Date(s1.replace(/-/g, "/"));
    let s2 = new Date();
    let timeDifference = s2.getTime() - s1.getTime();

    let days = Math.floor(timeDifference / (1000 * 60 * 60 * 24));
    let hours = Math.floor((timeDifference % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
    let minutes = Math.floor((timeDifference % (1000 * 60 * 60)) / (1000 * 60));

    let result = days + "天" + hours + "小时" + minutes + "分钟";
    document.getElementById('runningdays').innerHTML = result;
</script>
    </body>
</html>
