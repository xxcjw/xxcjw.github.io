[{"content":"连接实验室服务器 下载插件Remote-SSH 输入ssh user_name@server_IP 在插件下配置远程连接文件 1 2 3 4 5 6 7 8 9 Host xxcjw-A4000 HostName 58.199.157.119 Port 7722 User xxx Host xxcjw-A6000 HostName 10.0.65.51 Port 7722 User xxx VSCode远程连接实验室服务器（内部链接3）\nvscode连接Linux服务器-博客园\nVSCode SSH远程连接与删除\n免密登录 本质上就是本地生成公钥复制到虚拟机或者服务器的authorized_keys文件上即可。\n本机 cmd 输入ssh-keygen -t rsa，然后连续回车直到结束 服务器执行同样操作，之后进入.ssh这个文件夹进行操作 VsCode配置ssh免密远程登录\n","date":"2025-04-15T20:07:17+08:00","permalink":"https://xxcjw.github.io/p/vscode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/","title":"Vscode远程连接"},{"content":"LLM输出向量池化方式 LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。\n平均池化 平均池化会计算序列中所有向量的平均值，从而得到一个固定长度的向量。这种方式能有效综合序列里所有元素的信息。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = torch.mean(hidden_states, dim=1) # 在序列长度维度上求平均 print(pooled_output.shape) # 输出: torch.Size([2, 768]) 最大池化 最大池化会在序列中每个维度选取最大值，以此生成一个固定长度的向量。它能提取序列中的关键特征。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output, _ = torch.max(hidden_states, dim=1) # 在序列长度维度上取最大值 print(pooled_output.shape) # 输出: torch.Size([2, 768]) [CLS] 池化 也就是 CLS Token Pooling 。在使用预训练模型（像 BERT 这类）时，通常会在输入序列开头添加一个特殊的 [CLS] 标记。[CLS] 池化就是直接选取这个标记对应的输出向量作为整个序列的表示。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = hidden_states[:, 0, :] # 选取每个样本的第一个向量 print(pooled_output.shape) # 输出: torch.Size([2, 768]) 加权平均池化 也就是 Weighted Mean Pooling ，加权平均池化会依据每个向量的重要性为其分配不同的权重，再计算加权平均值。这样可以更有针对性地综合序列信息。\n1 2 3 4 5 6 7 8 9 10 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) # 假设 weights 是每个向量的权重，形状为 (batch_size, sequence_length) weights = torch.rand(2, 10) weights = torch.softmax(weights, dim=1) # 确保权重和为 1 weighted_hidden_states = hidden_states * weights.unsqueeze(-1) pooled_output = torch.sum(weighted_hidden_states, dim=1) print(pooled_output.shape) # 输出: torch.Size([2, 768]) Last 池化 也就是 Last Token 池化。选取序列中最后一个 token 对应的向量作为整个序列的表示。这种池化方式在某些场景下是有效的，例如在处理文本生成或者问答任务时，模型最后一个输出的 token 可能包含了整个序列处理后的关键信息。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = hidden_states[:, -1, :] # 选取每个样本的最后一个向量 print(pooled_output.shape) # 输出: torch.Size([2, 768]) Transformers库中相关类 在 transformers 库中，AutoModel 和相关类提供了一个统一的接口来加载不同类型的预训练模型。你可以通过 AutoModel 系列类来简化模型加载过程，而无需关心特定模型的类型。\n常用的 AutoModel类 AutoModel: 加载基础模型，适用于不带头的 Transformer 模型。\nAutoModelForSequenceClassification: 加载用于序列分类任务的预训练模型。\nAutoModelForTokenClassification: 加载用于标记分类（如命名实体识别）的模型。\nAutoModelForQuestionAnswering: 加载用于问答任务的模型。\nAutoModelForCausalLM: 加载用于自回归语言建模（生成任务）的模型。\nAutoTokenizer: 用于加载与模型匹配的标记器（tokenizer），负责文本预处理和编码。\nAutoFeatureExtractor: 用于加载与图像或其他类型输入匹配的特征提取器。\n基础模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoModel, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; model = AutoModel.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) outputs = model(**inputs) last_hidden_state = outputs.last_hidden_state # 获取模型的最后一层隐藏状态 print(\u0026#34;Last hidden state shape:\u0026#34;, last_hidden_state.shape) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # -----------------查看隐藏层形状---------------------- print(output.last_hidden_state.shape) # ------------获取输入转换后的 token------------------- tokens = tokenizer.tokenize(question) print(tokens) # ----------------获取输入转换后的 token--------------- # 方式一 print(inputs) # 方式二 input_ids = inputs[\u0026#34;input_ids\u0026#34;][0] print(input_ids) # -------------查看每个 token 对应的文字---------------- # 方式一 tokens = tokenizer.convert_ids_to_tokens(input_ids) print(\u0026#34;每个 token 对应的文字：\u0026#34;,tokens) # 方式二（） decoded_text = tokenizer.decode(inputs[\u0026#34;input_ids\u0026#34;][0], skip_special_tokens=True) print(\u0026#34;Decoded text:\u0026#34;, decoded_text) 1 2 3 # 查看大模型词表大小（也可以根据config.json查看） embeddings = model.get_input_embeddings() print(f\u0026#39;嵌入矩阵大小: {embeddings.weight.size()}\u0026#39;) 生成任务 用于加载 自回归语言模型 的类。自回归语言模型（Causal LM）在给定一些上下文时，会生成下一个可能的单词或 token。通常用于生成任务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from transformers import AutoModelForCausalLM, AutoTokenizer import torch model_name = \u0026#34;gpt2\u0026#34; model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) output = model.generate( **inputs, # 输入ID（token化后的文本） max_length=50, # 生成的最大长度 num_return_sequences=1, # 返回生成的序列数 no_repeat_ngram_size=2, # 防止生成重复的n-gram temperature=0.7, # 温度（控制随机性,值越大越具创造性） top_k=50, # 只考虑前k个最可能的词 top_p=0.95, # 采用 nucleus sampling（前p%的词） do_sample=True # 启用采样（避免确定性输出） ) generated_text = tokenizer.decode(output[0], skip_special_tokens=True) print(generated_text) top_k：从生成词汇中选择前 k 个最有可能的词进行采样。较小的值意味着选择更有限的词汇。\ntop_p：从概率累积值中选择前 p% 的词汇，控制采样的多样性。与 top_k 类似，也是用于限制候选词的范围。\ndo_sample：是否启用采样。如果为 True，模型将从预测的概率分布中随机采样生成下一个 token；如果为 False，它将选择概率最大的 token（即确定性生成）。启用采样机制可以增加生成文本的多样性，避免每次生成的文本都相同。\n序列分类任务 如果在做分类任务（例如情感分析），可以使用这个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import AutoModelForSequenceClassification, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; # 假设是二分类任务 model = AutoModelForSequenceClassification.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;,num_labels=2) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # 前向传播 outputs = model(**inputs) logits = outputs.logits # 获取分类任务的输出logits print(\u0026#34;Logits:\u0026#34;, logits) print(outputs.logits.shape) # 查看输出形状 模型的输出并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层。\n1 2 3 import torch predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) print(predictions) 标记分类任务 假设做的是命名实体识别任务，可以使用这个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoModelForTokenClassification, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; model = AutoModelForTokenClassification.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # 前向传播 outputs = model(**inputs) logits_token_class = outputs.logits # 获取每个token的分类结果 print(\u0026#34;Token classification logits:\u0026#34;, logits_token_class.shape) 问答任务类 通常用于从给定的上下文中回答问题。模型通过读取文本（context）和问题（question），然后返回答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from transformers import AutoModelForQuestionAnswering, AutoTokenizer import torch model_name = \u0026#34;distilbert-base-uncased-distilled-squad\u0026#34; model = AutoModelForQuestionAnswering.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;What is the capital of France?\u0026#34; context = \u0026#34;France is a country in Europe. Paris is its capital and the largest city.\u0026#34; inputs = tokenizer(question, context, return_tensors=\u0026#34;pt\u0026#34;).to(model.device) # `start_logits` 和 `end_logits` 分别表示答案的起始和结束位置 with torch.no_grad(): outputs = model(**inputs) start_position = torch.argmax(outputs.start_logits) # 答案起始位置 end_position = torch.argmax(outputs.end_logits) # 答案结束位置 # 7. 解码答案 answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(inputs[\u0026#39;input_ids\u0026#39;][0][start_position:end_position+1]) ) print(\u0026#34;Question:\u0026#34;, question) print(\u0026#34;Answer:\u0026#34;, answer) 在问答任务中，模型通过 start_logits 和 end_logits 来输出答案的位置。需要从模型输出的 tokens 中提取出 起始位置 和 结束位置 的答案，然后将其通过 convert_ids_to_tokens 转换为字符串。decode 方法主要用于生成文本，而不是处理直接的预测位置（例如起始和结束位置）。\n在问答任务中，我们不是生成每个 token，而是从模型输出的 start_logits 和 end_logits 中直接找到答案的位置。\n大模型词表嵌入 模型 词表嵌入维度 参数数量 GPT-2 1,024维 15亿 GPT-3 12,288维 1750亿 BERT-base 768维 1.1亿 BERT-large 1,024维 3.4亿 LLaMA2-7B 4,096维 70亿 LLaMA-2-13B 5,120维 130亿 LLaMA-3-8B 4,096维 80亿 deepseek-R1-14B 5120维 140亿 qwen2.5-7B 3,584维 70亿 Bloom-7B1 4,096维 71亿 Bloom-1B7 2,048维 17亿 Bloom- 1,024维 5.6亿 ","date":"2025-04-15T18:28:48+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-4llm%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"LLM系列-4：LLM的使用"},{"content":"pipelines Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？\n1 2 3 4 5 6 7 8 9 10 ## 文本生成任务示例： from transformers import pipeline generator = pipeline(\u0026#34;text-generation\u0026#34;, model=\u0026#34;distilgpt2\u0026#34;) results = generator( \u0026#34;In this course, we will teach you how to\u0026#34;, max_length=30, num_return_sequences=2, ) print(results) 其背后经过了三个步骤：\n预处理，将原始文本转换为模型可以接受的输入格式 将处理好的输入送入模型，根据具体任务进行推理生成 对模型的输出进行后处理，将其转换为人类方便阅读的格式 使用分词器进行预处理 因为神经网络模型无法直接处理文本，因此首先需要通过分词器 (tokenizer)将文本转换为模型可以理解的数字。\n我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作。因此我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n1 2 3 4 5 6 7 8 from transformers import AutoTokenizer checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) question = \u0026#34;魔都是哪个城市?\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;) print(inputs) 输出为：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#39;input_ids\u0026#39;: tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0] ]), \u0026#39;attention_mask\u0026#39;: tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] ]) } 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\ncheckpoint（检查点）是指在模型训练过程中，为了避免可能出现的意外情况，定期保存模型的状态，这个状态包含了所使用的模型名称、模型的参数、优化器的状态、训练步数等信息。\n注意，此时的到的只是分词后token对应的ID，并没有得到经过LLM编码的高维向量。\n将预处理好的输入送入模型 预训练模型的下载/加载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。\n1 2 3 4 from transformers import AutoModel checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; model = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务。\n相信你还没有理解，来看接下来这一段代码：\n1 2 3 4 from transformers import AutoModelForCausalLM checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; model = AutoModelForCausalLM.from_pretrained(checkpoint) 那它们之间的区别就在于Head部分，如果使用AutoModel就相当于只是用预训练模型中最基础的 Transformer 模块，得到的是大模型的高维嵌入向量。而如果使用AutoModelForCausalLM，就相当于多加了一个Head来完成特定的任务。\nTransformers 库封装了很多这样不同的结构，常见的有：\n*Model （返回 hidden states） *ForCausalLM （用于条件语言模型）【我用过的就是这种】 *ForMaskedLM （用于遮盖语言模型） *ForMultipleChoice （用于多选任务） *ForQuestionAnswering （用于自动问答任务） *ForSequenceClassification （用于文本分类任务） *ForTokenClassification （用于 token 分类任务，例如 NER） Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。\nAutoModel.from_pretrained(checkpoint)\n这个方法加载的是基础的预训练模型架构，不包含特定任务的头部（head）。基础模型通常会输出隐藏状态（hidden states），这些隐藏状态可以作为特征表示，用于下游任务的进一步处理。例如，在文本分类任务中，可以将这些隐藏状态输入到一个全连接层进行分类。 适用于那些需要对模型进行自定义扩展或微调的场景，可以根据自己的需求添加特定任务的头部，以适应不同的任务。 AutoModelForCausalLM.from_pretrained(checkpoint)\n这个方法加载的是专门用于因果语言模型（Causal Language Model，CLM）任务的预训练模型。因果语言模型的目标是根据前面的上下文预测下一个词，常用于文本生成任务等。该模型已经包含了一个语言建模头部（language modeling head），可以直接用于生成文本，无需额外添加特定任务的头部。 使用于文本生成任务类。 完整代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer import torch checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) #------------------使用 AutoModel 加载基础模型----------------- model = AutoModel.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) print(outputs.last_hidden_state.shape) #------使用 AutoModelForSequenceClassification 加载模型-------- model = AutoModelForSequenceClassification.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) print(outputs.logits) print(outputs.logits.shape) outputs.logits 是模型最终的未经过归一化处理的预测分数，如[-1.5607, 1.6123]，它们并不是概率值。只是outputs中的一部分信息。如果直接print(outputs)输出的更全。\n其输出分别为：\n1 2 3 4 5 6 # AutoModel输出，(batch_size, sequence_length, vocab_size) torch.Size([1, 4, 768]) # AutoModelForSequenceClassification输出,句子的情感分类值 tensor([[-1.5607, 1.6123], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) torch.Size([1, 2]) # 标签，positive 或 negative 补充：查看输出的最后隐藏状态形状\n1 2 3 4 last_hidden_states = outputs.last_hidden_state print(last_hidden_states.shape) #-------------------等价于------------------- print(outputs.last_hidden_state.shape) 对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。还要经过解码操作，比如以下代码可以得到生成的文本。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer import torch checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model.generate(**inputs) output_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(output_text) 可以看到， pipeline 背后的工作原理就是最底层的实现方式，它封装好了底层代码方便使用而已。下面会具体介绍组成 pipeline 的两个重要组件模型（Models 类）和分词器（Tokenizers 类）。\noutputs = model(**inputs)\n这种调用方式主要用于获取模型在输入数据上的中间结果，比如隐藏状态（hidden states）、未归一化的预测分数（logits）等。这些结果通常用于进一步的任务处理。\noutput = model.generate(**inputs)\n此调用方式专门用于文本生成任务。它会基于输入的上下文，使用特定的生成策略（如贪心搜索、束搜索等）来生成后续的文本序列。\n底层\u0026ndash;模型 在大部分情况下，我们都应该使用 AutoModel 来加载模型。这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。\n所有存储在HuggingFace上的模型都可以通过 Model.from_pretrained() 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录）。\n1 2 3 model = BertModel.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) model = BertModel.from_pretrained(\u0026#34;./models/bert/\u0026#34;) 如果本地没有下载该模型的权重文件，代码运行后会自动缓存下载的模型权重，默认保存到 ~/.cache/huggingface/transformers。\n底层\u0026ndash;分词器 由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为编码 (Encoding)，包含两个步骤：\n分词：使用分词器按某种策略将文本切分为 tokens； 映射：将 tokens 转化为对应的 token IDs（词表）； 分词器的加载与模型相似，使用 Tokenizer.from_pretrained()函数。同样地，在大部分情况下我们都应该使用 AutoTokenizer 来加载分词器。\n1 tokenizer = AutoTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) 分词器编码-方式1 可以通过 encode() 函数将上述两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 [CLS] 和 [SEP] 。\n1 2 3 4 5 6 7 8 9 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequence = \u0026#34;Using a Transformer network is simple\u0026#34; sequence_ids = tokenizer.encode(sequence) print(sequence_ids) 其输出为如下，其中 101 和 102 分别是 [CLS] 和 [SEP] 对应的 token IDs。\n1 [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102] 分词器编码-方式2 注意，上面这些只是为了演示。在实际编码文本时，最常见的是直接使用分词器进行处理，这样不仅会返回分词后的 token IDs，还自动包含了模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 token_type_ids 和 attention_mask。\n1 2 3 4 5 6 7 8 9 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequence = \u0026#34;Using a Transformer network is simple\u0026#34; sequence_text = tokenizer(sequence) print(sequence_text) 其输出为如下\n1 2 3 {\u0026#39;input_ids\u0026#39;: [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 也就是说，两者区别在于：\ntokenizer.encode 方法：返回的是编码后的 ID 列表 tokenizer 方法：更通用，返回一个包含更多信息的字典,如 attention_mask 、token_type_ids等 分词器解码 文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。通过 decode() 函数解码前面生成的 token IDs：\n1 2 3 4 5 6 7 8 9 10 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]) print(decoded_string) 输出为\n1 2 Using a transformer network is simple [CLS] Using a Transformer network is simple [SEP] 底层\u0026ndash;Padding 与 Attention Mask 在实际中，一个 batch 包含多个输入，每个输入有长有短，而输入张量必须是严格的二维矩形，维度为 (batch size,sequence length)，即每一段文本编码后的 token IDs 数量必须一样多。我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度。\n在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了。它且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。\n正如前面所说，在实际使用时，应该直接使用分词器来完成包括分词、转换 token IDs、Padding、构建 Attention Mask、截断等操作。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequences = [ \u0026#34;I\u0026#39;ve been waiting for a HuggingFace course my whole life.\u0026#34;, \u0026#34;So have I!\u0026#34; ] model_inputs = tokenizer(sequences, padding=\u0026#34;longest\u0026#34;, return_tensors=\u0026#34;pt\u0026#34;, max_length=8, truncation=True) print(model_inputs) 分词器的输出包含了模型需要的所有输入项。包括 input_ids和 attention_mask。\nPadding 操作 Padding 操作通过 padding 参数来控制：\npadding=\u0026quot;longest\u0026quot;： 将序列填充到当前 batch 中最长序列的长度； padding=\u0026quot;max_length\u0026quot;：将所有序列填充到模型能够接受的最大长度，例如 BERT 模型就是 512。（如果代码指定了max_length的长度，就不是 512 了） padding=True： 等同于 padding=\u0026quot;longest\u0026quot;； 1 model_inputs = tokenizer(sequences, padding=\u0026#34;max_length\u0026#34;) 截断操作 截断操作通过 truncation 参数来控制，如果 truncation=True，那么大于模型最大接受长度的序列都会被截断。此外，也可以通过 max_length 参数来控制截断长度。\n1 model_inputs = tokenizer(sequences, max_length=8, truncation=True) 返回格式 分词器还可以通过 return_tensors 参数指定返回的张量格式：设为 pt 则返回 PyTorch 张量；tf 则返回 TensorFlow 张量，np 则返回 NumPy 数组。\n1 model_inputs = tokenizer(sequences, padding=True, return_tensors=\u0026#34;pt\u0026#34;) 完整格式 综上所述，实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \u0026#34;xxx-models\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequences = [ \u0026#34;I\u0026#39;ve been waiting for a HuggingFace course my whole life.\u0026#34;, \u0026#34;So have I!\u0026#34; ] tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) print(tokens) output = model(**tokens) print(output.logits) 其输出为\n1 2 3 4 5 6 7 8 9 10 11 {\u0026#39;input_ids\u0026#39;: tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \u0026#39;attention_mask\u0026#39;: tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])} tensor([[-1.5607, 1.6123], [-3.6183, 3.9137]], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) 在 padding=True, truncation=True 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断。\n底层\u0026ndash;模型推理和输出 在将文本转换为模型可接受的输入格式后，就可以进行模型推理了。模型推理是指将预处理后的输入数据送入模型，让模型根据其内部的参数和结构进行计算，从而得到每个token包含上下文信息的高维嵌入。不同任务类型的模型会返回不同形式的输出。\n1 2 output = model.generate(**inputs, max_length=50, num_beams=5, no_repeat_ngram_size=2) output_text = tokenizer.decode(output[0], skip_special_tokens=True) 下面是一些参数解释：\n**inputs：这是一个解包操作，inputs 通常是一个字典，包含了input_ids、attention_mask等信息。通过 **inputs 可以将字典中的键值对作为关键字参数传递给 generate() 方法，自动处理 input 的所有参数 输出长度：指定生成文本的最大长度（以token为单位），当生成文本达到这个长度时，生成过程将停止 max_length 生成序列的最大总长度，包含了输入加输出的token总长度 max_new_tokens 仅关注新生成的token数量，而不考虑输入序列本身的长度 num_beams：束搜索（Beam Search）的束宽。束搜索会在每一步保留 num_beams 个最有可能的候选序列，可以提高生成文本的质量，从而找到更优的生成结果，但同时也会增加计算量和内存消耗 no_repeat_ngram_size：模型在生成过程中不允许出现重复的 n - gram 大小。n - gram 是指连续的 n 个标记组成的序列，例如当 n = 2 时，就是不允许连续的两个标记组成的序列重复出现。用于避免生成的文本中出现重复的短语或句子，提高生成文本的多样性和质量。例如，如果生成的文本中已经出现了 “the dog”，那么在后续的生成过程中就不会再出现 “the dog” 这个 2 - gram output[0]：model.generate() 方法返回的是一个包含多个生成序列的张量，output[0] 表示取第一个生成序列。在 num_return_sequences = 1 的情况下，通常只生成一个序列 skip_special_tokens=True：在解码过程中是否跳过特殊符号，如[CLS]、[SEP]。设置为 True 可以去除这些特殊标记，使生成的文本更加干净和易读 生成任务输出 1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModelForCausalLM,AutoTokenizer checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) input_text = \u0026#34;The future of AI is\u0026#34; inputs = tokenizer(input_text, return_tensors=\u0026#34;pt\u0026#34;, padding=True, truncation=True) output = model.generate(**inputs, max_length=50, num_beams=5, no_repeat_ngram_size=2) output_text = tokenizer.decode(output[0], skip_special_tokens=True) print(output_text) 对于文本生成任务（如使用 AutoModelForCausalLM），模型通过 generate() 方法直接生成 token IDs，需通过分词器解码为文本。\n隐藏状态提取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoModel,AutoTokenizer checkpoint = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModel.from_pretrained(checkpoint) # 获取最后一层隐藏状态 input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) hidden_states = outputs.last_hidden_state print(\u0026#34;Hidden states shape:\u0026#34;, hidden_states.shape) # 输出 torch.Size([2, sequence_length, 768]) 生成参数控制：max_length、num_beams（束搜索）、temperature（采样温度）等参数可调节生成结果的质量和多样性。\n完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch from transformers import AutoTokenizer, AutoModelForCausalLM model_name = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;,torch_dtype=torch.float16) question = \u0026#34;魔都是哪个城市?\u0026#34; inputs = tokenizer( question, max_length=512, padding=\u0026#39;max_length\u0026#39;, return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # inputs = tokenizer(question, padding=True, return_tensors=\u0026#39;pt\u0026#39;).to(model.device) output = model.generate( **inputs, num_beams=5, max_new_tokens=1000, no_repeat_ngram_size=2, # temperature=0.7, # 增加创造性 ) output_text = tokenizer.decode(output[0], skip_special_tokens=True) print(output_text) device_map=\u0026quot;auto\u0026quot;：\n该参数用于指定模型在设备（如 CPU、GPU）上的分布方式。\u0026quot;auto\u0026quot; 表示让 transformers 库自动分配显卡，将模型拆分到多个 GPU 上，避免内存不足的问题。\n注意，如果设置了device_map=\u0026quot;auto\u0026quot;，就不要在输入以下代码，这可能会覆盖之前的设备映射，导致模型被加载到单一设备上\n1 2 3 # 检查是否有可用的 GPU,将模型移动到 GPU 设备 device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model.to(device) 例如，下面代码不可取\n1 2 3 4 5 6 7 model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\u0026#34;auto\u0026#34;) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model.to(device) question = \u0026#34;介绍一下上海大学。\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(device) ","date":"2025-04-13T10:56:01+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/","title":"LLM系列-3：模型与分词器"},{"content":"Attention由来 ​ NLP 神经网络模型的本质就是对输入文本进行编码，然后基于概率的思想完成 NLU 或 NLP 任务。常规的做法是首先对句子进行分词（token），然后将每个 token 都转化为对应的词向量 (token embeddings)，这样文本就转换为一个由词语向量组成的矩阵$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$ 。\n在 Transformer 模型提出之前，对 token 序列 X 的常规编码方式是通过循环网络 (RNNs) 和卷积网络 (CNNs)。 $$ RNN：\\boldsymbol{y}_t = f(\\boldsymbol{y}_{t - 1}, \\boldsymbol{x}_t)\\\\ CNN：\\boldsymbol{y}_t = f(\\boldsymbol{x}_{t - 1}, \\boldsymbol{x}_t, \\boldsymbol{x}_{t + 1}) $$ RNN（例如 LSTM）的方案很简单，每一个 token 对应的编码结果通过递归地计算得到，如公式1；\n但是递归的结构导致其无法并行计算，因此速度较慢。而且 RNN 本质是一个马尔科夫决策过程，难以学习到全局的结构信息；\nCNN 则通过滑动窗口基于局部上下文来编码文本，例如核尺寸为 3 的卷积操作就是使用每一个词自身以及前一个和后一个词来生成嵌入式表示，如公式2；\nCNN 能够并行地计算，因此速度很快，但是由于是通过窗口来进行编码，所以更侧重于捕获局部信息，难以建模长距离的语义依赖；\n于是 Google 提出的Attention Is All You Need论文给出了第三种解决方案：直接使用 Attention 机制编码整个文本。相比 RNN 要逐步递归才能获得全局信息，而 CNN 实际只能获取局部信息，需要通过层叠来增大感受野，Attention 机制一步到位获取了全局信息。\n点积注意力 虽然 Attention 有许多种实现方式，但是最常见的还是 Scaled Dot-product Attention。包含 2 个主要步骤：\n计算注意力权重：使用某种相似度函数度量每一个 query 向量和所有 key 向量之间的关联程度。\n特别地，Scaled Dot-product Attention 使用点积作为相似度函数，这样相似的 queries 和 keys 会具有较大的点积。这会破坏训练过程的稳定性。因此注意力分数还需要乘以一个缩放因子来标准化它们的方差，然后用一个 softmax 标准化。这样就得到了最终的注意力权重.\n更新 token embeddings：将权重 与对应的 value 向量相乘以获得向量更新后的语义表示。\n$$ \\mathrm{Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\mathrm{softmax}\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d_k}}\\right)\\boldsymbol{V} $$下面通过 Pytorch 来实现：\n词嵌入得到 首先需要将文本分词为词语 (token) 序列，然后将每一个词语转换为对应的词向量 (embedding)。Pytorch 提供了 torch.nn.Embedding 层来完成该操作，即构建一个从 token ID 到 token embedding 的映射表。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from torch import nn from transformers import AutoConfig from transformers import AutoTokenizer # 加载预训练分词器 model_ckpt = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 输入文本并进行分词 text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) print(inputs.input_ids) # 加载模型参数配置并创建嵌入层 config = AutoConfig.from_pretrained(model_ckpt) token_emb = nn.Embedding(config.vocab_size, config.hidden_size) print(token_emb) # 输出文本对应的嵌入向量 inputs_embeds = token_emb(inputs.input_ids) print(inputs_embeds.size()) 输出为：\n1 2 3 tensor([[ 2051, 10029, 2066, 2019, 8612]]) Embedding(30522, 768) torch.Size([1, 5, 768]) 可以看到，BERT-base-uncased 模型对应的词表大小为 30522，每个词语的词向量维度为 768。Embedding 层把输入的词语序列映射到了尺寸为 [batch_size, seq_len, hidden_dim] 的张量。\n这里我们通过设置 add_special_tokens=False 去除了分词结果中的 [CLS] 和 [SEP]。\ninput_ids 是如何得到的：\n分词器已经针对bert-base-uncased模型进行了预训练，了解该模型词汇表中的所有词元（token）。分词器会将输入文本拆分成一个个词元。把每个词元映射到词汇表中的对应索引。bert-base-uncased模型有一个预先定义好的词汇表，每个词元都有唯一的整数索引。\ninput_ids是如何转换为向量的：\n加载模型的配置信息后，这些配置信息包含了模型的各种参数，例如词汇表大小（vocab_size）和隐藏层大小（hidden_size）等。 接下来，使用nn.Embedding创建一个嵌入层。它的作用是将每个词的索引（即input_ids中的整数）映射到一个固定长度的向量。这里的config.vocab_size表示词汇表的大小，也就是模型所能处理的不同词元的数量；config.hidden_size表示每个词元对应的嵌入向量的维度。在bert-base-uncased模型中，vocab_size通常为 30522，hidden_size为 768。 nn.Embedding在创建时会随机初始化一个形状为(vocab_size, hidden_size)的权重矩阵。这个矩阵中的每一行对应词汇表中一个词元的嵌入向量。例如，矩阵的第i行就是词汇表中索引为i的词元的嵌入向量。 1 根据大模型config.json配置文件中的hidden_size和vocab_size配置项可以得到 预训练模型不是已经训练好了词汇表的嵌入了吗，为什么nn.Embedding是随机初始化：\n使用nn.Embedding创建的嵌入层的确是随机初始化的，但加载的预训练模型也有一个预训练的词汇表嵌入矩阵的。\n当你直接使用nn.Embedding创建嵌入层时，这通常意味着你打算从头开始训练模型，或者在已有模型基础上针对特定任务进行微调。随机初始化可以让模型在训练过程中根据具体任务的数据和目标来学习合适的词嵌入表示。通用的预训练词嵌入可能无法很好地适应特定领域的任务，这时随机初始化并重新训练嵌入层可能会得到更好的效果。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModel model_ckpt = \u0026#34;bert-base-uncased\u0026#34; model = AutoModel.from_pretrained(model_ckpt) # 获取预训练的嵌入层 token_emb = model.embeddings.word_embeddings text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) inputs_embeds = token_emb(inputs.input_ids) print(inputs_embeds.size()) 注意力计算 注意力机制-第三章\n多头注意力 Multi-head Attention 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention。\n每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。因此Multi-head Attention 可以捕获到更加复杂的特征信息。 $$ {head}_i= \\mathrm{Attention}(\\boldsymbol{Q}\\boldsymbol{W}_i^Q, \\boldsymbol{K}\\boldsymbol{W}_i^K, \\boldsymbol{V}\\boldsymbol{W}_i^V) \\\\ {MultiHead}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})= \\mathrm{Concat}(\\mathrm{head}_1, \\dots, \\mathrm{head}_h) $$ 以下是单个注意力头实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 from torch import nn class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None): attn_outputs = scaled_dot_product_attention( self.q(query), self.k(key), self.v(value), query_mask, key_mask, mask) return attn_outputs 每个头都会初始化三个独立的线性层，负责将 Q,K,V 序列映射到尺寸为 [batch_size, seq_len, head_dim] 的张量，其中 head_dim 是映射到的向量维度。（将得到的输入文本对应的词嵌入通过线性层转换得到 Q,K,V ，因为是多头，所以嵌入大小由原来的embed_dim变为head_dim）\n实践中一般将 head_dim 设置为 embed_dim 的因数，这样 token 嵌入式表示的维度就可以保持不变，例如 BERT 有 12 个注意力头，因此每个头的维度被设置为 768/12=64 。\n最后只需要拼接多个注意力头的输出就可以构建出 Multi-head Attention 层了（这里在拼接后还通过一个线性变换来生成最终的输出张量）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None): x = torch.cat([ h(query, key, value, query_mask, key_mask, mask) for h in self.heads ], dim=-1) x = self.output_linear(x) return x 从输入文本到经过 Attention 后得到的词嵌入实现过程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from transformers import AutoConfig from transformers import AutoTokenizer # 加载预训练分词器 model_ckpt = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 分词、加载模型参数、初始化嵌入 text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) config = AutoConfig.from_pretrained(model_ckpt) token_emb = nn.Embedding(config.vocab_size, config.hidden_size) inputs_embeds = token_emb(inputs.input_ids) # Attention更新嵌入 multihead_attn = MultiHeadAttention(config) query = key = value = inputs_embeds attn_output = multihead_attn(query, key, value) print(attn_output.size()) 其输出为：\n1 torch.Size([1, 5, 768]) Transformer架构 标准 Transformer 结构，Encoder 负责将输入的词语序列转换为词向量序列，Decoder 则基于 Encoder 的隐状态来迭代地生成词语序列作为输出，每次生成一个词语。\nTransformer Encoder 除了多个 Attention 之外，还包括The Feed-Forward Layer、Layer Normalization、Positional Embeddings等结构。\n1、The Feed-Forward Layer\n实际上就是两层全连接神经网络，它单独地处理序列中的每一个词向量，也被称为 position-wise feed-forward layer。常见做法是让第一层的维度是词向量大小的 4 倍，然后以 GELU 作为激活函数。\n2、Layer Normalization\n负责将一批 (batch) 输入中的每一个都标准化为均值为零且具有单位方差；Skip Connections 则是将张量直接传递给模型的下一层而不进行处理，并将其添加到处理后的张量中。\n3、Positional Embeddings\n由于注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。如果预训练数据集足够大，那么最简单的方法就是让模型自动学习位置嵌入。\nTransformer Decoder Transformer Decoder 与 Encoder 最大的不同在于 Decoder 有两个注意力子层：\nMasked multi-head self-attention layer：确保在每个时间步生成的词语仅基于过去的输出和当前预测的词，否则 Decoder 相当于作弊了； Encoder-decoder attention layer：以解码器的中间表示作为 queries，对 encoder stack 的输出 key 和 value 向量执行 Multi-head Attention。通过这种方式，Encoder-Decoder Attention Layer 就可以学习到如何关联来自两个不同序列的词语。 参考 注意力机制-第三章\n","date":"2025-04-13T10:55:51+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-2attention%E8%AF%A6%E8%A7%A3/","title":"LLM系列-2：Attention详解"},{"content":"学技术前先读史，对后续有很帮助。下面介绍一下统计语言模型发展史。\nN-gram 模型 其预测下一个词语的核心思想如下：\n$$ P(w_n|w_1, w_2, \\ldots, w_{n - 1}) $$ 即下一个词语出现的概率取决于在句子中出现在它之前的所有词，但是，随着文本长度的增加，条件概率会变得越来越难以计算，因而在实际计算时会假设每个词语仅与它前面的N−1个词语有关，即\n$$ P(w_i | w_1, w_2, \\ldots, w_{i - 1}) = P(w_i | w_{i - N + 1}, w_{i - N + 2}, \\ldots, w_{i - 1}) $$ 这种假设被称为马尔可夫（Markov）假设，对应的语言模型被称为 N 元（N-gram）模型。比如N=2时基于前一个单词预测当前单词；N=3时考虑前两个单词来预测当前单词；而N=1时，模型实际上就是一个上下文无关模型。\n通过统计语料库中 n - gram 序列的频率，估计给定前 n - 1 个元素后下一个元素出现的概率 ，从而选择概率最高的词作为预测结果。例如，在语料库中统计 “我喜欢” 后面接不同词的频率，若 “苹果” 出现次数最多，当输入 “我喜欢” 时，就可能预测下一个词是 “苹果” 。\nNNLM 模型 NNLM 模型的思路与统计语言模型保持一致，它通过输入词语前面的N−1个词语来预测当前词。其结构如图所示：\n具体来说，模型首先从词表C中查询得到前面N-1个词语对应的词向量，然后将这些词向量拼接后输入到带有激活函数的隐藏层中，通过Softmax函数预测当前词语的概率。特别地，包含所有词向量的词表矩阵 C 也是模型的参数，需要通过学习获得。\nWord2Vec 模型 真正将神经网络语言模型发扬光大的模型，Word2Vec 模型提供的词向量在很长一段时间里都是自然语言处理方法的标配。Word2Vec 的模型结构和 NNLM 基本一致，只是训练方法有所不同，分为 CBOW (Continuous Bag-of-Words) 和 Skip-gram 两种。\n其中 CBOW 使用周围的词语w(t-2),w(t-1),w(t+1),w(t+2)来预测当前词w(t)，而 Skip-gram 则正好相反，它使用当前词w(t)来预测它的周围词语。Word2Vec 模型训练目标也更多地是为获得词向量服务。特别是同时通过上文和下文来预测当前词语的 CBOW 训练方法打破了语言模型“只通过上文来预测当前词”的固定思维。\n通过将词汇表中的单词转换成高维空间向量来表示语义关系，为自然语言处理提供了有效的文本数值化方式，推动了深度学习在 NLP 领域的应用。\n数据：比如窗口大小设为 2，对于句子 “我 爱 自然 语言 处理” ，以 “自然” 为中心词，上下文词是 “我”“爱”“语言”“处理” 初始化：为每个单词初始化一个随机词向量 计算预测：CBOW 将上下文词向量相加输入隐藏层再到输出层，Skip - gram 则是将目标词向量输入隐藏层再到输出层，输出层都用 softmax函数计算词汇表中每个词作为预测结果的概率 更新权重：根据预测结果和真实标签计算损失，通过反向传播算法使损失最小化，不断迭代优化词向量 ELMo 模型 为了更好地解决多义词问题，提出了 ELMo 模型（Embeddings from Language Models）。与 Word2Vec 模型只能提供静态词向量不同，ELMo 模型会根据上下文动态地调整词语的词向量。\n具体来说，ELMo 模型首先对语言模型进行预训练，使得模型掌握编码文本的能力；然后在实际使用时，对于输入文本中的每一个词语，都提取模型各层中对应的词向量拼接起来作为新的词向量。ELMo 模型采用双层双向 LSTM 作为编码器，从两个方向编码词语的上下文信息，相当于将编码层直接封装到了语言模型中。\n训练完成后 ELMo 模型不仅学习到了词向量，还训练好了一个双层双向的 LSTM 编码器。对于输入文本中的词语，可以从第一层 LSTM 中得到包含句法信息的词向量，从第二层 LSTM 中得到包含语义信息的词向量，最终通过加权求和得到每一个词语最终的词向量。\n但是 ELMo 模型存在两个缺陷：首先它使用 LSTM 模型作为编码器，而不是当时已经提出的编码能力更强的 Transformer 模型；其次 ELMo 模型直接通过拼接来融合双向抽取特征的做法也略显粗糙。\n不久之后，将 ELMo 模型中的 LSTM 更换为 Transformer 的GPT、Bert模型就出现了。\nBERT 模型 2018 年底随着 BERT 模型（Bidirectional Encoder Representations from Transformers）的出现，这一阶段神经网络语言模型的发展终于出现了一位集大成者。\nBERT 模型采用和 GPT 模型类似的两阶段框架，首先对语言模型进行预训练，然后通过微调来完成下游任务。但是，BERT 不仅像 GPT 模型一样采用 Transformer 作为编码器，而且采用了类似 ELMo 模型的双向语言模型结构。因此 BERT 模型不仅编码能力强大，而且对各种下游任务，BERT 模型都可以通过简单地改造输出部分来完成。\n但是 BERT 模型的优点同样也是它的缺陷，由于 BERT 模型采用双向语言模型结构，因而无法直接用于生成文本。\n在 BERT 模型取得成功之后，在其基础上又提出了诸如RoBERTa等改良模型，其中具有代表性的就是微软提出的UNILM模型（可以使Bert具备生成能力），它把 BERT 模型的 MASK 机制运用到了一个很高的水平。\nBERT不能直接用于生成文本，原因如下：\n掩码语言模型（Masked Language Model, MLM）：BERT 通过随机遮盖文本中 15% 的 token，让模型根据上下文来预测被遮盖的词。这种训练方式让模型学会融合双向上下文信息理解语义，使其在理解上下文语义时表现优异。\n但也导致了一个关键问题——生成任务需要单向、逐步的预测。通常是自回归的，即逐词生成，每个新词的预测只能依赖已生成的左侧上下文。而BERT的双向机制在生成时会引入未来信息，导致信息泄露，与实际生成过程矛盾。不像自回归模型，如 GPT 那样从左到右依次根据前面生成的内容预测下一个词，所以难以直接用于文本生成任务。\nEncoder-only模型：BERT 架构只采用了 Transformer 的编码器部分，没有使用Transformer 解码器那样具备处理生成任务的结构设计。生成任务通常需要解码器逐步生成输出，按照顺序逐步生成下一个词，而 BERT 编码器缺乏这种从左到右、顺序生成的机制 。\n补充知识 LLM分类 一般分为三种：自回归模型、自编码模型和序列到序列模型。\n自回归（Autoregressive model）模型：decoder-only模型。采用经典的语言模型任务进行预训练，即给出上文，预测下文，对应原始Transformer模型的解码器部分，其中代表模型是GPT系列。模型一般会用于NLG的任务，如文本生成。 自编码（AutoEncoder model）模型：encoder-only模型。采用句子重建进行预训练，即预先通过某种方式破坏句子，掩码或打乱顺序，希望模型将被破坏的部分还原，对应原始Transformer模型的编码器部分，其中代表模型是BERT系列。与自回归模型不同，模型既可以看到上文信息，也可以看到下文信息，由于这样的特点，模型往往适用于NLU的任务，如文本分类、阅读理解等。 序列到序列（Sequence to Sequence Model）模型：则是同时使用了原始的编码器与解码器。这种模型最自然的应用便是文本摘要、机器翻译等任务，事实上基本所有的NLP任务都可以通过序列到序列解决。 NLG - 自然语言生成；NLU - 自然语言理解\nTransformer结构 标准的 Transformer 模型主要由两个模块构成：\nEncoder：负责理解输入文本，为每个输入构造对应的语义表示 Decoder：负责生成输出，使用 Encoder 输出的语义表示结合其他输入来生成目标序列 这两个模块可以根据任务的需求而单独使用：\n纯 Encoder 模型：适用于只需要理解输入语义的任务，例如句子分类、命名实体识别 纯 Decoder 模型：适用于生成式任务，例如文本生成 Encoder-Decoder 模型或 Seq2Seq：适用于需要基于输入的生成式任务，例如翻译、摘要 Transformer 模型本来是为了翻译任务而设计的。在训练过程中，Encoder 接受源语言的句子作为输入，而 Decoder 则接受目标语言的翻译作为输入。在 Encoder 中，由于翻译一个词语需要依赖于上下文，因此注意力层可以访问句子中的所有词语；而 Decoder 是顺序地进行解码，在生成每个词语时，注意力层只能访问前面已经生成的单词。\n例如，假设翻译模型当前已经预测出了三个词语，我们会把这三个词语作为输入送入 Decoder，然后 Decoder 结合 Encoder 所有的源语言输入来预测第四个词语。\n实际训练中为了加快速度，会将整个目标序列（真实值）都送入 Decoder，然后在注意力层中通过 Mask 遮盖掉未来的词语来防止信息泄露。因为若按顺序一个词一个词输入，效率很低。一次性输入整个目标序列，模型可以并行处理计算，大大加快训练速度 。\n其中，Decoder 中的第一个注意力层关注 Decoder 过去所有的输入，而第二个注意力层则是使用 Encoder 的输出，因此 Decoder 可以基于整个输入句子来预测当前词语。这对于翻译任务非常有用。\n在 Encoder/Decoder 的注意力层中，我们还会使用 Attention Mask 遮盖掉某些词语来防止模型关注它们，例如为了将数据处理为相同长度而向序列中添加的填充 (padding) 字符。\n总结 \u0026amp; 参考 可以看出，预测下一个词模型的基本范式都是学习融合上下文信息的每个词的嵌入表征，然后通过输出层（通常接 softmax 函数 ，维度大小是词库大小）计算词汇表中每个词作为下一个词的概率，从中选择概率最高的词或通过一定采样策略（如多项分布采样 ）确定下一个词。\n以 GPT 为例，它是自回归模型，从左到右依次根据已有的词预测下一个词 ；而 BERT 虽然主要用于完形填空任务（掩码语言模型），但也可以在微调后用于预测下一个词等生成任务 。\nTransformers-第一章\nTransformers-第二章\n","date":"2025-04-12T12:04:52+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-1nlp%E7%9A%84%E5%8F%91%E5%B1%95/","title":"LLM系列-1：NLP的发展"},{"content":"git设置代理 开vpn之后，当你还在为能正常打开Github、Youtobe而窃喜时，不知道你是否也遇到过这种情况：\n但当你使用git工具将本地代码文件上传Github时，却总是出现Failed to connect to github.com port 443: Timed out或者OpenSSL SSL_read: Connection was reset, errno 10054等git push失败的情况，让人火大。 或者发现git clone命令速度特别慢，有时还经常卡掉，这很让人着急。 总之，就四个字：失败、火大！于是搜各种教程，经过各种折腾，终于找到解决方案，归根结底还是因为代理设置的不正确！！！本文记录一下解决的方案。\n问题描述 简单来说，就是所使用的vpn正常，但是上传或下载Github文件时就很慢很卡，甚至连接不上出现报错情况。如果遇到这种情况，可以使用下面的方法解决。\n如果检查自己的vpn是否正常呢？有两种简单办法\n最简单的当然就是打开一个国外网站，比如YouTube等，如果能打开，证明梯子没问题； 另一种，就是win+r打开命令行，输入ping github.com看看能不能连上要使用的Github； 解决方法 遇到上述问题，就是代理设置不正确。可以通过以下命令查看和设置理。\n1、代理查询\n查看自己是否以前设置过代理，在CMD中输入以下命令进行查询\n1 2 git config --global http.proxy git config --global https.proxy 如果没有记录显示，则说明本电脑还没有配置过代理，否则需要先将代理进行删除，再进行后续的重新添加。\n2、代理取消\n如果执行上述代码之后，发现有代理存在，则执行下述命令，先将代理取移除\n1 2 git config --global --unset http.proxy git config --global --unset https.proxy git config 是 Git 用于配置和管理各种参数的命令，可以设置和修改与 Git 相关的配置选项。\nhttp.proxy 是用于设置 Git 在进行 HTTP 或 HTTPS 连接时使用的代理服务器。\n3、代理设置\n首先需要获取当前所使用vpn的代理服务器地址，格式为：127.0.0.1:xxxx（我的是7879）。这里一定要注意，不要照抄网上的端口号，需要根据自己的服务器进行修改，否则报错。得到代理服务器地址之后，使用如下命令配置代理\n1 2 git config --global https.proxy 127.0.0.1:7897 git config --global http.proxy 127.0.0.1:7897 如何知道自己的代理服务器地址呢？两种办法：\n一是通过自己的vpn查看，打开自己的vpn-设置-端口设置/端口号的选项，这个就是代理服务器的端口 打开电脑的控制面板-网络和Internet-Internet选项-连接-局域网设置，然后就可以看到代理服务器的地址了，见图 通过这两种方式得到的应该是一样的。\n然后就配置成功了！就可以成功上传和下载文件了。需要注意的是，有少数人刚配置后，效果显著，但用一段时间之后，发现git push上传文件时仍然失败，这时候按照第二个步骤将代理重新取消掉就好了，如果有过一段时间又不行了，在重新配置就行，这样反复操作可以解决，但为什么这样不得而知。\n原理介绍 代理服务器就是你的电脑和互联网的中介。当您访问外网时 , 你的请求首先转发到代理服务器，然后代理服务器替你访问外网，并将结果原封不动的给你的电脑，这样你的电脑就可以看到外网的内容啦。\n使用vpn后无法正常上网 解决方法 很多同学使用vpn访问外网，但当关闭VPN后，却发现没有办法正常连接到互联网了。解决方法如下：\n1 控制面板--\u0026gt;网络和Internet--\u0026gt;Internet选项--\u0026gt;弹出的Internet属性窗口--\u0026gt;连接--\u0026gt;局域网设置 将为LAN使用代理服务器（这些设置不用于拨号或VPN连接）前面的√取消掉可以了（开vpn的时候勾上是正常的，如果关掉vpn后能正常上网就不用取消）。到此为止不出意外就可以正常上网啦！\n原因分析 一句话：可能是由于VPN代理软件修改了电脑IP地址的获取方式。\n通常情况下，计算机的IP地址是通过动态主机配置协议（DHCP）从您的路由器或网络提供商的服务器上获取的。使用VPN代理软件时，它会在您的计算机和互联网之间创建一个安全的连接。在这种连接中，VPN软件会通过在您的计算机和VPN服务器之间建立虚拟通道来代理您的网络流量。当您的网络流量通过VPN服务器时，服务器会为您的流量分配一个新的IP地址。\n通过这些步骤，VPN代理软件成功修改了您计算机的IP地址获取方式。现在，计算机使用的是VPN服务器分配的IP地址，而不是原本的IP地址。这样做可以帮助您隐藏真实的IP地址并实现匿名上网，同时提供加密保护，确保您的网络流量在传输过程中得到安全保护。同时，当我们关闭VPN后，由于电脑IP地址任然为VPN分配的IP地址（而此时我们已经关闭了代理服务器的连接），这就导致我们无法正常访问互联网了。\n参考：使用vpn/代理后无法正常上网\n","date":"2025-04-10T20:18:22+08:00","permalink":"https://xxcjw.github.io/p/%E4%BB%A3%E7%90%86-%E8%81%94%E7%BD%91%E9%97%AE%E9%A2%98/","title":"代理 \u0026 联网问题"},{"content":"最常用 1 2 3 4 5 6 7 8 9 10 win + l #快速锁屏 ctrl + f #查找 alt + tab #切换页面 win + v #粘贴板 win + space #切换中英文 shift + delete #完全删除文件 tab #选中多行按tab一起缩进 shift + tab #多行取消缩进 ctrl + alt + . #黑屏时可关机、打开任务管理器 ctrl + alt + delete #黑屏时可关机、打开任务管理器 截图快捷键\n1 2 3 win + shift + s #自带截图 alt + z #截图软件截图 alt + a #微信截图 命令行类命令 系统信息查看类 1 2 3 4 5 6 nvidia-smi #查看gpu使用情况 ipconfig #显示ip地址信息 watch -n 1 nvidia-smi #动态追踪查看显存占用 ping xxx #查看本机能否连通某网址 free -h #查看服务器内存（Mem-物理内存，Swap-交换空间） df -h #查看服务器外存/硬盘使用情况（use%表示使用率） 会话管理指令 1 2 3 4 5 screen -S xxxx #创建会话，如screen -S test ctrl+a d #退出会话同时保持程序运行(按住Ctrl，依次再按a,d) screen -r xxxx #恢复会话，如screen -r test exit / ctrl+d #关闭会话，会提示：screen is terminating screen -ls #列出当前存在的会话列表 文件目录操作类 1 2 3 4 5 6 7 8 9 10 ⬆/⬇ #切换历史命令 cd ./xx #切换到某某文件夹 pwd #显示当前工作目录的完整路径 ls #列出当前目录中的文件（-l、-a） source xxx #在当前会话中执行某个脚本，使其立即生效 mkdir xxx #创建一个新目录 rmdir xxx #删除空目录（-p 一次删除多个空文件夹；-r 递归删除） rm xxx #删除不为空的文件或目录（-r递归删除，常用在目录删除） cp source destination #复制文件或目录到指定文件夹下 mv source destination #移动文件或目录 进程管理类 1 2 3 4 5 6 7 8 ps -p \u0026lt;PID\u0026gt; #查看进程信息（看不到使用者） ps -f -p \u0026lt;PID\u0026gt; #查看进程详细信息（f表示full，全部信息） kill \u0026lt;PID\u0026gt; #根据进程号杀死进程 kill -9 \u0026lt;PID\u0026gt; #强制杀死进程 top #实时监控系统中各个进程的资源占用情况 shutdown -h now #服务器立刻关机 shutdown -h 10 #10 分钟后自动关机 shutdown -r now #重启（=reboot也是重启） 文本处理类 1 2 3 4 5 grep \u0026#34;Hello\u0026#34; xx.txt #文本搜索指令，从txt文件中搜索包含指定字符串的行 grep -i \u0026#34;hello\u0026#34; xx.txt #忽略大小写搜索 grep -r \u0026#34;hello\u0026#34; /data #递归搜索，在一个目录搜索包含指定字符串的行 grep -n \u0026#34;Hello\u0026#34; xx.txt #显示匹配行的行号 grep -inr \u0026#34;hello\u0026#34; . #组合使用，点代表在当前目录下搜索 权限管理类 1 2 3 4 5 6 7 ls -l #列出当前目录中的文件以及权限（别称：ll） --------------------通过符号修改权限------------------- chmod u=rwx,g=rx,o=x xxx #对xxx文件夹的所有者、所属组和其他人修改权限 chmod g+w test.txt #对txt文件的所属组添加写的权限 chmod a-x test #对test文件夹的所有用户去除执行的权限（a表示所有人，即u+g+o） --------------------通过数字修改权限------------------- chmod 751 test #等价于chmod u=rwx,g=rx,o=x xxx（7 = rwx = 4+2+1，以此类推） 浏览器快捷键 1 2 3 4 5 ctrl + l #定位到地址栏（可通过设置后，通过应用快速打开某程序） ctrl + t #新建标签页 ctrl + shift + t #重新打开刚才关闭的标签页 ctrl + tab #切换标签页 ctrl + n #打开一个新的浏览器窗口 typora快捷键 1 2 3 4 5 6 7 ``` #插入代码块 $$ #插入公式 enter/shift + enter #不一样的换行方式 ctrl + t #新建表格 ctrl + enter #表格插入行 ctrl + shift + back #删除表格指定行 []() #插入链接（前面加个“!”就是插入图片） vim快捷键 1 2 3 :q #退出（如果有更改不会保存） :wq #保存并退出 :q! #强制退出并丢弃未保存的更改 其他 1 ctrl+shift+space #vscode参数 补充 \u0026amp; 参考 chmod 命令 （change mode）：修改文件或目录的权限\nchown 和 chgrp：修改文件拥有者或所属组\n格式：chmod [选项] 权限 文件名\n权限的本质就是可以干什么，权限=角色（user、group、others）+目标权限属性（rwx）。\nLinux下包含两种用户：超级用户（root）和普通用户。在 Linux 中的每个用户必须属于一个组，不能独立于组外 在 Linux 中每个文件有所有者、所在组、其它组的概念 所有者-User：文件或目录的创建者 所在组-Group：文件或目录所属的用户组 其他-Others：除了文件所有者和用户组之外的所有人 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 权限的基本介绍(0-9位说明)： 第 0 位确定文件类型(d, - , l , c , b) 第 1-3 位确定所有者（该文件的所有者）拥有该文件的权限。---User 第 4-6 位确定所属组（同用户组的）拥有该文件的权限，---Group 第 7-9 位确定其他用户拥有该文件的权限 ---Other 文件类型： - 普通文件；d 目录；l 软链接；c 字符设备；b 硬盘 rwx权限详解： [ r ]代表可读: 可以读取,查看 [ w ]代表可写: 可以修改,如果针对的是文件，则不可以删除；如果针对的是目录，则可以删除 [ x ]代表可执行：可以被执行 可用数字表示为: r=4,w=2,x=1 因此 rwx=4+2+1=7 数字含义： 如果是目录，则表示子目录个数；如果是文件，则表示硬链接数量 之后两项： 分别表示文件或目录的所有者和所属组 数字含义： 文件大小(字节)，如果是文件夹，显示 4096 字节 剩余项： 最后修改日期和文件名称 快捷键大全网址：\n快捷键速查表 - 星云导航\n快捷键备忘录\n","date":"2025-04-10T16:30:06+08:00","permalink":"https://xxcjw.github.io/p/%E7%94%B5%E8%84%91%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4/","title":"电脑相关指令"},{"content":"文章插入图片 如果想要在文章插入图片，有两种方式：\n方式一： 1 ![name](pic.png) 缺点是这种方式插入的图片不可以调节大小，默认占满页面宽度，很不美观。\n方式二： 1 \u0026lt;img src=\u0026#34;1.png\u0026#34; width=\u0026#34;80%\u0026#34; align=\u0026#34;middle\u0026#34; style=\u0026#34;zoom:60%;\u0026#34; /\u0026gt; 如果是在markdown文件里，是居中且缩放到原图的60%大小，但在网页上居中不起作用。\n方式三： 1 \u0026lt;center\u0026gt;\u0026lt;img src=\u0026#34;1.png\u0026#34; width=\u0026#34;80%\u0026#34; align=\u0026#34;middle\u0026#34; style=\u0026#34;zoom:60%;\u0026#34; /\u0026gt;\u0026lt;/center\u0026gt; 这样就可以实现图片居中且缩放成任意比例。\n友链布局修改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 title: 友链 slug: \u0026#34;links\u0026#34; layout: \u0026#34;links\u0026#34; menu: main: weight: -50 params: icon: link comments: false links: - title: GitHub description: 全世界最大的代码托管和开源项目平台. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png sites: - title: GitHub description: 全世界最大的代码托管和开源项目平台. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png 现在想要实现的效果如图所示，即按照类别来划分友链，如何实现呢？\n首先，在content/page/links/index.md文件中新建一个链接名，如links、sites等，然后把想要插入的友链放入； 然后，在layouts/page/links.html文件中写入如下代码。其中，xxxs就是上面的链接名，如links、sites等；类别名就是想要的分类名称，如推荐大佬、科技\u0026amp;论坛； 1 2 3 4 5 6 7 \u0026lt;header\u0026gt; \u0026lt;h2 class=\u0026#34;section-title\u0026#34; style=\u0026#34;margin-bottom: -20px;\u0026#34;\u0026gt;类别名\u0026lt;/h2\u0026gt; \u0026lt;/header\u0026gt; {{ if .Params.xxxs }} {{ partial \u0026#34;article/components/xxxs\u0026#34; . }} {{ end }} 最后，在layouts/partials/article/components文件下新建xxxs.html文件，文件不存在的就复制一下其他的（代码如下），然后将旧链接名修改为新建的链接名； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026lt;div class=\u0026#34;article-list--compact links\u0026#34;\u0026gt; {{ range $i, $hugo := .Params.hugos }} \u0026lt;article\u0026gt; \u0026lt;a href=\u0026#34;{{ $hugo.website }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;article-details\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;article-title\u0026#34;\u0026gt; {{- $hugo.title -}} \u0026lt;/h2\u0026gt; \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ with $hugo.description }} {{ . }} {{ else }} {{ $hugo.website }} {{ end }} \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; {{ with $hugo.image }} {{ $permalink := . }} {{ with ($.Resources.GetMatch (printf \u0026#34;%s\u0026#34; (. | safeURL))) }} {{ $permalink = .RelPermalink }} {{ end }} \u0026lt;div class=\u0026#34;article-image\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ $permalink }}\u0026#34; loading=\u0026#34;lazy\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; {{ end }} \u0026lt;/div\u0026gt; 文章内链跳转 写文章的时候总有一个需求就是关联之前写的文章，那么在hugo中应该要怎么用呢？\n方法一：markdown语法\n使用[]()创建链接，比如hugo常用命令。此方法缺点就是文章标题和链接变了，所有地方都需要手动修改。在测试的时候甚至会出现点击后会出现404找不到界面。\n方法二：hugo提供的ref功能\n用法如下所示（哈哈去掉），此方法的好处就是文章的链接变了，这里会跟着变的，不需要手动修改。缺点是文章的标题不能同步变化。hugo常用命令\n建议使用绝对路径（根目录为content目录），而非相对路径，否则容易出错 只有所引用的文件与当前文件在同一文件夹下时可以使用相对路径的方式（只有文件名） 1 [hugo常用命令]({{哈哈\u0026lt; ref \u0026#34;/post/2025-04/hugo快捷命令.md\u0026#34; \u0026gt;}}) 文章内容管理 本文按时间线进行内容管理，其目录结构如下，需要注意以下几点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 content/ └── post ├── 2025-01 │ ├── hugo博客搭建.md │ ├── git常用命令.md │ └── tools文件夹 │ ├── 1.jpg │ ├── 2.jpg │ ├── 3.png │ ├── index.md │ └── 4.png │ ├── 2025-02 │ ├── 博客搭建1.md │ ├── 博客搭建2.md │ ├── 博客搭建3.md │ ├── 博客搭建4.md │ ├── 博客搭建5.md │ ├── 博客搭建6.md │ ├── 博客搭建7.md │ ├── 博客搭建8.md │ ├── 博客搭建9.md │ └── 人生意义.md └── 博客搭建10 ├── index.md └── 博客记录说明 ├── 1.png ├── 2.png ├── 3.png ├── 4.png ├── 5.png ├── 6.png ├── 7.png ├── 8.png ├── 9.png ├── index.zh-cn.md └── index.zh-tw.md post目录下先按照年份建立子目录， 也可以按照 年/月，或者 年/月/日建立 没有图片的文章直接在一级子目录下保存 有图片的多建一级目录, md文件命名为index.lang.md(使用其他命名文件，图片不能显示), 图片放在同一目录/子文件夹 不同语言的md文件，放在一起，统一用不同的index.lang.md区分， 比如 index.zh-cn.md, index.zh-tw.md Bug问题 输入公式时（$$$$），必须空一行才能正常显示（markdown换行enter是区分段落，源代码默认会有一个空行，粘贴复制过来的文字可能是shift+enter换行，导致不是enter换行）。行内插入公式时使用$xxx$表示 加粗时，加粗的内容不能以符号结尾，否则在渲染时出错 Markdown 开源文档\n致谢 1、博客搭建教程\n简单来说，分为几步：\n下载解压hugo压缩包，cmd 打开命令行窗口，输入hugo new site xxxx创建文件（这里是dev） 复制hugo.exe文件到dev文件内 进入hugo官网，下载stack主题压缩包，将文件解压存储在dev\\themes文件下 将 exampleSite 样例数据中的 Content 和 hugo.yaml 复制到主文件夹中，并删掉hugo.toml 在dev文件夹，输入hugo server -D，发现已经正确显示 Github Action自动部署 莱特雷-letere\nHugo+Github博客部署\n使用 Hugo + Github Pages 部署个人博客\n2、修改美化\n首先修改dev文件夹下的hugo.yaml文件 其次，主要修改的是assets、layouts、static文件夹，换新电脑时可以直接复制过去 assets/scss/custom.scss文件修改的是大部分美化样式 layouts/_default/single.html文件修改的是：如果文章有目录，则把左侧边栏换为一个返回主页的按钮，如果文章没有目录，则启用左侧边栏 layouts/page文件夹和layouts/partials文件夹修改的是友链分类相关的文件 layouts/index.html文件修改的是：添加了首页欢迎字符面板 themes/hugo-theme-stack/layouts/partials/article/components/details.html文件修改的是：添加文章字数统计功能；而该文件夹下footer.html修改的是：添加了一行代码以及新建categories.html文件实现在文章末尾也显示分类标签 themes/hugo-theme-stack/layouts/partials/footer/footer.html删除了页脚信息，更简洁 themes/hugo-theme-stack/layouts/partials/article/components/related-content.html整个文件删除或注释，这样文章底部就不会显示相关文章了 assets/scss/partials文件夹以及layouts/partials/footer文件夹修改内容：添加博客运行时间以及样式 Stack 魔改美化-Naive Koala-2篇\nHugo Stack 主题美化-阿琦同学-很全-2篇\nL1nSn0wの小窝 - Stack主题的自定义\nHugo的Stack主题美化零碎-wfg\n使用 Hugo 对博客的重建与 Stack 主题优化记录-Exnadio\u0026rsquo;s Blog\nStack 主题的自定义-折腾日记\nHugo Stack 主题配置与使用 | Bore\u0026rsquo;s Notes\nHugo博客 | stack主题修改第一站-墨纹\nHugo Stack主题装修笔记-博客运行时间\nLeonus 博客\n张洪Heo - 分享设计与科技生活\n安知鱼 - 生活明朗 万物可爱\nHugo Theme Cybe\n3、图标网站\n打开stack官方文档Stack 官方文档，在``Custom Menu`栏下可以找到推荐的图标网站\nIcons网站\n","date":"2025-04-08T22:53:36+08:00","permalink":"https://xxcjw.github.io/p/stack%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9/","title":"Stack主题修改"},{"content":"创建文章 1 hugo new post/xxx/xxx.md 预览网站 1 2 hugo server -D hugo server 两者都是用于启动 Hugo 本地开发服务器的命令，区别主要在于是否包含草稿文章方面：\nhugo server：启动一个本地开发服务器，该服务器会对项目文件的变更进行监控，一旦文件有改动，就会实时重新生成网站。不过，在生成网站内容时，它默认不会包含草稿文章。 hugo server -D：-D 是 --buildDrafts 的简写。样会启动本地开发服务器，实现对文件变更的监控和网站的实时更新与刷新。不同的是，在生成网站时会将草稿文章也包含进去。 清除缓存 1 2 3 hugo --gc hugo --gc --minify hugo server --gc -D 当预览修改文章时，发现页面没变化，可以尝试清除缓存并重新构建：\nhugo --gc：--gc 标志的作用是在构建过程中执行垃圾回收（Garbage Collection），即清除不再使用的缓存文件，这样能释放磁盘空间。如果对网站配置或内容进行了大量修改，旧的缓存不再使用，该命令能保证生成的网站是最新状态。 hugo --gc --minify：除了具备执行垃圾回收功能外，--minify 标志还会对生成的 HTML、CSS、JavaScript 等文件进行压缩。可以减小文件大小，从而提升网站的加载速度。适合在准备将网站部署到生产环境时使用，能提供更流畅的访问体验。 hugo server --gc -D：启动一个实时预览服务器，同时执行垃圾回收，确保使用的是最新的缓存。适合在开发过程中使用，当频繁修改文章内容，并希望随时预览草稿文章在内的网站效果时最方便。 发布文章 1 2 3 4 5 git init git add . git commit -m \u0026#34;xxx\u0026#34; git remote add origin {github仓库地址} git push -u origin main git init：初始化一个新的 Git 仓库，会将当前目录变为一个Git仓库。并生成一个名为 .git 的隐藏目录，包含了 Git 管理项目所需的各种配置文件和数据结构。 git add .：用于把文件的修改添加到暂存区。可以在暂存区（Git中的一个中间区域）组织和规划哪些修改要包含在下次提交中。. 代表当前目录下的所有文件和子目录。 git commit -m \u0026quot;xxx\u0026quot;：用于将暂存区的修改保存到本地仓库的历史记录中，-m 标志后面跟着的 \u0026quot;xxx\u0026quot; 是本次提交的说明信息。 git remote add origin xxx：用于管理与远程仓库（一般是GitHub创建的远仓）的连接。add 子命令用于添加一个新的远程仓库。origin 是远程仓库的默认名。这会将本地仓库与该 GitHub 仓库建立连接。 git push -u origin main：用于将本地仓库的提交推送到远程仓库。-u会将本地的 main 分支与远程仓库的 main 分支关联起来，这样在后续的推送操作中，你只需要执行 git push 即可。origin 是远程仓库的别名，main 是要推送的本地分支名称。 1 2 git push git push -u origin main git push -u origin main：除了将本地的 main 分支推送到远程 origin 仓库的 main 分支外，还会建立本地 main 分支和远程 origin/main 分支的关联。建立关联后，后续使用 git push 或 git pull 时，Git 会知道默认操作的远程分支。通常在首次将本地分支推送到远程仓库时使用，这样后续无需每次都指定远程仓库和分支。 git push：如果本地分支已经和远程分支建立了关联，使用该命令可以简化操作，快速将本地更新推送到远程。适合在本地分支和远程分支已经建立关联的情况下。 其他 1 git remote -v 查看本地仓库与哪些远程仓库进行了连接的命令。执行该命令后，会列出本地仓库所关联的所有远程仓库的别名以及对应的远程仓库的 URL 地址。这里会展示 fetch（拉取）和 push（推送）对应的地址，一般情况下二者是相同的。\n1 hugo version 查看hugo版本，我的是0.145版本。\n","date":"2025-04-08T08:37:54+08:00","permalink":"https://xxcjw.github.io/p/hugo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Hugo常用命令"},{"content":"1、接好 11111\n2、和黑 1 jaba $x_a$+1\n豆包\n","date":"2025-04-07T09:12:42+08:00","permalink":"https://xxcjw.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B/","title":"大模型"}]