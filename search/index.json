[{"content":"MOE架构详解 概述 MOE 架构的基本思想是在传统 Transformer 模型中，将每个前馈网络（FFN）层替换为一个 MOE 层。一个 MOE 层通常由两个关键部分组成：\n专家网络（Experts） 每个专家是一个独立的子网络（通常是 FFN），在实际计算中只有部分专家会被激活参与处理。通过让多个专家分担不同数据子集的计算，模型在预训练时可以以较低的计算开销获得大参数量带来的表示能力。 门控网络（Gating/Router） 该模块负责根据输入 token 的特征动态选择激活哪些专家。门控网络一般采用一个带 softmax 的简单前馈网络来计算每个专家的权重。经过训练后，门控网络会逐步学会将相似的输入路由到表现更好的专家。 专家是如何学习的 MOE模型中的专家方向是事先规划好的还是自发生成的。答案是：专家都是”自学成才“的。\n在 MOE 模型中，每个专家网络只在接收到相应路由时参与计算。初期，由于门控网络参数随机，各专家接收到的数据分布比较均匀；但随着训练进行，局部梯度更新使得某些专家逐渐专注于处理特定类型的输入数据。这种“自发专化”现象使得模型整体具备了多样化的表示能力。\n门控网络与动态路由策略 基本门控函数 最常见的门控函数即为一个简单的前馈网络，其将输入与权重矩阵相乘后经过 softmax 得到各专家的概率分布。这种设计允许每个 token 可以被分配给多个专家（通常选择 top‑k 个）\n噪声注入 为了防止门控网络总是偏向于选取少数专家，常用的策略是在计算专家得分时加入噪声。\n噪声注入：在原始得分上添加随机噪声，使得初期路由决策具有更多随机性，帮助专家获得均衡的训练机会。 保留 Top‑K：只选取得分最高的 k 个专家，其余专家的权重设为 0，确保计算资源只分配给表现最优的专家。 参考 MOE 大模型架构与机制详解 —— 以 D\n","date":"2025-04-22T19:22:06+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-6/","title":"LLM系列-6"},{"content":"基础语法 菜鸟教程 |Java 教程\n廖雪峰的官方网站\n论坛 \u0026amp; 经验 java学习网站推荐-导航\n学习经验分享 · 语雀\njava八股 Java 全栈知识体系\n小林coding\nJavaGuide（Java学习\u0026amp;面试指南）\n二哥的Java进阶之路\n编程指北-计算机学习指南\njava API文档 存在码\nJava SE 文档 — API 和文档 | Oracle 中国\n","date":"2025-04-20T18:17:54+08:00","permalink":"https://xxcjw.github.io/p/java_index/","title":"Java_index"},{"content":"","date":"2025-04-20T18:16:45+08:00","permalink":"https://xxcjw.github.io/p/java%E5%9F%BA%E7%A1%80-4/","title":"Java基础 4"},{"content":"","date":"2025-04-20T18:16:34+08:00","permalink":"https://xxcjw.github.io/p/java%E5%9F%BA%E7%A1%80-3/","title":"Java基础 3"},{"content":"java基础-1\n方法（即函数） what is 方法 1、理解：方法是程序中最小的执行单元，如main方法【其实就是函数】\n2、什么时候用，有什么用：重复的代码、或具有普遍特性和功能的代码可以抽取到方法中，如发射功能\n3、好处\n将一些代码打包在一起，提高代码的复用性 提高代码的可维护性 4、写方法时注意思维，三连问：我要干嘛？我需要用到什么？是否需要返回值？\n方法的格式 1、分为两步：（1）方法定义（2）方法调用\n2、形参和实参\n形参：方法定义中的参数 实参：方法调用中的参数 3、注意事项\n方法不调用就不执行 方法与方法平级，且不能互相嵌套（可以嵌套） return关键字 方法没有返回值：可以省略不写；如果一定要写，只写return，后面不能跟具体数据，表示结束方法 方法有返回值：必须要写，且和返回值类型要一致 return与方法有关，与循环无关，执行return后整个方法全部结束（包括方法里的循环）（与break有点像） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 //不带返回值的方法定义和调用 //缺点：大部分情况我们希望能够拿到返回值并用该值进行后续一系列操作，而不是把值打印在控制台上 public static void 方法名(参数1,参数2,...) {...} public static void main(String[] args){ getSum(10,20); //实参 } public static void getSum(int num1,int num2){ //形参 int result = num1 + num2; System.out.println(result); } //----------------------------------------------------------------- //带返回值的方法定义和调用 public static 返回值类型 方法名(参数){ 方法体； return 返回值； } public static void main(String[] args){ //直接调用，上述示例就是 int sum = getSum(10,20); //赋值调用 System.out.println(getSum(10,20)); //输出调用 } public static int getSum(int a,int b){ int result = a + b; return result; } 方法重载 这样定义方法和调用方法就不需要那么麻烦了，只需要一个名称就可以了\n1 2 3 4 5 6 7 8 9 10 class Demo1{ public static void Sum(int num1,int num2){ return num1 + num2; } public static void Sum(int num1,int num2,int num3){ return num1 + num2 + num3; } } 方法内存图★ 方法被调用之后就会进栈运行（因为可以嵌套，符合后进先出） 方法执行完毕之后，出栈，方法里定义的变量随之消失 基本和引用数据类型🤔 简单记忆：除了基本数据类型其他都是引用数据类型 理解记忆：（内存角度理解）只要是new出来的都是引用数据类型，用的不是真实的数据 基本数据类型变量中存储的是真实的值；而引用数据类型中变量存储的是地址值而不是真实的值，即引用了其他空间中的数据\n基本数据类型：存储的真实数据 赋值给其他变量的，也是赋的真实值 引用数据类型：存储的是地址值，真实的数据存储在其他空间中 赋值给其他变量，赋的是地址值 面向对象 简单理解就是找现成工具使用。重点学的就是：学习获取已有对象并使用、学习如何设计对象并使用。\n类和对象 （1）理解\n在java中，必须先设计类，才能获得对象。两者之间的关系为：\n类：是对象共同特征的描述（设计图） 对象：是真实存在的具体东西/实例（根据设计图创造出的工具） new一个东西就是创建了一个对象 1 2 3 4 类是一个模板，描述对象的状态和行为 对象是类的一个实例，有状态和行为 方法就是行为，一个类可以有很多行为 实例变量就是对象的实例，对象状态由zhe\u0026#39;x （2）类的定义\n类的组成是由属性和行为两部分组成，图如上\n属性：在类中通过成员变量来体现 行为：在类中通过成员方法来体现（和前面的方法相比去掉static关键字即可） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Phone { //成员变量 String brand; int price; //成员方法 public void call() { System.out.println(\u0026#34;打电话\u0026#34;); } public void sendMessage() { System.out.println(\u0026#34;发短信\u0026#34;); } } （3）对象的使用\n创建对象的格式：类名 对象名 = new 类名(); 调用成员的格式：对象名.成员变量、对象名.成员方法() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class PhoneDemo { public static void main(String[] args) { //创建对象 Phone p = new Phone(); //使用成员变量 System.out.println(p.brand); System.out.println(p.price); p.brand = \u0026#34;小米\u0026#34;; p.price = 2999; System.out.println(p.brand); System.out.println(p.price); //使用成员方法 p.call(); p.sendMessage(); } } （4）类的注意事项\n用来描述一类事物的类，专业叫做：Javabean类。在Javabean类中，是不写main方法的； 之前，编写main方法的类，叫做测试类。我们可以在测试类中创建javabean类的对象并进行赋值调用； 建议一个java文件只写一个类，因为一个文件中只能有一个类是public修饰的，写多个类没意义 类名首字母大写，驼峰命名。类中成员变量的完整定义格式为：\n修饰符 数据类型 变量名称 = 初始化值; 数据类型 变量名称; （一般写法，比如上面定义的手机类） 但是修饰符可以先不写（还没学到）；因为类是抽象的，一般也不写初始化值（虽然没写，但会有一个默认值），而是在创建对象的时候赋值。\n封装 介绍 （1）理解\n封装是面向对象三大特征之一（封装，继承，多态）\n封装就是把数据的属性和方法捆绑在一起，形成一个独立的单元。\n封装代码实现将类的某些信息隐藏在类内部，不允许外部程序直接访问，而是通过该类提供的方法来实现对隐藏信息的操作和访问。访问成员变量private，提供对应的get xxx()/set xxx()方法。\n（2）封装好处\n那可太多了，首先就是让编程变简单，java API文档已经有很多封装好的类以及对应的方法（比如String类，里面封装了很多方法），只需要查文档，找到相应的类，调方法就行。用什么找什么。\nprivate 关键字 为了使数据更安全，防止非法数据，在编写成员变量和方法时看情况使用private关键字。private是一个权限修饰符，可以用来修饰成员（成员变量，成员方法）\n被private修饰的成员，只能在本类进行访问，而public正好相反，能被所有类访问\n针对private修饰的成员变量，如果需要被其他类使用，提供相应的操作：\n提供get变量名()方法，用于获取成员变量的值，方法用public修饰（无参有返回） 因为无返回值，类型是void 提供set变量名(参数)方法，用于设置成员变量的值，方法用public修饰（有参无返回） 有返回值，类型是定义的类型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class Student { //成员变量 private String name; private int age; //get/set方法 public void setName(String n) { name = n; } public String getName() { return name; } public void setAge(int a) { //可以设置年龄范围，超出此范围非法 age = a; } public int getAge() { return age; } public void show() { System.out.println(name + \u0026#34;,\u0026#34; + age); } } //----------------------- 学生测试类--------------- public class StudentDemo { public static void main(String[] args) { //创建对象 Student s = new Student(); //使用set方法给成员变量赋值 s.setName(\u0026#34;林青霞\u0026#34;); s.setAge(30); s.show(); //使用get方法获取成员变量的值 System.out.println(s.getName() + \u0026#34;---\u0026#34; + s.getAge()); System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge()); } } 上面代码存在一个问题，即变量名字要知其义，如果改为\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //--------------错误示例,输出为null------------ class Student { //成员变量 private String name; //默认值为null private int age; //get/set方法 public void setName(String name) { name = name; } public String getName() { return name; } } 这样的话，无论在测试类中传进来的参数是什么，输出得到的name永远是null（形参name自己赋给自己，成员变量name值没有变，仍为null）。这需要先了解两个概念：\n成员变量和局部变量 根据就近原则，age输出的是局部变量值10。如果想要输出成员变量值null，就需要this关键字。\n1 2 3 4 5 6 7 public class GirFriend{ private int age; public void method(){ int age = 10; System.out.printIn(this.age); } } this 关键字 this修饰的变量用于指代成员变量，其主要作用是（区分局部变量和成员变量的重名问题） 方法的形参如果与成员变量同名，不带this修饰的变量指的是形参，而不是成员变量 方法的形参没有与成员变量同名，不带this修饰的变量指的是成员变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class Student { private String name; private int age; public void setName(String name) { //等号右边：name为局部变量，即形参中传递的数据 //等号左边：成员变量 this.name = name; } public String getName() { return name; } public void setAge(int age) { this.age = age; } public int getAge() { return age; } public void show() { System.out.println(name + \u0026#34;,\u0026#34; + age); } } 构造方法 作用：在创建对象的时候给成员变量赋值的，完成对象数据的初始化\n分类：无参构造（通过set、get赋值和取值）和有参构造（创建对象时自动初始化，get取值）\n格式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class Student{ private String name; private int age; // ---空参构造方法--- public Student(){ ... } // ---带参构造方法--- public Student(String name,int age){ ... } } 特点：\n方法名与类名相同，大小写也要一致 没有返回值类型，连void也没有 没有返回值，不能写return 执行时机：\n创建对象的时候，虚拟机会自动调用构造方法，作用是给成员变量初始化的 每创建一次对象，就会调用一次构造方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Student { private String name; private int age; //-------------------分割线----------------- public Student() {} //空参构造 public Student(String name,int age) { //有参构造 this.name = name; this.age = age; } //-------------------分割线------------------ public void setName(String name) { this.name = name; } public String getName() { return name; } public void setAge(int age) { this.age = age; } public int getAge() { return age; } public void show() { System.out.println(name + \u0026#34;,\u0026#34; + age); } } //--------------------下面是测试类------------ public class StudentDemo { public static void main(String[] args) { //创建对象,调用空参构造,后续赋值的话通过set/get赋值 Student s1 = new Student(); s1.show(); //创建对象，调用有参构造 Student s2 = new Student(\u0026#34;林青霞\u0026#34;,30); s2.show(); } } 注意事项：\n构造方法的创建 如果没有定义构造方法，系统将给出一个默认的无参数构造方法（上面写出的就是） 如果定义了构造方法，系统将不再提供默认的构造方法 构造方法的重载 带参构造和无参构造，两者方法名相同，但参数不同，这叫构造方法的重载 推荐的使用方式 无论是否使用，带参构造和无参构造都写 任何类定义出来，默认就自带了无参构造器，写不写都有；\n但一旦定义了有参构造，默认的无参构造就消失了，此时需要自己写无参构造器了；\n标准JavaBean类 以上就是面向对象的全部知识，下面就是整合上面所学知识。\n类名驼峰命名、成员变量使用private修饰 提供两个构造方法：无参构造和带参构造 get()和set()方法：提供每一个成员变量对应的set/get方法 其他行为方法 带参和无参构造方法、get/set方法生成快捷键：alt + insert；\n插件：PTG-1秒生成标准javabean\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Student { //成员变量 private String name; private int age; //无参和带参构造方法 public Student() { } public Student(String name, int age) { this.name = name; this.age = age; } //成员方法：get/set public void setName(String name) { this.name = name; } public String getName() { return name; } public void setAge(int age) { this.age = age; } public int getAge() { return age; } //其他方法 public void show() { System.out.println(name + \u0026#34;,\u0026#34; + age); } } ","date":"2025-04-20T18:16:24+08:00","permalink":"https://xxcjw.github.io/p/java%E5%9F%BA%E7%A1%80-2/","title":"Java基础-2"},{"content":"java概述 cmd命令 指定位置快捷打开cmd：在文件夹下的路径上直接输入cmd回车打开\n环境变量：就是把文件/软件所在的路径记录下来，从而在任何地方都能够打开。（一般是添加到系统环境变量中的path，直接把软件的完整路径复制就行，而不是添加到用户环境变量）\n1 过程：当在任何一个位置打开文件时，系统会从上往下依次扫描所记录的全部环境变量，一个一个查 看这个路径下是否有所要打开的文 件，直到找到 java安装和运行 （建议把所有开发相关的软件放在同一文件夹下）\n1、下载和安装JDK\n2、下载IDEA\njava最好用的集成开发环境（编译、调试等），下载时下载收费版的exe文件（zip文件会包含一些说明文档，这些我们不需要），然后傻瓜式安装即可\n输入psvm或main会自动生成main语句 输入so会自动生成输出语句 （javac、java工具都在jdk文件下的bin目录下，安装时自动配置了环境变量，所以在命令行任何地方都能使用）运行java程序包含三个步骤：\n编写代码 编译文件（即翻译成机器认识的语言） javac是jdk提供的一个编译工具，编译后生成.class文件 运行程序 java也是jdk提供的一个工具，用来运行程序（运行时不加后缀） Idea UI的一些使用步骤：\n项目是整体的大文件（微信）；项目中模块是相互独立的（聊天、好友、朋友圈、我）；模块中有多个包组成，即文件夹；在包里面有一个或多个类用来写java代码 注意：包的命令是有规范的，一般用公司域名的反写+包的作用命名，如com.itheima.demo1 配置：设置字体（consolas）、字号（18）、自动导包、不区分大小写、背景图片 项目模块相关操作：\n类的操作：新建类、删除类、修改类 修改类需要在类的文件右键重命名进行修改，而不能直接在代码里直接修改类名（要和文件名保持一致） 模块的操作：新建、删除、修改、导入 项目的操作：新建、打开、关闭、修改 配置环境变量 为java相关的软件单独配置一个路径：JAVA_HOME\n这样配置其实也就等价于直接置环境变量E:\\develop\\jdk\\bin了（只不过path里面有很多环境变量，可能会操作到其他变量，如果不小心修改就完了）\njava历史和分类 java历史 JDK（Java Development Kit）是Java开发工具包的缩写，主要组成部分有：\njava运行环境 \u0026ndash;JRE java虚拟机 \u0026ndash;JVM java工具 \u0026ndash;如javac、java、javap等 java基础类库 \u0026ndash;如rt.jar 长期支持版本：\nJDK 8 发布于2014年 JDK 11 发布于2018年 JDK 17 发布于2021年 JDK 21 发布于2023年 一文彻底搞懂令人疑惑的Java和JDK的版本命名！\nJava Platform, 版本更新说明\nJava版本历史图 - 大佬总结\njava分类 Java SE：java语言标准版，是其他两个版本的基础，用于桌面应用的开发 Java ME：java语言的小型版，用于嵌入式设备开发（可忽略） Java EE：java语言的企业版，在SE基础上添加了一些企业级开发的规范和框架（最火爆） java跨平台原理 java语言的跨平台是通过虚拟机实现的\njava语言不是直接运行在操作系统里面的，而是运行在虚拟机中的\n针对不同的操作系统（平台），安装不同的虚拟机就可以了\njdk和jre java历史中已经简单介绍了，现在详细介绍一下：\nJDK：java开发工具包，用于开发和运行java程序代码 JVM虚拟机：java程序运行的地方 核心类库：各种类 开发工具：javac、java、jdb、jhat JRE：java运行环境，用来运行java代码，相当于从JDK取了一部分出来 JVM 核心类库 部分运行工具 关系：JDK包含JRE，JRE包含JVM\nJRE 与 JDK的区别 | 菜鸟教程\nIDEA快捷键 1 2 3 4 5 6 7 8 Ctrl + alt + L 自动格式化代码（排版） main 打印主函数语句 so/sout 打印输出语句 5.fori / arr.fori 打印for循环语句，循环5次 / 数组遍历 ctrl+alt+v 自动补全代码 ctrl+p 查看形参、参数等 ctrl+alt+M 自动抽取方法（选中代码，按快捷键自动抽取） alt + insert 构造方法、set/getshegn\u0026#39;che 常用库 Scanner 1 2 3 import java.util.Scanner; //导包 Scanner sc = new Scanner(System.in); //创建对象 int i = sc.nextInt(); //接收数据 Random 1 2 3 4 5 6 7 import java.util.Random; //导包（自动） Random r = new Random(); //创建对象 int number = r.nextInt(终止范围); //生成随机数，左闭右开，一定是从0开始 //实现任意数到任意数的范围，如7-15（对应0-8） Random r = new Random(); int number = r.nextInt(9) + 7; Java 基础语法 | 菜鸟教程\njava语法基础 常量、变量、数据类型 （1）注释和关键字\n注释分为单行注释、多行注释和文档注释 关键字：程序根据关键字知道要做什么事情 （2）第一个关键字-class\nclass关键字表示定义一个类，后面跟随类名。那如何理解类呢？可以把类当成人体的细胞，一个java程序由超级多的类组成，编写代码的时候看需要那个细胞就编写那个细胞，从而组成一个完整的人\n注：类名要和文件名保持一致\n（3）常量/字面量\n字面量分类：整数、小数、字符串、字符、布尔、空（null） 一些特殊的字面量：制表符\\t、空类型null 1 2 3 4 5 6 null不能直接打印，只能用字符串的形式打印，如System.out.println(\u0026#34;null\u0026#34;); 直接输出会导致编译错误：System.out.println(null); 制表符的作用是把前面的字符串长度补齐到8，或者8的倍数，从而想表格一样整齐，如 System.out.println(\u0026#34;name\u0026#34;+\u0026#34;\\t\u0026#34;+\u0026#34;tom\u0026#34;); 补4个空格 System.out.println(\u0026#34;age\u0026#34;+\u0026#34;\\t\u0026#34;+\u0026#34;23\u0026#34;); 补5个空格 （4）变量\n变量可以重复使用，但不能重复定义，即不能写两次int a = xxx; 变量使用之前一定要进行赋值 变量的作用域范围：只在当前所属的大括号内有效 一条语句中可以定义多个变量，如int d=100,e=200,f=300; （5）数据类型-1\njava中的数据类型分为：基本数据类型（四类八种）和引用数据类型【string是典型的引用数据类型】 【四类八种外加引，1+1+2+4】 1 2 3 4 注意事项： 定义long类型变量：需在数值后面加入L作为后缀（大小写均可） long a = 99999L; 定义float类型变量：需在数值后面加入F作为后缀（大小写均可） float a = 23.3F; 如果不加后缀的话使用的是默认的数据类型（int和double） （6）计算机中的数据存储\n二进制：0b开头； 十进制； 八进制：0开头； 十六进制：0x开头 之所以使用二进制是因为：二进制很好区分，以前是有孔和没孔，现在是高压和低压 我们知道，计算机中任何数据都是二进制存储。可以把计算机中所有数据归为：文本、图片、声音三类数据，那这三类数据是如何存储的呢？ 文本：又包括数字、字母、汉字三类，字母和汉字都是通过码表先转换为十进制数字，然后用二进制存储在计算机中，如Unicode编码表包含了世界各国语言对应的数字\n图片：图片就是一个一个的像素（分辨率）组成，一个像素对应一个数字（对于灰度图，数字范围是0-255表示不同灰度等级）存储在计算机；而如果是彩色图片，无非就是每一个像素搭配了不同的三原色RGB实现存储（RGB红绿蓝，一个像素点对应三个数字）\n声音：也是类似，对声音的波形图进行采样再进行存储，即将声音的波形图划分，图上每一个点对应一个数字（类似码表的一一映射）。日常生活中的声音无损和有损就是采样点的多少，采样的多就和真实没啥区别。图片的有损和无损也是同样道理。\n（7）标识符-2\n标识符/变量名命令建议（阿里规范）\n（8）键盘录入-3\nJava有一个类叫Scanner，这个类就可以接受键盘输入的数字。（变量i记录键盘所敲的数字）\n类\u0026ndash;\u0026gt;对象\u0026ndash;\u0026gt;多个实例\n（9）输出语句\n1 2 3 System.out.println(arr[i]); //打印后换行 System.out.print(arr[i] + \u0026#34; \u0026#34;); //打印不换行 System.out.println(); //只进行换行，不输出内容 运算符🌙 （9）数值拆分\n使用运算符进行数值拆分\n1 2 3 4 5 1234 个位：数值 % 10 十位：数值 / 10 % 10 百位：数值 / 100 % 10 千位：数值 / 1000 % 10 自增自减运算符：a++先用后加，a的值发生改变；++a先加后用，a的值发生改变\n1 2 3 4 int x = 10; int y = x++; int z = ++x; 最终x=12，y=10，z=12 扩展赋值运算符：+=、-=、*=、/=、%=底层都隐藏了一个强制类型转换\n1 2 3 short s = 1; s += 1; //先进行s+1，为int类型，然后强转后赋值 等同于s = (short)(s + 1); 逻辑运算符：\n短路逻辑运算符：\n因为逻辑运算符效率比较低（无论左边能不能确定结果，右边都要判断），所以之后常用的是短路逻辑运算符来提高效率。\n举个例子：登陆时需要输入用户名和密码，使用逻辑运算符\u0026amp;时无论用户名输入是否正确，密码都需要判断；而使用短路逻辑运算符\u0026amp;\u0026amp;时，如果用户名输入错误，那无论密码正确与否都不重要的，即不用判断密码了，代码只判断用户名错误就结束了\n三元运算符：关系表达式 ? 表达式1 : 表达式2;\n先执行关系表达式，结果如果为真，则输出表达式1，结果如果为假，则输出表达式2\n（10）运算符优先级\n所有数学运算基本都是从左往右进行的；只有单目运算符、赋值运算符和三目运算符例外（从右往左）\nTip:注意最后一行的扩展赋值运算符是从右往左执行的；记死小括号最高，扩展赋值最低\n在大部分编程语言中，都是这样的：有些远算符是从左到右，有些是从右到左（如赋值）\n（11）类型转换\nbyte、short、char三种类型的数据在运算的时候，都会直接提升为int，然后再进行运算（隐式）\n数字相加 字符串相加：当执行“+”操作时，只要出现字符串，就不是运算操作，而是拼接操作了 字符相加：先把字符通过ascii转为数字在相加 1 2 3 4 5 6 7 8 9 10 byte b1 = 10; byte b2 = 20; byte result = (byte)(b1+b2); b1+b2在计算前都会先转成int，相加后也是int类型，而最终要赋给byte类型，因此要用强转 byte b1 += 10; //这个不用强转也不报错，原因见上面 1+99+\u0026#34;xxcjw\u0026#34;+1 //输出结果是\u0026#34;100xxcjw1\u0026#34; true+\u0026#34;xxcjw\u0026#34; //输出结果是\u0026#34;truexxcjw\u0026#34; 连续+操作时，从左到右依次执行；当+前后有字符串时，就是拼接操作 原码、反码、补码 正数的原码、反码、补码就是其本身，不变 原码：数据的二进制表现形式，最左边是符号位 弊端：如果是负数计算，运算方向和实际相反，导致结果错误 反码：（负数反码为）符号位不变，其余数值取反 出现目的：为了解决原码不能计算负数的问题出现 弊端：0有两种表示，如果负数运算时跨0，与实际结果会有1的偏差 补码：负数的补码在反码的基础上+1 为了解决负数反码计算时跨0的问题而出现 计算机中的存储和计算都是以补码形式进行的（核心！） 1 2 3 4 5 看个例子： int a = 300; //0000 0000 0000 0000 0000 0000 1100 1000 byte b = (byte)a; //1100 1000 所以b就是1100 1000，注意啦，这个数值在计算机是以补码存储的，正数无所谓， 但此时这个是负数，将补码转为原码最终就是-56，所以b=-56 计算总结：\n从位的角度理解逻辑运算符（以原码反码补码解释）\n流程控制语句 分支结构 （1）if语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 if( xxxx ) { # 第一种结构 xxxxx; } if( xxxx ) { # 第二种结构 xxxxx; } else { xxxx; } if( xxxx ) { # 第三种结构 xxxxx; } else if { xxxxx; } else { xxxxx; } (2)switch语句\n注意一点：case后面的值只能是常量而不能是变量；switch后面的表达式通常是变量，来表示各种情况\ndefault可以写在任意位置，但习惯写在最后来表示异常情况 case穿透：就是去掉了break语句导致的 拿表达式的值和下面每一个case的值匹配，如果匹配上，则执行相应语句，如果有break，则结束 如果没有发现break，那么程序会继续执行下一下case的语句体，一直遇到break为止 switch新特性，即优化了写法（JDK12及以后） if第三种结构适用于范围判断；switch适用于枚举，把有限的数据一一列举出来 1 2 3 4 5 6 7 8 9 10 11 12 switch(表达式){ case 值1: 语句1; break; case 值2: 语句2; break; ... default: 语句n+1; break; } 示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 case穿透实例：输入日期，如果是1-5，则是工作日；如果是6-7，则是休息日 //比如输入2，和case 1不匹配；和case 2匹配，则执行case 2的语句体，发现没有break，则会继续往下执行case的语句体（就不用匹配了），直到遇见break，即执行完case 5的语句体结束 //1、键盘录入 Scanner sc = new Scanner(System.in); System.out.println(\u0026#34;请录入一个整数表示星期\u0026#34;); int week = sc.nextInt(); //2、利用switch语句选择 switch (week){ case 1: case 2: case 3: case 4: case 5: System.out.println(\u0026#34;工作日\u0026#34;); break; case 6: case 7: System.out.println(\u0026#34;休息日\u0026#34;); break; default: System.out.println(\u0026#34;没有这个星期\u0026#34;); break; } //简化代码 switch (week){ case 1,2,3,4,5: System.out.println(\u0026#34;工作日\u0026#34;); break; case 6,7: System.out.println(\u0026#34;休息日\u0026#34;); break; default: System.out.println(\u0026#34;没有这个星期\u0026#34;); break; } 循环结构 （1）for循环\n1 2 3 4 5 6 7 for(初始化语句;条件判断语句;条件控制语句){ 循环体语句； } for(int i = 0;i \u0026lt;= 10;i++){ System.out.println(\u0026#34;xxcjw\u0026#34;); } 变量的作用范围只在变量所属的大括号中有效 之后如果要写累加求和的变量，可以把变量定义在循环的外面。当把变量定义在循环里时，当前变量只在本次循环中有效，当本次循环结束时，变量会消失；当下次循环开始时，又会重新定义一个该变量 for循环中的累加思想：定义一个变量在循环外，循环内+= for循环中的统计思想：统计个数用自增运算符，循环外定义一个变量，循环内自增 （2）while循环\n1 2 3 4 5 6 7 8 9 10 11 初始化语句； while(条件判断语句){ 循环体语句； 条件控制语句； } int i =1; while(i \u0026lt;= 100){ System.out.println(i); i++; } （3）for和while区别\n初始化语句中变量的作用域区别（不绝对） 开发中根据情况使用 跳转控制语句⭐ continue：跳出本次循环，继续下次循环 break：跳出整个循环 ⭐在for循环或while循环中，适当使用跳转控制语句可以大大提高效率，要把循环语句和跳转控制语句绑在一起，提起一方就想到另一方。比如循环100次，但在第10次就得到结果了（求平方根），那使用break后面就不用在执行了，提高效率。\n⭐比如经常碰见的场景：（猜数字）不知道循环多少次，那就写死循环，直到相等时break退出就行\n⭐注意：这两个语句都和判断语句if无关\n一些思想😊 倒序输出思想、标记思想、统计思想\n练习两道力扣题：（1）回文数（2）判断质数（3）猜数字\n1、回文数（注意与数值切分的区别，倒序输出思想）\n1 2 3 4 5 6 7 8 9 10 11 12 //回文数：正着读和反着读都一样，如121，1221 //思路：把数字倒过来跟原来的数字进行比较（容易想到的是数值切分和拼接，但有很多细节） int x =12345; int temp = x; //临时变量记录原来输入的值，用于最后比较，因为x会变化 int num = 0; //记录倒过来之后的值 while(){ //一开始不知道循环结束条件可以先不写 int ge = x % 10; //从右往左获取每一位数字 x = x / 10; num = num * 10 + ge; //拼接 } System.out.println(num == temp); //比较 2、判断是否为质数（用到了标记思想）\n标记思想通常用于数据筛选与过滤中，比如筛选特定数据、路径搜索与图算法中的节点状态deng\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //判断一个属是否为质数（只能被1和它本身整除） //1、键盘录入一个整数 Scanner sc = new Scanner(System.in); System.out,println(\u0026#34;请输入一个正整数\u0026#34;); int number = sc.nextInt(); //表示最初就认为是一个质数，标记思想 boolean flag = true; //2、for循环判断 for (int i = 2;i \u0026lt; number;i++){ //number.fori快捷键 if(number % i == 0){ flag = false; System.out.println(number + \u0026#34;不是一个质数\u0026#34;); break; } } if(flag){ System.out.println(number + \u0026#34;是一个质数\u0026#34;); }else{ System.out.println(number + \u0026#34;是一个质数\u0026#34;); } 进一步优化：\n1 2 3 //上述代码，如果输入的数很大，则需要循环很多次，下面进行优化 //思路：如果一个数不是质数，则它的因子中必定有一个是小于其平方根的，如81，只需要循环到9 // for(int i = 2;i \u0026lt;= number的平方根；i++) 3、猜数字游戏【注：统计思想和求和思想见for循环】\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //1、生成1-100之间的随机数字 Random r = new Random(); int number = r.nextInt(100) + 1; //2、键盘录入 Scanner sc = new Scanner(System.in); int count = 0; while(true){ //死循环，和break配合 System.out.println(\u0026#34;输入你猜的数字：\u0026#34;); int guessNumber = sc.nextInt(); //3、判断，给出提示 if(guessNumber \u0026gt; number){ System.out.println(\u0026#34;大了\u0026#34;); } else if(guessNumber \u0026lt; number) { System.out.println(\u0026#34;小了\u0026#34;); } else { System.out.println(\u0026#34;猜中了\u0026#34;); break; } //4、保底机制 count++; if(count == 3){ System.out.println(\u0026#34;猜中了\u0026#34;); break; } } 数组 定义和初始化 1、定义\n数组就是一种容器，可以用来存储多个同种类型数据\n2、静态初始化\n1 2 3 4 5 6 7 8 9 //完整格式（了解） 数据类型[] 数组名 = new 数据类型[]{num1,num2...} int[] array = new int[]{11,22,33}; //简化格式（掌握） 数据类型[] 数组名 = {num1,num2...} int[] array = {11,22,33}; double[] array = {11.1,22.2,33.3}; string[] array = {\u0026#34;张三\u0026#34;,\u0026#34;李四\u0026#34;,\u0026#34;王五\u0026#34;}; 3、数组元素访问\n数组的长度属性：数组名.length arr.length\n1 2 3 4 5 6 7 8 9 10 11 12 13 //直接打印数组名输出的是数组的地址值 int[] array = {11,22,33}; System.out.println(array); //输出 [D@776ec8df //[-表示是一个数组；D-表示数组里面元素是double类型；@-间隔符号，固定格式；776ec8df-地址值 //用索引数组实现元素访问 System.out.println(array[0]); //打印第一个元素 array[0] = 100; //修改数组元素 //循环遍历 for(int i = 0;i \u0026lt; array.length;i++){ System.out.println(array[i]); } 小技巧：一个循环尽量只干一件事情；比如一个循环修改数组元素值，用另一个循环打印输出\n4、动态初始化\n定义：初始化时只指定数组长度，系统为该数组分配初始值\n格式：数据类型[] 数组名 = new 数据类型[数组长度]\n1 2 3 4 5 6 7 8 9 10 11 //动态初始化 String[] arr = new String[50]; //添加数据 arr[0] = \u0026#34;张三\u0026#34;; arr[1] = \u0026#34;里斯\u0026#34;; /* 引用数据类型 默认初始化为null 整数数据类型 默认初始化为0 浮点数数据类型 默认初始化为0.0 char数据类型 默认初始化为空格 */ 5、区别\n数组常见操作 （1）求最值\n思路：定义一个变量存储最值（参照物），拿数组中的元素比较\n1 2 3 4 5 6 int max = arr[0]; for(int i = 1;i \u0026lt; arr.length;i++){ if(arr[i] \u0026gt; max){ max = arr[i]; } } （2）随机生成+求和\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 int[] arr = new int[10]; //动态初始化 Random r = new Random(); //生成10个随机数 for(int i = 0;i \u0026lt; arr.length;i++){ int number = r.nextInt(100) + 1; arr[i] = number; } //求和，一个循环干一件事 int sum = 0; for(int i = 0;i \u0026lt; arr.length;i++){ sum += arr[i]; } （3）交换数据\n思路：定义两个变量分别从前往后和从后往前记录索引\n1 2 3 4 5 6 7 8 9 10 11 12 13 //输入12345，输出54321 int[] arr = {1,2,3,4,5}; //先进行两个数据的交换，然后无非就是多次循环 for(int i = 0,j = arr.length - 1;i \u0026lt; j;i++,j--){ int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } for(int i = 0;i \u0026lt; arr.length;i++){ System.out.print(arr[i] + \u0026#34; \u0026#34;); } （4）打乱数据\n关键问题：获取随机索引；对每个元素进行随机交换数据\n1 2 3 4 5 6 7 8 9 10 11 int[] arr = {1,2,3,4,5}; Random r = new Random(); for(int i = 0;i \u0026lt; arr.length;i++){ //定义随机索引 int randomIndex = r.nextInt(arr.length); //交换数据 int temp = arr[i]; arr[i] = arr[randomIndex]; arr[randomIndex] = temp; } （5）打印数组+逗号\n1 2 3 4 5 6 7 8 9 //将数组打印至一行，并以逗号相隔 for(int i = 0;i \u0026lt; arr.length;i++){ if(i == arr.length - 1){ System.out.print(arr[i]); } else { System.out.print(arr[i] + \u0026#34;,\u0026#34;); } } 6、copy数组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //将数组arr从索引from开始，到索引to结束（不包含）的元素复制到新数组中 int[] arr = {1,2,3,4,5,6,8,7}; int[] copyArr = copyOfRange(arr,3,7); //定义方法来做这件事 public static int[] copyOfRange(int[] arr,int from,int to){ int[] newArr = new int[to - from]; int index = 0; //伪造索引的思想 for(int i = from;i \u0026lt; to;i++){ newArr[index] = arr[i]; //这里需要注意，新数组的下标不是从0开始 index++; } return newArr; } 数组内存图 （1）java（虚拟机）内存分配\n一看见new就知道是存储在堆中的，堆中的每一个对象/数组都对应一个地址值（即只要是new出来的一定在堆里面开辟了一个小空间）\n（2）例子：数组内存图\n","date":"2025-04-20T15:52:46+08:00","permalink":"https://xxcjw.github.io/p/java%E5%9F%BA%E7%A1%80-1/","title":"Java基础-1"},{"content":"总体内容 BERT是用了Transformer的encoder侧的网络。\nBert网络结构 如图所示, 最左边的就是BERT的架构图, 可以看到BERT采用了Transformer Encoder block进行连接, 因为是一个典型的双向编码模型,多头自注意力中的每个头的输入都同时蕴含了前后文的语言信息。从上面的架构图中可以看到, 宏观上BERT分三个主要模块：\n最底层黄色标记的Embedding模块 中间层蓝色标记的Transformer模块 最上层绿色标记的预微调模块 Embedding层 BERT中的 Embedding 模块是由三种 Embedding 共同组成而成。将文本输入到LLM后，首先进行的是 tokenization 处理，且两个特殊的 Token 会插入在文本开头 [CLS] 和结尾 [SEP]。[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。\nToken Embeddings：将词表中的每个 token 转化为高维向量作为输入，首个单词 CLS 用于分类任务 通过建立词表将每个token转换成一个高维向量，作为模型输入。特别的，英文词汇会做更细粒度的切分。将词切割成更细粒度的 Word Piece 是为了解决未登录词的常见方法。 Segment Embeddings：用于区分句子界限，分别以 0 和 1 标记不同句子 Position Embeddings：用于记录文本中各 token 的相对顺序 BERT 中处理的最长序列是 512 个 Token，长度超过 512 会被截取，BERT 在各个位置上学习一个向量来表示序列顺序的信息编码进来，这意味着 Position Embeddings 实际上是一个 (512, 768) 的 lookup 表。 最后，BERT 模型将 Token Embeddings (1, n, 768) + Segment Embeddings(1, n, 768) + Position Embeddings(1, n, 768) 求和的方式得到一个 Embedding(1, n, 768) 作为模型的输入。\nTransformer中position Embedding是用的三角函数，BERT中也有一个Postion Embedding是随机初始化，然后从数据中学出来的。\n[CLS]的作用\nBERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。\nEncoder层 BERT是用了Transformer的encoder侧的网络，BERT的维度是768维度，然后分成12个head，每个head的维度是64维。BERT模型分为24层和12层两种，其差别就是使用transformer encoder的层数的差异。\nBERT-base : L=12，H=768，A=12，参数总量110M； BERT-large: L=24，H=1024，A=16，参数总量340M； 参考 The Illustrated Transformer – 图解transformer （超详细）\n读懂BERT，看这一篇就够了 （结构详解）\nBERT模型架构与详解 （清晰）\nBERT 详解_bert模型\nBERT系列（二）BERT模型的核心架构 （链接多）\n博主推荐：\n后厂NLPer-博客 语言模型 \u0026amp; Co. |杰伊·阿拉马尔 （图解系列，优质博主）\nJay Alammar （同上，旧博客）\n","date":"2025-04-17T15:42:07+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-5bert%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/","title":"LLM系列-5：Bert模型详解"},{"content":"连接实验室服务器 下载插件Remote-SSH 输入ssh user_name@server_IP 在插件下配置远程连接文件 1 2 3 4 5 6 7 8 9 Host xxcjw-A4000 HostName 58.199.157.119 Port 7722 User xxx Host xxcjw-A6000 HostName 10.0.65.51 Port 7722 User xxx VSCode远程连接实验室服务器（内部链接3）\nvscode连接Linux服务器-博客园\nVSCode SSH远程连接与删除\n免密登录 本质上就是本地生成公钥复制到虚拟机或者服务器的authorized_keys文件上即可。\n本机 cmd 输入ssh-keygen -t rsa，然后连续回车直到结束 服务器执行同样操作，之后进入.ssh这个文件夹进行操作 VsCode配置ssh免密远程登录\n","date":"2025-04-15T20:07:17+08:00","permalink":"https://xxcjw.github.io/p/vscode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/","title":"Vscode远程连接"},{"content":"LLM输出向量池化方式 LLM 输出池化（Output Pooling）是一种对大模型输出进行处理的操作，旨在将模型生成的一系列特征向量（多个token）或表征转换为一个固定长度的向量。\n平均池化 平均池化会计算序列中所有向量的平均值，从而得到一个固定长度的向量。这种方式能有效综合序列里所有元素的信息。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = torch.mean(hidden_states, dim=1) # 在序列长度维度上求平均 print(pooled_output.shape) # 输出: torch.Size([2, 768]) 最大池化 最大池化会在序列中每个维度选取最大值，以此生成一个固定长度的向量。它能提取序列中的关键特征。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output, _ = torch.max(hidden_states, dim=1) # 在序列长度维度上取最大值 print(pooled_output.shape) # 输出: torch.Size([2, 768]) [CLS] 池化 也就是 CLS Token Pooling 。在使用预训练模型（像 BERT 这类）时，通常会在输入序列开头添加一个特殊的 [CLS] 标记。[CLS] 池化就是直接选取这个标记对应的输出向量作为整个序列的表示。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = hidden_states[:, 0, :] # 选取每个样本的第一个向量 print(pooled_output.shape) # 输出: torch.Size([2, 768]) 加权平均池化 也就是 Weighted Mean Pooling ，加权平均池化会依据每个向量的重要性为其分配不同的权重，再计算加权平均值。这样可以更有针对性地综合序列信息。\n1 2 3 4 5 6 7 8 9 10 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) # 假设 weights 是每个向量的权重，形状为 (batch_size, sequence_length) weights = torch.rand(2, 10) weights = torch.softmax(weights, dim=1) # 确保权重和为 1 weighted_hidden_states = hidden_states * weights.unsqueeze(-1) pooled_output = torch.sum(weighted_hidden_states, dim=1) print(pooled_output.shape) # 输出: torch.Size([2, 768]) Last 池化 也就是 Last Token 池化。选取序列中最后一个 token 对应的向量作为整个序列的表示。这种池化方式在某些场景下是有效的，例如在处理文本生成或者问答任务时，模型最后一个输出的 token 可能包含了整个序列处理后的关键信息。\n1 2 3 4 5 6 import torch # 假设 hidden_states 是模型的输出，形状为 (batch_size, sequence_length, hidden_size) hidden_states = torch.randn(2, 10, 768) pooled_output = hidden_states[:, -1, :] # 选取每个样本的最后一个向量 print(pooled_output.shape) # 输出: torch.Size([2, 768]) Transformers库中相关类 在 transformers 库中，AutoModel 和相关类提供了一个统一的接口来加载不同类型的预训练模型。你可以通过 AutoModel 系列类来简化模型加载过程，而无需关心特定模型的类型。\n常用的 AutoModel类 AutoModel: 加载基础模型，适用于不带头的 Transformer 模型。\nAutoModelForSequenceClassification: 加载用于序列分类任务的预训练模型。\nAutoModelForTokenClassification: 加载用于标记分类（如命名实体识别）的模型。\nAutoModelForQuestionAnswering: 加载用于问答任务的模型。\nAutoModelForCausalLM: 加载用于自回归语言建模（生成任务）的模型。\nAutoTokenizer: 用于加载与模型匹配的标记器（tokenizer），负责文本预处理和编码。\nAutoFeatureExtractor: 用于加载与图像或其他类型输入匹配的特征提取器。\n基础模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoModel, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; model = AutoModel.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) outputs = model(**inputs) last_hidden_state = outputs.last_hidden_state # 获取模型的最后一层隐藏状态 print(\u0026#34;Last hidden state shape:\u0026#34;, last_hidden_state.shape) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # -----------------查看隐藏层形状---------------------- print(output.last_hidden_state.shape) # ------------获取输入转换后的 token------------------- tokens = tokenizer.tokenize(question) print(tokens) # ----------------获取输入转换后的 token--------------- # 方式一 print(inputs) # 方式二 input_ids = inputs[\u0026#34;input_ids\u0026#34;][0] print(input_ids) # -------------查看每个 token 对应的文字---------------- # 方式一 tokens = tokenizer.convert_ids_to_tokens(input_ids) print(\u0026#34;每个 token 对应的文字：\u0026#34;,tokens) # 方式二（） decoded_text = tokenizer.decode(inputs[\u0026#34;input_ids\u0026#34;][0], skip_special_tokens=True) print(\u0026#34;Decoded text:\u0026#34;, decoded_text) 1 2 3 # 查看大模型词表大小（也可以根据config.json查看） embeddings = model.get_input_embeddings() print(f\u0026#39;嵌入矩阵大小: {embeddings.weight.size()}\u0026#39;) 生成任务 用于加载 自回归语言模型 的类。自回归语言模型（Causal LM）在给定一些上下文时，会生成下一个可能的单词或 token。通常用于生成任务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from transformers import AutoModelForCausalLM, AutoTokenizer import torch model_name = \u0026#34;gpt2\u0026#34; model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) output = model.generate( **inputs, # 输入ID（token化后的文本） max_length=50, # 生成的最大长度 num_return_sequences=1, # 返回生成的序列数 no_repeat_ngram_size=2, # 防止生成重复的n-gram temperature=0.7, # 温度（控制随机性,值越大越具创造性） top_k=50, # 只考虑前k个最可能的词 top_p=0.95, # 采用 nucleus sampling（前p%的词） do_sample=True # 启用采样（避免确定性输出） ) generated_text = tokenizer.decode(output[0], skip_special_tokens=True) print(generated_text) top_k：从生成词汇中选择前 k 个最有可能的词进行采样。较小的值意味着选择更有限的词汇。\ntop_p：从概率累积值中选择前 p% 的词汇，控制采样的多样性。与 top_k 类似，也是用于限制候选词的范围。\ndo_sample：是否启用采样。如果为 True，模型将从预测的概率分布中随机采样生成下一个 token；如果为 False，它将选择概率最大的 token（即确定性生成）。启用采样机制可以增加生成文本的多样性，避免每次生成的文本都相同。\n序列分类任务 如果在做分类任务（例如情感分析），可以使用这个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import AutoModelForSequenceClassification, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; # 假设是二分类任务 model = AutoModelForSequenceClassification.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;,num_labels=2) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # 前向传播 outputs = model(**inputs) logits = outputs.logits # 获取分类任务的输出logits print(\u0026#34;Logits:\u0026#34;, logits) print(outputs.logits.shape) # 查看输出形状 模型的输出并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层。\n1 2 3 import torch predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) print(predictions) 标记分类任务 假设做的是命名实体识别任务，可以使用这个类。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoModelForTokenClassification, AutoTokenizer import torch model_name = \u0026#34;bert-base-uncased\u0026#34; model = AutoModelForTokenClassification.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # 前向传播 outputs = model(**inputs) logits_token_class = outputs.logits # 获取每个token的分类结果 print(\u0026#34;Token classification logits:\u0026#34;, logits_token_class.shape) 问答任务类 通常用于从给定的上下文中回答问题。模型通过读取文本（context）和问题（question），然后返回答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from transformers import AutoModelForQuestionAnswering, AutoTokenizer import torch model_name = \u0026#34;distilbert-base-uncased-distilled-squad\u0026#34; model = AutoModelForQuestionAnswering.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name) question = \u0026#34;What is the capital of France?\u0026#34; context = \u0026#34;France is a country in Europe. Paris is its capital and the largest city.\u0026#34; inputs = tokenizer(question, context, return_tensors=\u0026#34;pt\u0026#34;).to(model.device) # `start_logits` 和 `end_logits` 分别表示答案的起始和结束位置 with torch.no_grad(): outputs = model(**inputs) start_position = torch.argmax(outputs.start_logits) # 答案起始位置 end_position = torch.argmax(outputs.end_logits) # 答案结束位置 # 7. 解码答案 answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(inputs[\u0026#39;input_ids\u0026#39;][0][start_position:end_position+1]) ) print(\u0026#34;Question:\u0026#34;, question) print(\u0026#34;Answer:\u0026#34;, answer) 在问答任务中，模型通过 start_logits 和 end_logits 来输出答案的位置。需要从模型输出的 tokens 中提取出 起始位置 和 结束位置 的答案，然后将其通过 convert_ids_to_tokens 转换为字符串。decode 方法主要用于生成文本，而不是处理直接的预测位置（例如起始和结束位置）。\n在问答任务中，我们不是生成每个 token，而是从模型输出的 start_logits 和 end_logits 中直接找到答案的位置。\n大模型词表嵌入 模型 词表嵌入维度 参数数量 GPT-2 1,024维 15亿 GPT-3 12,288维 1750亿 BERT-base 768维 1.1亿 BERT-large 1,024维 3.4亿 LLaMA2-7B 4,096维 70亿 LLaMA-2-13B 5,120维 130亿 LLaMA-3-8B 4,096维 80亿 deepseek-R1-14B 5120维 140亿 qwen2.5-7B 3,584维 70亿 Bloom-7B1 4,096维 71亿 Bloom-1B7 2,048维 17亿 Bloom-560M 1,024维 5.6亿 BERT模型分为24层和12层两种，BERT-base使用的是12层的Transformer Encoder结构，BERT-Large使用的是24层的Transformer Encoder结构。\n对于不同的大型语言模型，它们能处理的最大token长度（也称为上下文窗口大小）是由模型本身决定的。\n大模型内部都存在一个词表，存储了一系列token的集合。它决定了模型所 “认识” 的词汇和符号的范围。\n在LLM的config.json 文件中，一般可以找到相关参数。\n如在 deepseek-r1-14b 的配置文件中，max_position_embeddings参数定义了模型能够处理的最大位置嵌入数，也就是模型能够接受的最大token长度 而vocab_size参数就是词表的大小（即多少行）；hidden_size参数就是每一个token所对应的嵌入向量大小（即多少列） LLM流程 Token是LLM处理文本的基本单位。当我们将文本输入 LLM 时，模型首先会将文本切分成 Token 序列，然后再对这些 Token 通过词表转换成高维嵌入，最终经过大模型处理生成我们期望的输出结果。\n此外，还有一些相关概念：\n最大输出长度（输出限制）：模型单次生成文本的最大长度 上下文长度：指的是模型在处理输入时能够 “看到” 或考虑的文本范围，即模型在生成输出时，会参考的前文的 token 数量。 上下文截断：应对超长对话的策略。 上下文截断是一种在工程层面实施的策略，而非模型本身固有的能力。其具体指的是：如果用户在多轮对话中累积的输入和输出 Token 数量超出最大上下文长度的限制，服务端通常会保留最近的内容，而丢弃早期输入，即只能 “记住最近的，遗忘久远的”。\n模型的最大输入 token 长度有多种不同叫法：\n最大位置嵌入（Max Position Embeddings）：位置嵌入用于标记每个 token 在输入序列中的位置，规定了模型能处理的输入序列最大长度\n上下文窗口大小（Context Window Size）：指的是模型在处理输入时能够考虑的上下文范围，也就是输入序列的最大长度。这个概念在自回归模型中常用，模型生成每个 token 时会参考之前的上下文，上下文窗口大小决定了能参考的最大 token 数量。\n参考链接 大模型关键参数解读\n","date":"2025-04-15T18:28:48+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-4llm%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"LLM系列-4：LLM的使用"},{"content":"pipelines Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节（分词、编解码过程）。我们只需输入文本，就能得到预期的答案。那其背后具体做了什么呢？\n1 2 3 4 5 6 7 8 9 10 ## 文本生成任务示例： from transformers import pipeline generator = pipeline(\u0026#34;text-generation\u0026#34;, model=\u0026#34;distilgpt2\u0026#34;) results = generator( \u0026#34;In this course, we will teach you how to\u0026#34;, max_length=30, num_return_sequences=2, ) print(results) 其背后经过了三个步骤：\n预处理，将原始文本转换为模型可以接受的输入格式 将处理好的输入送入模型，根据具体任务进行推理生成 对模型的输出进行后处理，将其转换为人类方便阅读的格式 使用分词器进行预处理 因为神经网络模型无法直接处理文本，因此首先需要通过分词器 (tokenizer)将文本转换为模型可以理解的数字。\n我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作。因此我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n1 2 3 4 5 6 7 8 from transformers import AutoTokenizer checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) question = \u0026#34;魔都是哪个城市?\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;) print(inputs) 输出为：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#39;input_ids\u0026#39;: tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0] ]), \u0026#39;attention_mask\u0026#39;: tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] ]) } 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\ncheckpoint（检查点）是指在模型训练过程中，为了避免可能出现的意外情况，定期保存模型的状态，这个状态包含了所使用的模型名称、模型的参数、优化器的状态、训练步数等信息。\n注意，此时的到的只是分词后token对应的ID，并没有得到经过LLM编码的高维向量。\n将预处理好的输入送入模型 预训练模型的下载/加载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。\n1 2 3 4 from transformers import AutoModel checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; model = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务。\n相信你还没有理解，来看接下来这一段代码：\n1 2 3 4 from transformers import AutoModelForCausalLM checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; model = AutoModelForCausalLM.from_pretrained(checkpoint) 那它们之间的区别就在于Head部分，如果使用AutoModel就相当于只是用预训练模型中最基础的 Transformer 模块，得到的是大模型的高维嵌入向量。而如果使用AutoModelForCausalLM，就相当于多加了一个Head来完成特定的任务。\nTransformers 库封装了很多这样不同的结构，常见的有：\n*Model （返回 hidden states） *ForCausalLM （用于条件语言模型）【我用过的就是这种】 *ForMaskedLM （用于遮盖语言模型） *ForMultipleChoice （用于多选任务） *ForQuestionAnswering （用于自动问答任务） *ForSequenceClassification （用于文本分类任务） *ForTokenClassification （用于 token 分类任务，例如 NER） Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。\nAutoModel.from_pretrained(checkpoint)\n这个方法加载的是基础的预训练模型架构，不包含特定任务的头部（head）。基础模型通常会输出隐藏状态（hidden states），这些隐藏状态可以作为特征表示，用于下游任务的进一步处理。例如，在文本分类任务中，可以将这些隐藏状态输入到一个全连接层进行分类。 适用于那些需要对模型进行自定义扩展或微调的场景，可以根据自己的需求添加特定任务的头部，以适应不同的任务。 AutoModelForCausalLM.from_pretrained(checkpoint)\n这个方法加载的是专门用于因果语言模型（Causal Language Model，CLM）任务的预训练模型。因果语言模型的目标是根据前面的上下文预测下一个词，常用于文本生成任务等。该模型已经包含了一个语言建模头部（language modeling head），可以直接用于生成文本，无需额外添加特定任务的头部。 使用于文本生成任务类。 完整代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer import torch checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) #------------------使用 AutoModel 加载基础模型----------------- model = AutoModel.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) print(outputs.last_hidden_state.shape) #------使用 AutoModelForSequenceClassification 加载模型-------- model = AutoModelForSequenceClassification.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) print(outputs.logits) print(outputs.logits.shape) outputs.logits 是模型最终的未经过归一化处理的预测分数，如[-1.5607, 1.6123]，它们并不是概率值。只是outputs中的一部分信息。如果直接print(outputs)输出的更全。\n其输出分别为：\n1 2 3 4 5 6 # AutoModel输出，(batch_size, sequence_length, vocab_size) torch.Size([1, 4, 768]) # AutoModelForSequenceClassification输出,句子的情感分类值 tensor([[-1.5607, 1.6123], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) torch.Size([1, 2]) # 标签，positive 或 negative 补充：查看输出的最后隐藏状态形状\n1 2 3 4 last_hidden_states = outputs.last_hidden_state print(last_hidden_states.shape) #-------------------等价于------------------- print(outputs.last_hidden_state.shape) 对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。还要经过解码操作，比如以下代码可以得到生成的文本。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer import torch checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model.generate(**inputs) output_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(output_text) 可以看到， pipeline 背后的工作原理就是最底层的实现方式，它封装好了底层代码方便使用而已。下面会具体介绍组成 pipeline 的两个重要组件模型（Models 类）和分词器（Tokenizers 类）。\noutputs = model(**inputs)\n这种调用方式主要用于获取模型在输入数据上的中间结果，比如隐藏状态（hidden states）、未归一化的预测分数（logits）等。这些结果通常用于进一步的任务处理。\noutput = model.generate(**inputs)\n此调用方式专门用于文本生成任务。它会基于输入的上下文，使用特定的生成策略（如贪心搜索、束搜索等）来生成后续的文本序列。\n底层\u0026ndash;模型 在大部分情况下，我们都应该使用 AutoModel 来加载模型。这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。\n所有存储在HuggingFace上的模型都可以通过 Model.from_pretrained() 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录）。\n1 2 3 model = BertModel.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) model = BertModel.from_pretrained(\u0026#34;./models/bert/\u0026#34;) 如果本地没有下载该模型的权重文件，代码运行后会自动缓存下载的模型权重，默认保存到 ~/.cache/huggingface/transformers。\n底层\u0026ndash;分词器 由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为编码 (Encoding)，包含两个步骤：\n分词：使用分词器按某种策略将文本切分为 tokens； 映射：将 tokens 转化为对应的 token IDs（词表）； 分词器的加载与模型相似，使用 Tokenizer.from_pretrained()函数。同样地，在大部分情况下我们都应该使用 AutoTokenizer 来加载分词器。\n1 tokenizer = AutoTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) 分词器编码-方式1 可以通过 encode() 函数将上述两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 [CLS] 和 [SEP] 。\n1 2 3 4 5 6 7 8 9 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequence = \u0026#34;Using a Transformer network is simple\u0026#34; sequence_ids = tokenizer.encode(sequence) print(sequence_ids) 其输出为如下，其中 101 和 102 分别是 [CLS] 和 [SEP] 对应的 token IDs。\n1 [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102] 分词器编码-方式2 注意，上面这些只是为了演示。在实际编码文本时，最常见的是直接使用分词器进行处理，这样不仅会返回分词后的 token IDs，还自动包含了模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 token_type_ids 和 attention_mask。\n1 2 3 4 5 6 7 8 9 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequence = \u0026#34;Using a Transformer network is simple\u0026#34; sequence_text = tokenizer(sequence) print(sequence_text) 其输出为如下\n1 2 3 {\u0026#39;input_ids\u0026#39;: [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 也就是说，两者区别在于：\ntokenizer.encode 方法：返回的是编码后的 ID 列表 tokenizer 方法：更通用，返回一个包含更多信息的字典,如 attention_mask 、token_type_ids等 分词器解码 文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。通过 decode() 函数解码前面生成的 token IDs：\n1 2 3 4 5 6 7 8 9 10 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]) print(decoded_string) 输出为\n1 2 Using a transformer network is simple [CLS] Using a Transformer network is simple [SEP] 底层\u0026ndash;Padding 与 Attention Mask 在实际中，一个 batch 包含多个输入，每个输入有长有短，而输入张量必须是严格的二维矩形，维度为 (batch size,sequence length)，即每一段文本编码后的 token IDs 数量必须一样多。我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度。\n在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了。它且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。\n正如前面所说，在实际使用时，应该直接使用分词器来完成包括分词、转换 token IDs、Padding、构建 Attention Mask、截断等操作。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoTokenizer checkpoint = \u0026#34;/bert-base-cased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequences = [ \u0026#34;I\u0026#39;ve been waiting for a HuggingFace course my whole life.\u0026#34;, \u0026#34;So have I!\u0026#34; ] model_inputs = tokenizer(sequences, padding=\u0026#34;longest\u0026#34;, return_tensors=\u0026#34;pt\u0026#34;, max_length=8, truncation=True) print(model_inputs) 分词器的输出包含了模型需要的所有输入项。包括 input_ids和 attention_mask。\nPadding 操作 Padding 操作通过 padding 参数来控制：\npadding=\u0026quot;longest\u0026quot;： 将序列填充到当前 batch 中最长序列的长度； padding=\u0026quot;max_length\u0026quot;：将所有序列填充到模型能够接受的最大长度，例如 BERT 模型就是 512。（如果代码指定了max_length的长度，就不是 512 了） padding=True： 等同于 padding=\u0026quot;longest\u0026quot;； 1 model_inputs = tokenizer(sequences, padding=\u0026#34;max_length\u0026#34;) 截断操作 截断操作通过 truncation 参数来控制，如果 truncation=True，那么大于模型最大接受长度的序列都会被截断。此外，也可以通过 max_length 参数来控制截断长度。\n1 model_inputs = tokenizer(sequences, max_length=8, truncation=True) 返回格式 分词器还可以通过 return_tensors 参数指定返回的张量格式：设为 pt 则返回 PyTorch 张量；tf 则返回 TensorFlow 张量，np 则返回 NumPy 数组。\n1 model_inputs = tokenizer(sequences, padding=True, return_tensors=\u0026#34;pt\u0026#34;) 完整格式 综上所述，实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \u0026#34;xxx-models\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequences = [ \u0026#34;I\u0026#39;ve been waiting for a HuggingFace course my whole life.\u0026#34;, \u0026#34;So have I!\u0026#34; ] tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) print(tokens) output = model(**tokens) print(output.logits) 其输出为\n1 2 3 4 5 6 7 8 9 10 11 {\u0026#39;input_ids\u0026#39;: tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \u0026#39;attention_mask\u0026#39;: tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])} tensor([[-1.5607, 1.6123], [-3.6183, 3.9137]], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) 在 padding=True, truncation=True 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断。\n底层\u0026ndash;模型推理和输出 在将文本转换为模型可接受的输入格式后，就可以进行模型推理了。模型推理是指将预处理后的输入数据送入模型，让模型根据其内部的参数和结构进行计算，从而得到每个token包含上下文信息的高维嵌入。不同任务类型的模型会返回不同形式的输出。\n1 2 output = model.generate(**inputs, max_length=50, num_beams=5, no_repeat_ngram_size=2) output_text = tokenizer.decode(output[0], skip_special_tokens=True) 下面是一些参数解释：\n**inputs：这是一个解包操作，inputs 通常是一个字典，包含了input_ids、attention_mask等信息。通过 **inputs 可以将字典中的键值对作为关键字参数传递给 generate() 方法，自动处理 input 的所有参数 输出长度：指定生成文本的最大长度（以token为单位），当生成文本达到这个长度时，生成过程将停止 max_length 生成序列的最大总长度，包含了输入加输出的token总长度 max_new_tokens 仅关注新生成的token数量，而不考虑输入序列本身的长度 num_beams：束搜索（Beam Search）的束宽。束搜索会在每一步保留 num_beams 个最有可能的候选序列，可以提高生成文本的质量，从而找到更优的生成结果，但同时也会增加计算量和内存消耗 no_repeat_ngram_size：模型在生成过程中不允许出现重复的 n - gram 大小。n - gram 是指连续的 n 个标记组成的序列，例如当 n = 2 时，就是不允许连续的两个标记组成的序列重复出现。用于避免生成的文本中出现重复的短语或句子，提高生成文本的多样性和质量。例如，如果生成的文本中已经出现了 “the dog”，那么在后续的生成过程中就不会再出现 “the dog” 这个 2 - gram output[0]：model.generate() 方法返回的是一个包含多个生成序列的张量，output[0] 表示取第一个生成序列。在 num_return_sequences = 1 的情况下，通常只生成一个序列 skip_special_tokens=True：在解码过程中是否跳过特殊符号，如[CLS]、[SEP]。设置为 True 可以去除这些特殊标记，使生成的文本更加干净和易读 生成任务输出 1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModelForCausalLM,AutoTokenizer checkpoint = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForCausalLM.from_pretrained(checkpoint) input_text = \u0026#34;The future of AI is\u0026#34; inputs = tokenizer(input_text, return_tensors=\u0026#34;pt\u0026#34;, padding=True, truncation=True) output = model.generate(**inputs, max_length=50, num_beams=5, no_repeat_ngram_size=2) output_text = tokenizer.decode(output[0], skip_special_tokens=True) print(output_text) 对于文本生成任务（如使用 AutoModelForCausalLM），模型通过 generate() 方法直接生成 token IDs，需通过分词器解码为文本。\n隐藏状态提取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import AutoModel,AutoTokenizer checkpoint = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModel.from_pretrained(checkpoint) # 获取最后一层隐藏状态 input_text = \u0026#34;Once upon a time\u0026#34; inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\u0026#34;pt\u0026#34;) outputs = model(**inputs) hidden_states = outputs.last_hidden_state print(\u0026#34;Hidden states shape:\u0026#34;, hidden_states.shape) # 输出 torch.Size([2, sequence_length, 768]) 生成参数控制：max_length、num_beams（束搜索）、temperature（采样温度）等参数可调节生成结果的质量和多样性。\n完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch from transformers import AutoTokenizer, AutoModelForCausalLM model_name = \u0026#34;/home/caijinwei/disk1/Hugging-Face/deepseek-14B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name,device_map=\u0026#34;auto\u0026#34;,torch_dtype=torch.float16) question = \u0026#34;魔都是哪个城市?\u0026#34; inputs = tokenizer( question, max_length=512, padding=\u0026#39;max_length\u0026#39;, return_tensors=\u0026#39;pt\u0026#39;).to(model.device) # inputs = tokenizer(question, padding=True, return_tensors=\u0026#39;pt\u0026#39;).to(model.device) output = model.generate( **inputs, num_beams=5, max_new_tokens=1000, no_repeat_ngram_size=2, # temperature=0.7, # 增加创造性 ) output_text = tokenizer.decode(output[0], skip_special_tokens=True) print(output_text) device_map=\u0026quot;auto\u0026quot;：\n该参数用于指定模型在设备（如 CPU、GPU）上的分布方式。\u0026quot;auto\u0026quot; 表示让 transformers 库自动分配显卡，将模型拆分到多个 GPU 上，避免内存不足的问题。\n注意，如果设置了device_map=\u0026quot;auto\u0026quot;，就不要在输入以下代码，这可能会覆盖之前的设备映射，导致模型被加载到单一设备上\n1 2 3 # 检查是否有可用的 GPU,将模型移动到 GPU 设备 device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model.to(device) 例如，下面代码不可取\n1 2 3 4 5 6 7 model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\u0026#34;auto\u0026#34;) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model.to(device) question = \u0026#34;介绍一下上海大学。\u0026#34; inputs = tokenizer(question,padding=True,return_tensors=\u0026#39;pt\u0026#39;).to(device) ","date":"2025-04-13T10:56:01+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/","title":"LLM系列-3：模型与分词器"},{"content":"Attention由来 ​ NLP 神经网络模型的本质就是对输入文本进行编码，然后基于概率的思想完成 NLU 或 NLP 任务。常规的做法是首先对句子进行分词（token），然后将每个 token 都转化为对应的词向量 (token embeddings)，这样文本就转换为一个由词语向量组成的矩阵$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$ 。\n在 Transformer 模型提出之前，对 token 序列 X 的常规编码方式是通过循环网络 (RNNs) 和卷积网络 (CNNs)。 $$ RNN：\\boldsymbol{y}_t = f(\\boldsymbol{y}_{t - 1}, \\boldsymbol{x}_t)\\\\ CNN：\\boldsymbol{y}_t = f(\\boldsymbol{x}_{t - 1}, \\boldsymbol{x}_t, \\boldsymbol{x}_{t + 1}) $$ RNN（例如 LSTM）的方案很简单，每一个 token 对应的编码结果通过递归地计算得到，如公式1；\n但是递归的结构导致其无法并行计算，因此速度较慢。而且 RNN 本质是一个马尔科夫决策过程，难以学习到全局的结构信息；\nCNN 则通过滑动窗口基于局部上下文来编码文本，例如核尺寸为 3 的卷积操作就是使用每一个词自身以及前一个和后一个词来生成嵌入式表示，如公式2；\nCNN 能够并行地计算，因此速度很快，但是由于是通过窗口来进行编码，所以更侧重于捕获局部信息，难以建模长距离的语义依赖；\n于是 Google 提出的Attention Is All You Need论文给出了第三种解决方案：直接使用 Attention 机制编码整个文本。相比 RNN 要逐步递归才能获得全局信息，而 CNN 实际只能获取局部信息，需要通过层叠来增大感受野，Attention 机制一步到位获取了全局信息。\n点积注意力 虽然 Attention 有许多种实现方式，但是最常见的还是 Scaled Dot-product Attention。包含 2 个主要步骤：\n计算注意力权重：使用某种相似度函数度量每一个 query 向量和所有 key 向量之间的关联程度。\n特别地，Scaled Dot-product Attention 使用点积作为相似度函数，这样相似的 queries 和 keys 会具有较大的点积。这会破坏训练过程的稳定性。因此注意力分数还需要乘以一个缩放因子来标准化它们的方差，然后用一个 softmax 标准化。这样就得到了最终的注意力权重.\n更新 token embeddings：将权重 与对应的 value 向量相乘以获得向量更新后的语义表示。\n$$ \\mathrm{Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\mathrm{softmax}\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d_k}}\\right)\\boldsymbol{V} $$下面通过 Pytorch 来实现：\n词嵌入得到 首先需要将文本分词为词语 (token) 序列，然后将每一个词语转换为对应的词向量 (embedding)。Pytorch 提供了 torch.nn.Embedding 层来完成该操作，即构建一个从 token ID 到 token embedding 的映射表。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from torch import nn from transformers import AutoConfig from transformers import AutoTokenizer # 加载预训练分词器 model_ckpt = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 输入文本并进行分词 text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) print(inputs.input_ids) # 加载模型参数配置并创建嵌入层 config = AutoConfig.from_pretrained(model_ckpt) token_emb = nn.Embedding(config.vocab_size, config.hidden_size) print(token_emb) # 输出文本对应的嵌入向量 inputs_embeds = token_emb(inputs.input_ids) print(inputs_embeds.size()) 输出为：\n1 2 3 tensor([[ 2051, 10029, 2066, 2019, 8612]]) Embedding(30522, 768) torch.Size([1, 5, 768]) 可以看到，BERT-base-uncased 模型对应的词表大小为 30522，每个词语的词向量维度为 768。Embedding 层把输入的词语序列映射到了尺寸为 [batch_size, seq_len, hidden_dim] 的张量。\n这里我们通过设置 add_special_tokens=False 去除了分词结果中的 [CLS] 和 [SEP]。\ninput_ids 是如何得到的：\n分词器已经针对bert-base-uncased模型进行了预训练，了解该模型词汇表中的所有词元（token）。分词器会将输入文本拆分成一个个词元。把每个词元映射到词汇表中的对应索引。bert-base-uncased模型有一个预先定义好的词汇表，每个词元都有唯一的整数索引。\ninput_ids是如何转换为向量的：\n加载模型的配置信息后，这些配置信息包含了模型的各种参数，例如词汇表大小（vocab_size）和隐藏层大小（hidden_size）等。 接下来，使用nn.Embedding创建一个嵌入层。它的作用是将每个词的索引（即input_ids中的整数）映射到一个固定长度的向量。这里的config.vocab_size表示词汇表的大小，也就是模型所能处理的不同词元的数量；config.hidden_size表示每个词元对应的嵌入向量的维度。在bert-base-uncased模型中，vocab_size通常为 30522，hidden_size为 768。 nn.Embedding在创建时会随机初始化一个形状为(vocab_size, hidden_size)的权重矩阵。这个矩阵中的每一行对应词汇表中一个词元的嵌入向量。例如，矩阵的第i行就是词汇表中索引为i的词元的嵌入向量。 1 根据大模型config.json配置文件中的hidden_size和vocab_size配置项可以得到 预训练模型不是已经训练好了词汇表的嵌入了吗，为什么nn.Embedding是随机初始化：\n使用nn.Embedding创建的嵌入层的确是随机初始化的，但加载的预训练模型也有一个预训练的词汇表嵌入矩阵的。\n当你直接使用nn.Embedding创建嵌入层时，这通常意味着你打算从头开始训练模型，或者在已有模型基础上针对特定任务进行微调。随机初始化可以让模型在训练过程中根据具体任务的数据和目标来学习合适的词嵌入表示。通用的预训练词嵌入可能无法很好地适应特定领域的任务，这时随机初始化并重新训练嵌入层可能会得到更好的效果。\n1 2 3 4 5 6 7 8 9 10 11 12 from transformers import AutoModel model_ckpt = \u0026#34;bert-base-uncased\u0026#34; model = AutoModel.from_pretrained(model_ckpt) # 获取预训练的嵌入层 token_emb = model.embeddings.word_embeddings text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) inputs_embeds = token_emb(inputs.input_ids) print(inputs_embeds.size()) 注意力计算 注意力机制-第三章\n多头注意力 Multi-head Attention 首先通过线性映射将 Q,K,V 序列映射到特征空间，每一组线性投影后的向量表示称为一个头 (head)，然后在每组映射后的序列上再应用 Scaled Dot-product Attention。\n每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。因此Multi-head Attention 可以捕获到更加复杂的特征信息。 $$ {head}_i= \\mathrm{Attention}(\\boldsymbol{Q}\\boldsymbol{W}_i^Q, \\boldsymbol{K}\\boldsymbol{W}_i^K, \\boldsymbol{V}\\boldsymbol{W}_i^V) \\\\ {MultiHead}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})= \\mathrm{Concat}(\\mathrm{head}_1, \\dots, \\mathrm{head}_h) $$ 以下是单个注意力头实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 from torch import nn class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() self.q = nn.Linear(embed_dim, head_dim) self.k = nn.Linear(embed_dim, head_dim) self.v = nn.Linear(embed_dim, head_dim) def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None): attn_outputs = scaled_dot_product_attention( self.q(query), self.k(key), self.v(value), query_mask, key_mask, mask) return attn_outputs 每个头都会初始化三个独立的线性层，负责将 Q,K,V 序列映射到尺寸为 [batch_size, seq_len, head_dim] 的张量，其中 head_dim 是映射到的向量维度。（将得到的输入文本对应的词嵌入通过线性层转换得到 Q,K,V ，因为是多头，所以嵌入大小由原来的embed_dim变为head_dim）\n实践中一般将 head_dim 设置为 embed_dim 的因数，这样 token 嵌入式表示的维度就可以保持不变，例如 BERT 有 12 个注意力头，因此每个头的维度被设置为 768/12=64 。\n最后只需要拼接多个注意力头的输出就可以构建出 Multi-head Attention 层了（这里在拼接后还通过一个线性变换来生成最终的输出张量）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size num_heads = config.num_attention_heads head_dim = embed_dim // num_heads self.heads = nn.ModuleList( [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] ) self.output_linear = nn.Linear(embed_dim, embed_dim) def forward(self, query, key, value, query_mask=None, key_mask=None, mask=None): x = torch.cat([ h(query, key, value, query_mask, key_mask, mask) for h in self.heads ], dim=-1) x = self.output_linear(x) return x 从输入文本到经过 Attention 后得到的词嵌入实现过程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from transformers import AutoConfig from transformers import AutoTokenizer # 加载预训练分词器 model_ckpt = \u0026#34;bert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # 分词、加载模型参数、初始化嵌入 text = \u0026#34;time flies like an arrow\u0026#34; inputs = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, add_special_tokens=False) config = AutoConfig.from_pretrained(model_ckpt) token_emb = nn.Embedding(config.vocab_size, config.hidden_size) inputs_embeds = token_emb(inputs.input_ids) # Attention更新嵌入 multihead_attn = MultiHeadAttention(config) query = key = value = inputs_embeds attn_output = multihead_attn(query, key, value) print(attn_output.size()) 其输出为：\n1 torch.Size([1, 5, 768]) Transformer架构 标准 Transformer 结构，Encoder 负责将输入的词语序列转换为词向量序列，Decoder 则基于 Encoder 的隐状态来迭代地生成词语序列作为输出，每次生成一个词语。\nTransformer Encoder 除了多个 Attention 之外，还包括The Feed-Forward Layer、Layer Normalization、Positional Embeddings等结构。\n1、The Feed-Forward Layer\n实际上就是两层全连接神经网络，它单独地处理序列中的每一个词向量，也被称为 position-wise feed-forward layer。常见做法是让第一层的维度是词向量大小的 4 倍，然后以 GELU 作为激活函数。\n2、Layer Normalization\n负责将一批 (batch) 输入中的每一个都标准化为均值为零且具有单位方差；Skip Connections 则是将张量直接传递给模型的下一层而不进行处理，并将其添加到处理后的张量中。\n3、Positional Embeddings\n由于注意力机制无法捕获词语之间的位置信息，因此 Transformer 模型还使用 Positional Embeddings 添加了词语的位置信息。如果预训练数据集足够大，那么最简单的方法就是让模型自动学习位置嵌入。\nTransformer Decoder Transformer Decoder 与 Encoder 最大的不同在于 Decoder 有两个注意力子层：\nMasked multi-head self-attention layer：确保在每个时间步生成的词语仅基于过去的输出和当前预测的词，否则 Decoder 相当于作弊了； Encoder-decoder attention layer：以解码器的中间表示作为 queries，对 encoder stack 的输出 key 和 value 向量执行 Multi-head Attention。通过这种方式，Encoder-Decoder Attention Layer 就可以学习到如何关联来自两个不同序列的词语。 参考 注意力机制-第三章\n","date":"2025-04-13T10:55:51+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-2attention%E8%AF%A6%E8%A7%A3/","title":"LLM系列-2：Attention详解"},{"content":"学技术前先读史，对后续有很帮助。下面介绍一下统计语言模型发展史。\nN-gram 模型 其预测下一个词语的核心思想如下：\n$$ P(w_n|w_1, w_2, \\ldots, w_{n - 1}) $$ 即下一个词语出现的概率取决于在句子中出现在它之前的所有词，但是，随着文本长度的增加，条件概率会变得越来越难以计算，因而在实际计算时会假设每个词语仅与它前面的N−1个词语有关，即\n$$ P(w_i | w_1, w_2, \\ldots, w_{i - 1}) = P(w_i | w_{i - N + 1}, w_{i - N + 2}, \\ldots, w_{i - 1}) $$ 这种假设被称为马尔可夫（Markov）假设，对应的语言模型被称为 N 元（N-gram）模型。比如N=2时基于前一个单词预测当前单词；N=3时考虑前两个单词来预测当前单词；而N=1时，模型实际上就是一个上下文无关模型。\n通过统计语料库中 n - gram 序列的频率，估计给定前 n - 1 个元素后下一个元素出现的概率 ，从而选择概率最高的词作为预测结果。例如，在语料库中统计 “我喜欢” 后面接不同词的频率，若 “苹果” 出现次数最多，当输入 “我喜欢” 时，就可能预测下一个词是 “苹果” 。\nNNLM 模型 NNLM 模型的思路与统计语言模型保持一致，它通过输入词语前面的N−1个词语来预测当前词。其结构如图所示：\n具体来说，模型首先从词表C中查询得到前面N-1个词语对应的词向量，然后将这些词向量拼接后输入到带有激活函数的隐藏层中，通过Softmax函数预测当前词语的概率。特别地，包含所有词向量的词表矩阵 C 也是模型的参数，需要通过学习获得。\nWord2Vec 模型 真正将神经网络语言模型发扬光大的模型，Word2Vec 模型提供的词向量在很长一段时间里都是自然语言处理方法的标配。Word2Vec 的模型结构和 NNLM 基本一致，只是训练方法有所不同，分为 CBOW (Continuous Bag-of-Words) 和 Skip-gram 两种。\n其中 CBOW 使用周围的词语w(t-2),w(t-1),w(t+1),w(t+2)来预测当前词w(t)，而 Skip-gram 则正好相反，它使用当前词w(t)来预测它的周围词语。Word2Vec 模型训练目标也更多地是为获得词向量服务。特别是同时通过上文和下文来预测当前词语的 CBOW 训练方法打破了语言模型“只通过上文来预测当前词”的固定思维。\n通过将词汇表中的单词转换成高维空间向量来表示语义关系，为自然语言处理提供了有效的文本数值化方式，推动了深度学习在 NLP 领域的应用。\n数据：比如窗口大小设为 2，对于句子 “我 爱 自然 语言 处理” ，以 “自然” 为中心词，上下文词是 “我”“爱”“语言”“处理” 初始化：为每个单词初始化一个随机词向量 计算预测：CBOW 将上下文词向量相加输入隐藏层再到输出层，Skip - gram 则是将目标词向量输入隐藏层再到输出层，输出层都用 softmax函数计算词汇表中每个词作为预测结果的概率 更新权重：根据预测结果和真实标签计算损失，通过反向传播算法使损失最小化，不断迭代优化词向量 ELMo 模型 为了更好地解决多义词问题，提出了 ELMo 模型（Embeddings from Language Models）。与 Word2Vec 模型只能提供静态词向量不同，ELMo 模型会根据上下文动态地调整词语的词向量。\n具体来说，ELMo 模型首先对语言模型进行预训练，使得模型掌握编码文本的能力；然后在实际使用时，对于输入文本中的每一个词语，都提取模型各层中对应的词向量拼接起来作为新的词向量。ELMo 模型采用双层双向 LSTM 作为编码器，从两个方向编码词语的上下文信息，相当于将编码层直接封装到了语言模型中。\n训练完成后 ELMo 模型不仅学习到了词向量，还训练好了一个双层双向的 LSTM 编码器。对于输入文本中的词语，可以从第一层 LSTM 中得到包含句法信息的词向量，从第二层 LSTM 中得到包含语义信息的词向量，最终通过加权求和得到每一个词语最终的词向量。\n但是 ELMo 模型存在两个缺陷：首先它使用 LSTM 模型作为编码器，而不是当时已经提出的编码能力更强的 Transformer 模型；其次 ELMo 模型直接通过拼接来融合双向抽取特征的做法也略显粗糙。\n不久之后，将 ELMo 模型中的 LSTM 更换为 Transformer 的GPT、Bert模型就出现了。\nBERT 模型 2018 年底随着 BERT 模型（Bidirectional Encoder Representations from Transformers）的出现，这一阶段神经网络语言模型的发展终于出现了一位集大成者。\nBERT 模型采用和 GPT 模型类似的两阶段框架，首先对语言模型进行预训练，然后通过微调来完成下游任务。但是，BERT 不仅像 GPT 模型一样采用 Transformer 作为编码器，而且采用了类似 ELMo 模型的双向语言模型结构。因此 BERT 模型不仅编码能力强大，而且对各种下游任务，BERT 模型都可以通过简单地改造输出部分来完成。\n但是 BERT 模型的优点同样也是它的缺陷，由于 BERT 模型采用双向语言模型结构，因而无法直接用于生成文本。\n在 BERT 模型取得成功之后，在其基础上又提出了诸如RoBERTa等改良模型，其中具有代表性的就是微软提出的UNILM模型（可以使Bert具备生成能力），它把 BERT 模型的 MASK 机制运用到了一个很高的水平。\nBERT不能直接用于生成文本，原因如下：\n掩码语言模型（Masked Language Model, MLM）：BERT 通过随机遮盖文本中 15% 的 token，让模型根据上下文来预测被遮盖的词。这种训练方式让模型学会融合双向上下文信息理解语义，使其在理解上下文语义时表现优异。\n但也导致了一个关键问题——生成任务需要单向、逐步的预测。通常是自回归的，即逐词生成，每个新词的预测只能依赖已生成的左侧上下文。而BERT的双向机制在生成时会引入未来信息，导致信息泄露，与实际生成过程矛盾。不像自回归模型，如 GPT 那样从左到右依次根据前面生成的内容预测下一个词，所以难以直接用于文本生成任务。\nEncoder-only模型：BERT 架构只采用了 Transformer 的编码器部分，没有使用Transformer 解码器那样具备处理生成任务的结构设计。生成任务通常需要解码器逐步生成输出，按照顺序逐步生成下一个词，而 BERT 编码器缺乏这种从左到右、顺序生成的机制 。\n补充知识 LLM分类 一般分为三种：自回归模型、自编码模型和序列到序列模型。\n自回归（Autoregressive model）模型：decoder-only模型。采用经典的语言模型任务进行预训练，即给出上文，预测下文，对应原始Transformer模型的解码器部分，其中代表模型是GPT系列。模型一般会用于NLG的任务，如文本生成。 自编码（AutoEncoder model）模型：encoder-only模型。采用句子重建进行预训练，即预先通过某种方式破坏句子，掩码或打乱顺序，希望模型将被破坏的部分还原，对应原始Transformer模型的编码器部分，其中代表模型是BERT系列。与自回归模型不同，模型既可以看到上文信息，也可以看到下文信息，由于这样的特点，模型往往适用于NLU的任务，如文本分类、阅读理解等。 序列到序列（Sequence to Sequence Model）模型：则是同时使用了原始的编码器与解码器。这种模型最自然的应用便是文本摘要、机器翻译等任务，事实上基本所有的NLP任务都可以通过序列到序列解决。 NLG - 自然语言生成；NLU - 自然语言理解\nTransformer结构 标准的 Transformer 模型主要由两个模块构成：\nEncoder：负责理解输入文本，为每个输入构造对应的语义表示 Decoder：负责生成输出，使用 Encoder 输出的语义表示结合其他输入来生成目标序列 这两个模块可以根据任务的需求而单独使用：\n纯 Encoder 模型：适用于只需要理解输入语义的任务，例如句子分类、命名实体识别 纯 Decoder 模型：适用于生成式任务，例如文本生成 Encoder-Decoder 模型或 Seq2Seq：适用于需要基于输入的生成式任务，例如翻译、摘要 Transformer 模型本来是为了翻译任务而设计的。在训练过程中，Encoder 接受源语言的句子作为输入，而 Decoder 则接受目标语言的翻译作为输入。在 Encoder 中，由于翻译一个词语需要依赖于上下文，因此注意力层可以访问句子中的所有词语；而 Decoder 是顺序地进行解码，在生成每个词语时，注意力层只能访问前面已经生成的单词。\n例如，假设翻译模型当前已经预测出了三个词语，我们会把这三个词语作为输入送入 Decoder，然后 Decoder 结合 Encoder 所有的源语言输入来预测第四个词语。\n实际训练中为了加快速度，会将整个目标序列（真实值）都送入 Decoder，然后在注意力层中通过 Mask 遮盖掉未来的词语来防止信息泄露。因为若按顺序一个词一个词输入，效率很低。一次性输入整个目标序列，模型可以并行处理计算，大大加快训练速度 。\n其中，Decoder 中的第一个注意力层关注 Decoder 过去所有的输入，而第二个注意力层则是使用 Encoder 的输出，因此 Decoder 可以基于整个输入句子来预测当前词语。这对于翻译任务非常有用。\n在 Encoder/Decoder 的注意力层中，我们还会使用 Attention Mask 遮盖掉某些词语来防止模型关注它们，例如为了将数据处理为相同长度而向序列中添加的填充 (padding) 字符。\n总结 \u0026amp; 参考 可以看出，预测下一个词模型的基本范式都是学习融合上下文信息的每个词的嵌入表征，然后通过输出层（通常接 softmax 函数 ，维度大小是词库大小）计算词汇表中每个词作为下一个词的概率，从中选择概率最高的词或通过一定采样策略（如多项分布采样 ）确定下一个词。\n以 GPT 为例，它是自回归模型，从左到右依次根据已有的词预测下一个词 ；而 BERT 虽然主要用于完形填空任务（掩码语言模型），但也可以在微调后用于预测下一个词等生成任务 。\nTransformers-第一章\nTransformers-第二章\n","date":"2025-04-12T12:04:52+08:00","permalink":"https://xxcjw.github.io/p/llm%E7%B3%BB%E5%88%97-1nlp%E7%9A%84%E5%8F%91%E5%B1%95/","title":"LLM系列-1：NLP的发展"},{"content":"git设置代理 开vpn之后，当你还在为能正常打开Github、Youtobe而窃喜时，不知道你是否也遇到过这种情况：\n但当你使用git工具将本地代码文件上传Github时，却总是出现Failed to connect to github.com port 443: Timed out或者OpenSSL SSL_read: Connection was reset, errno 10054等git push失败的情况，让人火大。 或者发现git clone命令速度特别慢，有时还经常卡掉，这很让人着急。 总之，就四个字：失败、火大！于是搜各种教程，经过各种折腾，终于找到解决方案，归根结底还是因为代理设置的不正确！！！本文记录一下解决的方案。\n问题描述 简单来说，就是所使用的vpn正常，但是上传或下载Github文件时就很慢很卡，甚至连接不上出现报错情况。如果遇到这种情况，可以使用下面的方法解决。\n如果检查自己的vpn是否正常呢？有两种简单办法\n最简单的当然就是打开一个国外网站，比如YouTube等，如果能打开，证明梯子没问题； 另一种，就是win+r打开命令行，输入ping github.com看看能不能连上要使用的Github； 解决方法 遇到上述问题，就是代理设置不正确。可以通过以下命令查看和设置理。\n1、代理查询\n查看自己是否以前设置过代理，在CMD中输入以下命令进行查询\n1 2 git config --global http.proxy git config --global https.proxy 如果没有记录显示，则说明本电脑还没有配置过代理，否则需要先将代理进行删除，再进行后续的重新添加。\n2、代理取消\n如果执行上述代码之后，发现有代理存在，则执行下述命令，先将代理取移除\n1 2 git config --global --unset http.proxy git config --global --unset https.proxy git config 是 Git 用于配置和管理各种参数的命令，可以设置和修改与 Git 相关的配置选项。\nhttp.proxy 是用于设置 Git 在进行 HTTP 或 HTTPS 连接时使用的代理服务器。\n3、代理设置\n首先需要获取当前所使用vpn的代理服务器地址，格式为：127.0.0.1:xxxx（我的是7879）。这里一定要注意，不要照抄网上的端口号，需要根据自己的服务器进行修改，否则报错。得到代理服务器地址之后，使用如下命令配置代理\n1 2 git config --global https.proxy 127.0.0.1:7897 git config --global http.proxy 127.0.0.1:7897 如何知道自己的代理服务器地址呢？两种办法：\n一是通过自己的vpn查看，打开自己的vpn-设置-端口设置/端口号的选项，这个就是代理服务器的端口 打开电脑的控制面板-网络和Internet-Internet选项-连接-局域网设置，然后就可以看到代理服务器的地址了，见图 通过这两种方式得到的应该是一样的。\n然后就配置成功了！就可以成功上传和下载文件了。需要注意的是，有少数人刚配置后，效果显著，但用一段时间之后，发现git push上传文件时仍然失败，这时候按照第二个步骤将代理重新取消掉就好了，如果有过一段时间又不行了，在重新配置就行，这样反复操作可以解决，但为什么这样不得而知。\n原理介绍 代理服务器就是你的电脑和互联网的中介。当您访问外网时 , 你的请求首先转发到代理服务器，然后代理服务器替你访问外网，并将结果原封不动的给你的电脑，这样你的电脑就可以看到外网的内容啦。\n使用vpn后无法正常上网 解决方法 很多同学使用vpn访问外网，但当关闭VPN后，却发现没有办法正常连接到互联网了。解决方法如下：\n1 控制面板--\u0026gt;网络和Internet--\u0026gt;Internet选项--\u0026gt;弹出的Internet属性窗口--\u0026gt;连接--\u0026gt;局域网设置 将为LAN使用代理服务器（这些设置不用于拨号或VPN连接）前面的√取消掉可以了（开vpn的时候勾上是正常的，如果关掉vpn后能正常上网就不用取消）。到此为止不出意外就可以正常上网啦！\n原因分析 一句话：可能是由于VPN代理软件修改了电脑IP地址的获取方式。\n通常情况下，计算机的IP地址是通过动态主机配置协议（DHCP）从您的路由器或网络提供商的服务器上获取的。使用VPN代理软件时，它会在您的计算机和互联网之间创建一个安全的连接。在这种连接中，VPN软件会通过在您的计算机和VPN服务器之间建立虚拟通道来代理您的网络流量。当您的网络流量通过VPN服务器时，服务器会为您的流量分配一个新的IP地址。\n通过这些步骤，VPN代理软件成功修改了您计算机的IP地址获取方式。现在，计算机使用的是VPN服务器分配的IP地址，而不是原本的IP地址。这样做可以帮助您隐藏真实的IP地址并实现匿名上网，同时提供加密保护，确保您的网络流量在传输过程中得到安全保护。同时，当我们关闭VPN后，由于电脑IP地址任然为VPN分配的IP地址（而此时我们已经关闭了代理服务器的连接），这就导致我们无法正常访问互联网了。\n参考：使用vpn/代理后无法正常上网\n","date":"2025-04-10T20:18:22+08:00","permalink":"https://xxcjw.github.io/p/%E4%BB%A3%E7%90%86-%E8%81%94%E7%BD%91%E9%97%AE%E9%A2%98/","title":"代理 \u0026 联网问题"},{"content":"最常用 1 2 3 4 5 6 7 8 9 10 11 win + l #快速锁屏 ctrl + f #查找 alt + tab #切换页面 win + v #粘贴板 win + space #切换中英文 shift + delete #完全删除文件 tab #选中多行按tab一起缩进 shift + tab #多行取消缩进 ctrl + alt + . #黑屏时可关机、打开任务管理器 ctrl + alt + delete #黑屏时可关机、打开任务管理器 alt + 左键 #word、IDEA中可以竖着xuan\u0026#39;zho 截图快捷键\n1 2 3 win + shift + s #自带截图 alt + z #截图软件截图 alt + a #微信截图 命令行类命令 系统信息查看类 1 2 3 4 5 6 nvidia-smi #查看gpu使用情况 ipconfig #显示ip地址信息 watch -n 1 nvidia-smi #动态追踪查看显存占用 ping xxx #查看本机能否连通某网址 free -h #查看服务器内存（Mem-物理内存，Swap-交换空间） df -h #查看服务器外存/硬盘使用情况（use%表示使用率） 会话管理指令 1 2 3 4 5 screen -S xxxx #创建会话，如screen -S test ctrl+a d #退出会话同时保持程序运行(按住Ctrl，依次再按a,d) screen -r xxxx #恢复会话，如screen -r test exit / ctrl+d #关闭会话，会提示：screen is terminating screen -ls #列出当前存在的会话列表 文件目录操作类 1 2 3 4 5 6 7 8 9 10 ⬆/⬇ #切换历史命令 cd ./xx #切换到某某文件夹 pwd #显示当前工作目录的完整路径 ls #列出当前目录中的文件（-l、-a） source xxx #在当前会话中执行某个脚本，使其立即生效 mkdir xxx #创建一个新目录 rmdir xxx #删除空目录（-p 一次删除多个空文件夹；-r 递归删除） rm xxx #删除不为空的文件或目录（-r递归删除，常用在目录删除） cp source destination #复制文件或目录到指定文件夹下 mv source destination #移动文件或目录 进程管理类 1 2 3 4 5 6 7 8 ps -p \u0026lt;PID\u0026gt; #查看进程信息（看不到使用者） ps -f -p \u0026lt;PID\u0026gt; #查看进程详细信息（f表示full，全部信息） kill \u0026lt;PID\u0026gt; #根据进程号杀死进程 kill -9 \u0026lt;PID\u0026gt; #强制杀死进程 top #实时监控系统中各个进程的资源占用情况 shutdown -h now #服务器立刻关机 shutdown -h 10 #10 分钟后自动关机 shutdown -r now #重启（=reboot也是重启） 文本处理类 1 2 3 4 5 grep \u0026#34;Hello\u0026#34; xx.txt #文本搜索指令，从txt文件中搜索包含指定字符串的行 grep -i \u0026#34;hello\u0026#34; xx.txt #忽略大小写搜索 grep -r \u0026#34;hello\u0026#34; /data #递归搜索，在一个目录搜索包含指定字符串的行 grep -n \u0026#34;Hello\u0026#34; xx.txt #显示匹配行的行号 grep -inr \u0026#34;hello\u0026#34; . #组合使用，点代表在当前目录下搜索 权限管理类 1 2 3 4 5 6 7 ls -l #列出当前目录中的文件以及权限（别称：ll） --------------------通过符号修改权限------------------- chmod u=rwx,g=rx,o=x xxx #对xxx文件夹的所有者、所属组和其他人修改权限 chmod g+w test.txt #对txt文件的所属组添加写的权限 chmod a-x test #对test文件夹的所有用户去除执行的权限（a表示所有人，即u+g+o） --------------------通过数字修改权限------------------- chmod 751 test #等价于chmod u=rwx,g=rx,o=x xxx（7 = rwx = 4+2+1，以此类推） 浏览器快捷键 1 2 3 4 5 ctrl + l #定位到地址栏（可通过设置后，通过应用快速打开某程序） ctrl + t #新建标签页 ctrl + shift + t #重新打开刚才关闭的标签页 ctrl + tab #切换标签页 ctrl + n #打开一个新的浏览器窗口 typora快捷键 1 2 3 4 5 6 7 8 ``` #插入代码块 $$ #插入公式 enter/shift + enter #不一样的换行方式 ctrl + t #新建表格 ctrl + enter #表格插入行 ctrl + shift + back #删除表格指定行 []() #插入链接（前面加个“!”就是插入图片） - [ ] xxx #插入 vim快捷键 1 2 3 :q #退出（如果有更改不会保存） :wq #保存并退出 :q! #强制退出并丢弃未保存的更改 万能命令 win 万能命令是一个在线工具快捷跳转平台。在浏览任意网页时，输入wn.run/即可。\n其他 1 ctrl+shift+space #vscode参数 补充 \u0026amp; 参考 chmod 命令 （change mode）：修改文件或目录的权限\nchown 和 chgrp：修改文件拥有者或所属组\n格式：chmod [选项] 权限 文件名\n权限的本质就是可以干什么，权限=角色（user、group、others）+目标权限属性（rwx）。\nLinux下包含两种用户：超级用户（root）和普通用户。在 Linux 中的每个用户必须属于一个组，不能独立于组外 在 Linux 中每个文件有所有者、所在组、其它组的概念 所有者-User：文件或目录的创建者 所在组-Group：文件或目录所属的用户组 其他-Others：除了文件所有者和用户组之外的所有人 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 权限的基本介绍(0-9位说明)： 第 0 位确定文件类型(d, - , l , c , b) 第 1-3 位确定所有者（该文件的所有者）拥有该文件的权限。---User 第 4-6 位确定所属组（同用户组的）拥有该文件的权限，---Group 第 7-9 位确定其他用户拥有该文件的权限 ---Other 文件类型： - 普通文件；d 目录；l 软链接；c 字符设备；b 硬盘 rwx权限详解： [ r ]代表可读: 可以读取,查看 [ w ]代表可写: 可以修改,如果针对的是文件，则不可以删除；如果针对的是目录，则可以删除 [ x ]代表可执行：可以被执行 可用数字表示为: r=4,w=2,x=1 因此 rwx=4+2+1=7 数字含义： 如果是目录，则表示子目录个数；如果是文件，则表示硬链接数量 之后两项： 分别表示文件或目录的所有者和所属组 数字含义： 文件大小(字节)，如果是文件夹，显示 4096 字节 剩余项： 最后修改日期和文件名称 快捷键大全网址：\n快捷键速查表 - 星云导航\n快捷键备忘录\n","date":"2025-04-10T16:30:06+08:00","permalink":"https://xxcjw.github.io/p/%E7%94%B5%E8%84%91%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4/","title":"电脑相关指令"},{"content":"文章插入图片 如果想要在文章插入图片，有两种方式：\n方式一： 1 ![name](pic.png) 缺点是这种方式插入的图片不可以调节大小，默认占满页面宽度，很不美观。\n方式二： 1 \u0026lt;img src=\u0026#34;1.png\u0026#34; width=\u0026#34;80%\u0026#34; align=\u0026#34;middle\u0026#34; style=\u0026#34;zoom:60%;\u0026#34; /\u0026gt; 如果是在markdown文件里，是居中且缩放到原图的60%大小，但在网页上居中不起作用。\n方式三： 1 \u0026lt;center\u0026gt;\u0026lt;img src=\u0026#34;1.png\u0026#34; width=\u0026#34;80%\u0026#34; align=\u0026#34;middle\u0026#34; style=\u0026#34;zoom:60%;\u0026#34; /\u0026gt;\u0026lt;/center\u0026gt; 这样就可以实现图片居中且缩放成任意比例。\n友链布局修改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 title: 友链 slug: \u0026#34;links\u0026#34; layout: \u0026#34;links\u0026#34; menu: main: weight: -50 params: icon: link comments: false links: - title: GitHub description: 全世界最大的代码托管和开源项目平台. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png sites: - title: GitHub description: 全世界最大的代码托管和开源项目平台. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png 现在想要实现的效果如图所示，即按照类别来划分友链，如何实现呢？\n首先，在content/page/links/index.md文件中新建一个链接名，如links、sites等，然后把想要插入的友链放入； 然后，在layouts/page/links.html文件中写入如下代码。其中，xxxs就是上面的链接名，如links、sites等；类别名就是想要的分类名称，如推荐大佬、科技\u0026amp;论坛； 1 2 3 4 5 6 7 \u0026lt;header\u0026gt; \u0026lt;h2 class=\u0026#34;section-title\u0026#34; style=\u0026#34;margin-bottom: -20px;\u0026#34;\u0026gt;类别名\u0026lt;/h2\u0026gt; \u0026lt;/header\u0026gt; {{ if .Params.xxxs }} {{ partial \u0026#34;article/components/xxxs\u0026#34; . }} {{ end }} 最后，在layouts/partials/article/components文件下新建xxxs.html文件，文件不存在的就复制一下其他的（代码如下），然后将旧链接名修改为新建的链接名； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026lt;div class=\u0026#34;article-list--compact links\u0026#34;\u0026gt; {{ range $i, $hugo := .Params.hugos }} \u0026lt;article\u0026gt; \u0026lt;a href=\u0026#34;{{ $hugo.website }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;article-details\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;article-title\u0026#34;\u0026gt; {{- $hugo.title -}} \u0026lt;/h2\u0026gt; \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ with $hugo.description }} {{ . }} {{ else }} {{ $hugo.website }} {{ end }} \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; {{ with $hugo.image }} {{ $permalink := . }} {{ with ($.Resources.GetMatch (printf \u0026#34;%s\u0026#34; (. | safeURL))) }} {{ $permalink = .RelPermalink }} {{ end }} \u0026lt;div class=\u0026#34;article-image\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ $permalink }}\u0026#34; loading=\u0026#34;lazy\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; {{ end }} \u0026lt;/div\u0026gt; 文章内链跳转 写文章的时候总有一个需求就是关联之前写的文章，那么在hugo中应该要怎么用呢？\n方法一：markdown语法\n使用[]()创建链接，比如hugo常用命令。此方法缺点就是文章标题和链接变了，所有地方都需要手动修改。在测试的时候甚至会出现点击后会出现404找不到界面。\n方法二：hugo提供的ref功能\n用法如下所示（哈哈去掉），此方法的好处就是文章的链接变了，这里会跟着变的，不需要手动修改。缺点是文章的标题不能同步变化。hugo常用命令\n建议使用绝对路径（根目录为content目录），而非相对路径，否则容易出错 只有所引用的文件与当前文件在同一文件夹下时可以使用相对路径的方式（只有文件名） 1 [hugo常用命令]({{哈哈\u0026lt; ref \u0026#34;/post/2025-04/hugo快捷命令.md\u0026#34; \u0026gt;}}) 文章内容管理 本文按时间线进行内容管理，其目录结构如下，需要注意以下几点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 content/ └── post ├── 2025-01 │ ├── hugo博客搭建.md │ ├── git常用命令.md │ └── tools文件夹 │ ├── 1.jpg │ ├── 2.jpg │ ├── 3.png │ ├── index.md │ └── 4.png │ ├── 2025-02 │ ├── 博客搭建1.md │ ├── 博客搭建2.md │ ├── 博客搭建3.md │ ├── 博客搭建4.md │ ├── 博客搭建5.md │ ├── 博客搭建6.md │ ├── 博客搭建7.md │ ├── 博客搭建8.md │ ├── 博客搭建9.md │ └── 人生意义.md └── 博客搭建10 ├── index.md └── 博客记录说明 ├── 1.png ├── 2.png ├── 3.png ├── 4.png ├── 5.png ├── 6.png ├── 7.png ├── 8.png ├── 9.png ├── index.zh-cn.md └── index.zh-tw.md post目录下先按照年份建立子目录， 也可以按照 年/月，或者 年/月/日建立 没有图片的文章直接在一级子目录下保存 有图片的多建一级目录, md文件命名为index.lang.md(使用其他命名文件，图片不能显示), 图片放在同一目录/子文件夹 不同语言的md文件，放在一起，统一用不同的index.lang.md区分， 比如 index.zh-cn.md, index.zh-tw.md Bug问题 输入公式时（$$$$），必须空一行才能正常显示（markdown换行enter是区分段落，源代码默认会有一个空行，粘贴复制过来的文字可能是shift+enter换行，导致不是enter换行）。行内插入公式时使用$xxx$表示 加粗时，加粗的内容不能以符号结尾，否则在渲染时出错 Markdown 开源文档\n致谢 1、博客搭建教程\n简单来说，分为几步：\n下载解压hugo压缩包，cmd 打开命令行窗口，输入hugo new site xxxx创建文件（这里是dev） 复制hugo.exe文件到dev文件内 进入hugo官网，下载stack主题压缩包，将文件解压存储在dev\\themes文件下 将 exampleSite 样例数据中的 Content 和 hugo.yaml 复制到主文件夹中，并删掉hugo.toml 在dev文件夹，输入hugo server -D，发现已经正确显示 Github Action自动部署 莱特雷-letere\nHugo+Github博客部署\n使用 Hugo + Github Pages 部署个人博客\n2、修改美化\n首先修改dev文件夹下的hugo.yaml文件 其次，主要修改的是assets、layouts、static文件夹，换新电脑时可以直接复制过去 assets/scss/custom.scss文件修改的是大部分美化样式 layouts/_default/single.html文件修改的是：如果文章有目录，则把左侧边栏换为一个返回主页的按钮，如果文章没有目录，则启用左侧边栏 layouts/page文件夹和layouts/partials文件夹修改的是友链分类相关的文件 layouts/index.html文件修改的是：添加了首页欢迎字符面板 themes/hugo-theme-stack/layouts/partials/article/components/details.html文件修改的是：添加文章字数统计功能；而该文件夹下footer.html修改的是：添加了一行代码以及新建categories.html文件实现在文章末尾也显示分类标签 themes/hugo-theme-stack/layouts/partials/footer/footer.html删除了页脚信息，更简洁 themes/hugo-theme-stack/layouts/partials/article/components/related-content.html整个文件删除或注释，这样文章底部就不会显示相关文章了 assets/scss/partials文件夹以及layouts/partials/footer文件夹修改内容：添加博客运行时间以及样式 Stack 魔改美化-Naive Koala-2篇\nHugo Stack 主题美化-阿琦同学-很全-2篇\nL1nSn0wの小窝 - Stack主题的自定义\nHugo的Stack主题美化零碎-wfg\n使用 Hugo 对博客的重建与 Stack 主题优化记录-Exnadio\u0026rsquo;s Blog\nStack 主题的自定义-折腾日记\nHugo Stack 主题配置与使用 | Bore\u0026rsquo;s Notes\nHugo博客 | stack主题修改第一站-墨纹\nHugo Stack主题装修笔记-博客运行时间\nLeonus 博客\n张洪Heo - 分享设计与科技生活\n安知鱼 - 生活明朗 万物可爱\nHugo Theme Cybe\n3、图标网站\n打开stack官方文档Stack 官方文档，在``Custom Menu`栏下可以找到推荐的图标网站\nIcons网站\n","date":"2025-04-08T22:53:36+08:00","permalink":"https://xxcjw.github.io/p/stack%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9/","title":"Stack主题修改"},{"content":"创建文章 1 hugo new post/xxx/xxx.md 预览网站 1 2 hugo server -D hugo server 两者都是用于启动 Hugo 本地开发服务器的命令，区别主要在于是否包含草稿文章方面：\nhugo server：启动一个本地开发服务器，该服务器会对项目文件的变更进行监控，一旦文件有改动，就会实时重新生成网站。不过，在生成网站内容时，它默认不会包含草稿文章。 hugo server -D：-D 是 --buildDrafts 的简写。样会启动本地开发服务器，实现对文件变更的监控和网站的实时更新与刷新。不同的是，在生成网站时会将草稿文章也包含进去。 清除缓存 1 2 3 hugo --gc hugo --gc --minify hugo server --gc -D 当预览修改文章时，发现页面没变化，可以尝试清除缓存并重新构建：\nhugo --gc：--gc 标志的作用是在构建过程中执行垃圾回收（Garbage Collection），即清除不再使用的缓存文件，这样能释放磁盘空间。如果对网站配置或内容进行了大量修改，旧的缓存不再使用，该命令能保证生成的网站是最新状态。 hugo --gc --minify：除了具备执行垃圾回收功能外，--minify 标志还会对生成的 HTML、CSS、JavaScript 等文件进行压缩。可以减小文件大小，从而提升网站的加载速度。适合在准备将网站部署到生产环境时使用，能提供更流畅的访问体验。 hugo server --gc -D：启动一个实时预览服务器，同时执行垃圾回收，确保使用的是最新的缓存。适合在开发过程中使用，当频繁修改文章内容，并希望随时预览草稿文章在内的网站效果时最方便。 发布文章 1 2 3 4 5 git init git add . git commit -m \u0026#34;xxx\u0026#34; git remote add origin {github仓库地址} git push -u origin main git init：初始化一个新的 Git 仓库，会将当前目录变为一个Git仓库。并生成一个名为 .git 的隐藏目录，包含了 Git 管理项目所需的各种配置文件和数据结构。 git add .：用于把文件的修改添加到暂存区。可以在暂存区（Git中的一个中间区域）组织和规划哪些修改要包含在下次提交中。. 代表当前目录下的所有文件和子目录。 git commit -m \u0026quot;xxx\u0026quot;：用于将暂存区的修改保存到本地仓库的历史记录中，-m 标志后面跟着的 \u0026quot;xxx\u0026quot; 是本次提交的说明信息。 git remote add origin xxx：用于管理与远程仓库（一般是GitHub创建的远仓）的连接。add 子命令用于添加一个新的远程仓库。origin 是远程仓库的默认名。这会将本地仓库与该 GitHub 仓库建立连接。 git push -u origin main：用于将本地仓库的提交推送到远程仓库。-u会将本地的 main 分支与远程仓库的 main 分支关联起来，这样在后续的推送操作中，你只需要执行 git push 即可。origin 是远程仓库的别名，main 是要推送的本地分支名称。 1 2 git push git push -u origin main git push -u origin main：除了将本地的 main 分支推送到远程 origin 仓库的 main 分支外，还会建立本地 main 分支和远程 origin/main 分支的关联。建立关联后，后续使用 git push 或 git pull 时，Git 会知道默认操作的远程分支。通常在首次将本地分支推送到远程仓库时使用，这样后续无需每次都指定远程仓库和分支。 git push：如果本地分支已经和远程分支建立了关联，使用该命令可以简化操作，快速将本地更新推送到远程。适合在本地分支和远程分支已经建立关联的情况下。 其他 1 git remote -v 查看本地仓库与哪些远程仓库进行了连接的命令。执行该命令后，会列出本地仓库所关联的所有远程仓库的别名以及对应的远程仓库的 URL 地址。这里会展示 fetch（拉取）和 push（推送）对应的地址，一般情况下二者是相同的。\n1 hugo version 查看hugo版本，我的是0.145版本。\n","date":"2025-04-08T08:37:54+08:00","permalink":"https://xxcjw.github.io/p/hugo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Hugo常用命令"}]